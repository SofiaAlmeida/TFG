% article example for classicthesis.sty
\documentclass[10pt,a4paper]{article} % KOMA-Script article scrartcl
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{url}
\usepackage[nochapters]{./classicthesis} % nochapters

\usepackage{amsmath}
\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definición}[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{example}[theorem]{Ejemplo}


\begin{document}
    \pagestyle{plain}
    \title{\rmfamily\normalfont\spacedallcaps{the title}}
    \author{\spacedlowsmallcaps{Sofía Almeida Bruno}}
    \date{} % no date

    \maketitle

    \begin{abstract}
        \noindent\lipsum[1] Just a test.\footnote{This is a footnote.}
    \end{abstract}

    \tableofcontents

    \section{Varios}
    Objetivo: aplicar información mutua para seleccionar variables (en un problema de aprendizaje ...) utilizando clustering previamente para disminuir el volumen de los datos a tratar.

    Problema: Una persona no puede trabajar con todas las vraibles que tiene un determinado problema, tratamos de reducirlas, seleccionamos variables utilizando como criterio la información mutua (basada en el concepto de entropía...)
    ...maldición de la dimensionalidad...calculamos $2^d-1$ distancias
    La información mutua se basa en un calculo de distancias. Queremos comprobar si reduciendo el número de instancias mediante técnicas de clustering la información mutua no se ve afectada.
     
    \section{Teoría de la información}

    El concepto de información es demasiado amplio como para ser recogido en una única definición. Sin embargo, podemos definir el concepto de entropía, que mide la cantidad de información necesaria para describir una variable aleatoria, o el de información mutua, que refleja la información que una variable aleatoria contiene sobre otra.

    \subsection{Entropía}

    Intuitivamente, la entropía es una medida de la información o incertidumbre de una variable aleatoria. El término entropía viene del griego, donde significa \textit{transformación} y tiene ¿utilidad? en otras áreas como la física, donde se define como el logaritmo de la razón entre temperatura final e inicial de un sistema.

    La idea de relacionar el número de estados de un sistema con una medida física viene del siglo XIX, en el que Rudolph Clausius sugirió la denominación ``entropía'' para esta medida. En 1928, Ralph Vinton Lyon Hartley define el término en el contexto de la teoría de la información. Aunque no es hasta 1948 que Claude Shannon no da una definición matemática del concepto de información,  sentando las bases de la teoría de la información, que pasó a formar parte de la teoría de la probabilidad. En su concepto de entropía no todos los símbolos de un alfabeto tienen porqué ser equiprobables. Esto nos permite medir la capacidad de comunicación de un canal. Andrei N. Kolmogorov, en la década de los 60, desarrolló su teoría de la complejidad, otra forma de teoría de la información.\\

Sea $\Omega$ un espacio muestral discreto y finito y una variable aleatoria, $X = x_1, \cdots, x_N = \mathcal{X}$, con función masa de probabilidad $P_X(x_i) = p(x_i)$.

\begin{definition}[Entropía]
  La entropía de una variable aleatoria discreta $X$ viene dada por \[H(X) = - \sum_{x\in \mathcal{X}} p(x) \log p(x).\]
\end{definition}

En la fórmula anterior definiremos $0 \log 0 = 0$. Así, la entropía queda definida por un funcional de la distribución de $X$, no depende de los valores que tome la variable sino de sus probabilidades.

Según cuál sea la base del logaritmo utilizado en la fórmula de la entropía utilizaremos unas unidades u otras. Notamos $H_b(X) = - \sum_{x\in \mathcal{X}} p(x) \log_b p(x)$. Si el logaritmo tiene base 2, la entropía se medirá en bits, mientras que si la base es $e$, las unidades serán \textit{nats}.\\

Dada la variable aleatoria discreta $X$ con distribución $p(x)$, podemos considerar la variable aleatoria $g(X)$, cuya esperanza, $E$, viene dada por:\[
E_pg(X) = \sum_{x\in \mathcal{X}}g(x)p(x).
\]
Tomando $g(X) = \log \frac{1}{p(X)}$ obtenemos otra definición de entropía, la usada en termodinámica:\[
H(X) = E_p \log \frac{1}{p(X)}.
\]

La fórmula anterior coincide, efectivamente, con la definición de entropía dada:\[
H(X) = E_p \log \frac{1}{p(X)} = \sum_{x\in \mathcal{X}}\left ( \log \frac{1}{p(x)} \right ) p(x) = - \sum_{x \in \mathcal{X}}p(x)\log p(x).
\]

\begin{lemma} La entropía de una variable aleatoria discreta es no negativa, $H(X) \ge 0$.
\end{lemma}
\begin{proof}
  La probabilidad de un elemento $x$ es positiva, de hecho se encuentra en el intervalo $[0,1]$, luego el logaritmo de la probabilidad será negativo, ya que en 1 es donde cambia su signo. El producto de ambos factores es negativo para cualquier $x \in \mathcal{X}$, luego el signo menos en la fórmula de la entropía nos permite concluir que $H(X) = - \sum_{x \in \mathcal{X}}p(x)\log p(x) \ge 0$.
\end{proof}

El siguiente lema, mediante propiedades del logaritmo, nos permitirá cambiar la base con la que calculemos la entropía.

\begin{lemma}\[H_b(X) = (\log_ba) H_a(X).\]
\end{lemma}
\begin{proof}
  Partiremos de la definición alternativa de $H_b$ y usaremos que $\log_bp = \log_ba\ \log_ap$.
  \[
  H_b(X) = E_p \left(\log_b \frac{1}{p(X)}\right ) = E_p \left ( \log_b a\ \log_a \frac{1}{p(X)} \right ) = \log_b{a}\ H_a(X).
  \]
\end{proof}

\begin{example}
  Sea $X = 
     \begin{cases}
       1, \quad\text{con probabilidad } p\\
       0, \quad\text{con probabilidad } 1-p \\
     \end{cases}.$\\
     La entropía viene dada por $H(X) = - \left ( p \log p + (1-p) \log (1-p) \right ) =: H(p)$.

     La función $H(p)$ es cóncava. Cuando $p=0$ o $p=1$, $H(p) = 0$, luego no hay incertidumbre, se conoce el valor que tomará la variable $X$. La incertidumbre máxima se alcanza en $p=\frac{1}{2}$: \[H\left (\frac{1}{2}\right ) = - \left ( \frac{1}{2}\log\frac{1}{2} + \frac{1}{2} \log\frac{1}{2} \right ) = - \log \frac{1}{2} = 1.\]

\end{example}

Pasamos a un ejemplo en el que el valor de las probabilidades es uno concreto:
\begin{example}
  Sea $X = 
     \begin{cases}
       a, \quad\text{con probabilidad } \frac{1}{2}\\
       b, \quad\text{con probabilidad } \frac{1}{4} \\
       c, \quad\text{con probabilidad } \frac{1}{8} \\
       d, \quad\text{con probabilidad } \frac{1}{8} \\
     \end{cases}.$ \\

     Utilizamos la fórmula de la entropía y obtenemos \[H(X) = - \left (\frac{1}{2} \log \frac{1}{2} + \frac{1}{4} \log \frac{1}{4} + \frac{1}{4} \log \frac{1}{8} \right ) = - \left (-\frac{1}{2} - \frac{1}{2} - \frac{3}{4} \right ) = 1 + \frac{3}{4} = \frac{7}{4} \text{ bits}.\]
\end{example}

Tenemos la definición de entropía de una variable aleatoria, podemos extenderla a un par de variables aleatorias.

\begin{definition}[Entropía conjunta]
  La entropía conjunta, $H(X,Y)$, de un par de variables aleatorias discretas $(X,Y)$ con distribución conjunta $p(x,y)$ se define como \[
  H(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y), \]
  que podemos expresar también como \[
  H(X,Y) = - E \log p(X,Y).\]
\end{definition}

En el siguiente lema recogemos algunas propiedades de la entropía conjunta.

\begin{lemma} Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifican:
  \begin{enumerate}
  \item $H(X) \le H(X, Y)$.
  \item $H(X,Y) \le H(X) + H(Y)$.
  \end{enumerate}
\end{lemma}

\begin{proof}\hfill
  \begin{enumerate}
  \item Para cualesquiera $x \in \mathcal{X}, y \in \mathcal{Y}$ se tiene que $p(x,y) \le p(x)$ y $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$, luego:
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \ge - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x)\\
   &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) = H(X).
\end{align*}

\item Usando la definición, $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y propiedades del logaritmo:
  \begin{align*}
    &H(X,Y) - (H(X) + H(Y)) \\ &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} p(x)\log p(x) + \sum_{y \in \mathcal{Y}} p(y)\log p(y)\\
    &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(y)\\
    &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x)p(y)}{p(x,y)}
  \end{align*}
  \end{enumerate}
\end{proof}

Buscamos que la entropía cumpla una serie de propiedades para dar un enfoque axiomático de la medida de información:

    \begin{itemize}
    \item Si las probabilidades de un evento sufren un pequeño cambio, la medida asociada debe cambiar de manera acorde, es decir, que la medida sea continua respecto a la probablidad.
    \item La medida de información debe ser simétrica, esto es, no debe cambiar según el orden de las variables a considerar.
    \item La entropía máxima se obtiene cuando los eventos son igualmente probables. Es decir, cuando unas variables no aportan información adicional sobre otras, estamos ante el máximo nivel de incertidumbre.
    \end{itemize}

    La entropía de un alfabeto finito depende solo de las distribuciones de los procesos.

    
   % \finalVersionString
    \section{Clustering}
    Podemos enunciar el problema de clustering (también segmentación o agrupamiento) de la siguiente forma:
    \begin{quote}
      Dado un conjunto finito $X=\{x_1,\dots,x_n\}$ encontrar un entero $c$, $2 \leq c < n$, y una c-partición de $X$ en conjuntos homogéneos.
    \end{quote}
    Ante este problema una de las primeras dudas que surge es: qué criterio de agrupamiento deberíamos usar. Se pueden usar algunas propiedades matemáticas como la distancia, curvatura, conexión, \dots  En cualquier caso la selección del criterio será subjetiva, no existe un criterio de semejanza universal.
    
    A continuación, comentaremos dos algoritmos de clustering: \textit{Hard c-means} y \textit{Fuzzy c-means}, ambos basados en la optimización de una función objetivo. Para ello es necesario introducir las nociones de conjuntos \textit{hard} y \textit{fuzzy}.

    Sea $A \in \mathcal{P}(A)$. Definiremos ambos tipos de conjuntos a partir de su función característica. La función característica de un subconjunto \textit{hard} $A \subset X$, $\mathcal{X}_A:X \to \{0,1\}$, viene dada por:
\[  
\mathcal{X}_A(x) = 
     \begin{cases}
       1, \quad\text{si } x \in A\\
       0, \quad\text{si } x \notin A \\ 
     \end{cases}.
\]

La diferencia con un subconjunto \textit{fuzzy} será el codominio de la función característica. En un conjunto \textit{hard} el codominio es discreto, mientras que la función característica de un conjunto \textit{fuzzy} podrá tomar cualquier valor en el intervalo $[0,1]$, $\mathcal{X}:X \to [0,1]$.

A partir de estos conceptos podemos determinar cuándo una partición es \textit{hard} o \textit{fuzzy}.


    
\subsection{\textit{Hard c-means}}
Algoritmo propuesto por R. Duda y P. Hart en 1973.
    
    \subsection{\textit{Fuzzy c-means}}
    
    % bib stuff
    \nocite{*}
    \addtocontents{toc}{\protect\vspace{\beforebibskip}}
    \addcontentsline{toc}{section}{\refname}
    \bibliographystyle{plain}
    \bibliography{../Bibliography}
\end{document}
