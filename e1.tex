% article example for classicthesis.sty
\documentclass[10pt,a4paper]{article} % KOMA-Script article scrartcl
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage{url}
\usepackage[nochapters]{./classicthesis} % nochapters

%%% Matemáticas
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[Symbol]{upgreek} %pi
% WIDE HAT
\usepackage{scalerel,stackengine}
\stackMath
\newcommand\reallywidehat[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern.1pt\mathchar"0362\kern.1pt}%
    {\rule{0ex}{\textheight}}%WIDTH-LIMITED CIRCUMFLEX
  }{\textheight}% 
}{2.4ex}}%
\stackon[-6.9pt]{#1}{\tmpbox}%
}

%%% Tablas
\usepackage{tabularx}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{diagbox}

%%% Bibliografía
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{spanish}{
  urlseen = {Último acceso}
}
\addbibresource{citations.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definición}[section]

\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{corollary}{Corolario}[theorem]

\begin{document}     
    \section{Teoría de la información}

    El concepto de información es demasiado amplio como para ser recogido en una única definición. Sin embargo, podemos definir el concepto de entropía, que mide la cantidad de información necesaria para describir una variable aleatoria, o el de información mutua, que refleja la información que una variable aleatoria contiene sobre otra.

    \subsection{Entropía}

    Intuitivamente, la entropía es una medida de la información o incertidumbre de una variable aleatoria. El término entropía viene del griego, donde significa \textit{transformación} y tiene relevancia también en otras áreas, como en la física donde se define como el logaritmo del cociente entre temperatura final e inicial de un sistema.

    La idea de relacionar el número de estados de un sistema con una medida física viene del siglo XIX, en el que Rudolph Clausius sugirió la denominación ``entropía'' para esta medida. En 1928, Ralph Vinton Lyon Hartley define el término en el contexto de la teoría de la información, básicamente como el logaritmo del tamaño del alfabeto. Aunque no es hasta 1948 cuando Claude Shannon da una definición matemática del concepto de información como lo conocemos hoy en día,  sentando las bases de la teoría de la información, que pasó a formar parte de la teoría de la probabilidad. En su concepto de entropía no todos los símbolos de un alfabeto tienen porqué ser equiprobables. Esto nos permite medir la capacidad de comunicación de un canal. Andrei N. Kolmogorov, en la década de los 60, desarrolló su teoría de la complejidad, otra forma de teoría de la información.\\

Sea una variable aleatoria, $X$ que toma valores en el alfabeto $\mathcal{X} = \{x_1, \cdots, x_N \}$, con función masa de probabilidad $P_X(x_i) = p(x_i)$.

\begin{definition}[Entropía]
  La entropía de una variable aleatoria discreta $X$ viene dada por \[H(X) = - \sum_{x\in \mathcal{X}} p(x) \log p(x).\]
\end{definition}

En la fórmula anterior definiremos, por continuidad, $0 \log 0 = 0$. Así, la entropía queda definida por un funcional de la distribución de $X$,es decir, que no depende de los valores que tome la variable sino de sus probabilidades.

Según cuál sea la base del logaritmo utilizado en la fórmula de la entropía utilizaremos unas unidades u otras. Notamos $H_b(X) = - \sum_{x\in \mathcal{X}} p(x) \log_b p(x)$. Si el logaritmo tiene base 2, la entropía se medirá en bits, mientras que si la base es $e$, las unidades serán \textit{nats} (unidad natural de información).\\

Dada la variable aleatoria discreta $X$ con distribución $p(x)$, podemos considerar la variable aleatoria $g(X)$, cuya esperanza, $E$, viene dada por:\[
E_pg(X) = \sum_{x\in \mathcal{X}}g(x)p(x).
\]
Tomando $g(X) = \log \frac{1}{p(X)}$ obtenemos otra definición de entropía, la usada en termodinámica:\[
H(X) = E_p \log \frac{1}{p(X)}.
\]

La fórmula anterior coincide, efectivamente, con la definición de entropía dada:\[
H(X) = E_p \log \frac{1}{p(X)} = \sum_{x\in \mathcal{X}}\left ( \log \frac{1}{p(x)} \right ) p(x) = - \sum_{x \in \mathcal{X}}p(x)\log p(x).
\]

\begin{lemma} La entropía de una variable aleatoria discreta es no negativa, $H(X) \ge 0$.
\end{lemma}
\begin{proof}
  \[H(X) = - \sum_{x \in \mathcal{X}}\underbrace{p(x)}_{\in [0,1]}\underbrace{\log p(x)}_{\leq 0} \ge 0.\]

  %% El producto de ambos factores es negativo para cualquier $x \in \mathcal{X}$, luego el signo menos en la fórmula de la entropía nos permite concluir que $H(X) = - \sum_{x \in \mathcal{X}}p(x)\log p(x) \ge 0$.
\end{proof}

El siguiente lema, mediante propiedades del logaritmo, nos permitirá cambiar la base con la que calculemos la entropía.

\begin{lemma}\[H_b(X) = (\log_ba) H_a(X).\]
\end{lemma}
\begin{proof}
  Partiremos de la definición alternativa de $H_b$ y usaremos que $\log_bp = \log_ba\ \log_ap$.
  \[
  H_b(X) = E_p \left(\log_b \frac{1}{p(X)}\right ) = E_p \left ( \log_b a\ \log_a \frac{1}{p(X)} \right ) = \log_b{a}\ H_a(X).
  \]
\end{proof}

\begin{example}
  Sea \[X = 
     \begin{cases}
       1, \quad\text{con probabilidad } p\\
       0, \quad\text{con probabilidad } 1-p \\
     \end{cases}.\]
     La entropía de la variable aleatoria $X$ la notaremos $H(p)$ y se calcula como sigue: $ H(p) := H(X) = - \left ( p \log p + (1-p) \log (1-p) \right )$.
     Cuando $p=0$ o $p=1$, $H(p) = 0$, luego no hay incertidumbre, se conoce el valor que tomará la variable $X$. Calcularemos la primera y segunda derivada de $H(p)$ para conocer su comportamiento en el intervalo $(0,1)$.
     \begin{align*}
       H'(p) &= - \left ( \log p + \frac{p}{p} - \log(1-p) - \frac{1-p}{1-p} \right ) = - \left ( \log \left (\frac{p}{1-p} \right) \right ).\\
     H''(p) &= - \left ( \frac{1}{p} + \frac{1}{1-p} \right ) \leq 0 \quad \forall p \in (0,1).
     \end{align*}
     
     Hemos comprobado que la función $H(p)$ es cóncava. La incertidumbre máxima se alcanzará en $p=\frac{1}{2}$: \[H\left (\frac{1}{2}\right ) = - \left ( \frac{1}{2}\log\frac{1}{2} + \frac{1}{2} \log\frac{1}{2} \right ) = - \log \frac{1}{2} = 1.\]

\end{example}

Pasamos a un ejemplo en el que el valor de las probabilidades es uno concreto:
\begin{example}
  Sea  \[X = 
     \begin{cases}
       a, \quad\text{con probabilidad } \frac{1}{2}\\
       b, \quad\text{con probabilidad } \frac{1}{4} \\
       c, \quad\text{con probabilidad } \frac{1}{8} \\
       d, \quad\text{con probabilidad } \frac{1}{8} \\
     \end{cases}.\]

     Utilizamos la fórmula de la entropía y obtenemos \[H(X) = - \left (\frac{1}{2} \log \frac{1}{2} + \frac{1}{4} \log \frac{1}{4} + \frac{1}{4} \log \frac{1}{8} \right ) = - \left (-\frac{1}{2} - \frac{1}{2} - \frac{3}{4} \right ) = 1 + \frac{3}{4} = \frac{7}{4} \text{ bits}.\]
\end{example}

La definición de entropía de una variable aleatoria se puede aplicar a un par de variables aleatorias dando lugar a la entropía conjunta.

\begin{definition}[Entropía conjunta]
  La entropía conjunta, $H(X,Y)$, de un par de variables aleatorias discretas $(X,Y)$ con distribución conjunta $p(x,y)$ se define como \[
  H(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y), \]
  que podemos expresar también como \[
  H(X,Y) = - E \log p(X,Y).\]
\end{definition}
Presentamos el siguiente lema que será útil para la demostración de propiedades en lo sucesivo.

\begin{lemma}[Desigualdad de Gibbs]\label{l:gibbs}
  Dadas dos funciones masa de probabilidad $\{p_i\}$ y $\{q_i\}$, entonces \[
\sum_i p_i \log \frac{p_i}{q_i} \ge 0,
\]
dándose la igualdad si, y solo si, $q_i = p_i$ para todo $i$.
\end{lemma}

\begin{proof}
  Sea $I = \{i : p_i > 0\}$, entonces\[
- \sum_{i \in I} p_i \log \frac{p_i}{q_i} = \sum_{i \in I} p_i \log \frac{q_i}{p_i},
  \]
  usando $\log x \le x - 1$, con igualdad si, y solo si, x = 1, obtenemos\[
  \sum_{i \in I} p_i \log \frac{q_i}{p_i} \leq
  \sum_{i\in I}p_i \left ( \frac{q_i}{p_i} - 1 \right )
  = \sum_{i\in I}q_i - \underbrace{\sum_{i \in I}p_i}_{= 1} \leq 0.\]

  Tendremos la igualdad cuando $\frac{q_i}{p_i} = 1$ para todo $i$.
\end{proof}

En el siguiente lema recogemos algunas propiedades de la entropía conjunta.

\begin{lemma}\label{l:prop_ent_conj} Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifican:
  \begin{enumerate}
  \item $H(X) \le H(X, Y)$.
  \item $H(X,Y) \le H(X) + H(Y)$. La igualdad se dará si, y solo si, $X$ e $Y$ son independientes.
  \end{enumerate}
\end{lemma}

\begin{proof}\hfill
  \begin{enumerate}
  \item Para cualesquiera $x \in \mathcal{X}, y \in \mathcal{Y}$ se tiene que $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y $p(x,y) \le p(x)$, luego:
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \ge - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x)\\
   &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) = H(X).
\end{align*}

\item Usando la definición, $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y propiedades del logaritmo:
  \begin{align*}
    &H(X,Y) - (H(X) + H(Y)) \\ &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} p(x)\log p(x) + \sum_{y \in \mathcal{Y}} p(y)\log p(y)\\
    &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(y)\\
    &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x)p(y)}{p(x,y)} \leq 0,
  \end{align*}
  donde la última desigualdad viene de aplicar el Lema \ref{l:gibbs} sobre las funciones masa de probabilidad $p(x,y)$ y $p(x)p(y)$. Esta última sería la función masa de probabilidad conjunta de dos variables aleatorias $X,Y$ con distribuciones $p(x), p(y)$ independientes. La igualdad ocurre si, y solo si, $p(x)p(y) = p(x,y)$, esto es si $X$ e $Y$ son independientes.
  \end{enumerate}
\end{proof}

Al considerar dos distribuciones distintas, además de su entropía conjunta podemos definir la entropía condicional de una respecto de la otra.

\begin{definition}[Entropía condicional]
  Si $(X,Y)$ sigue una distribución $p(x,y)$, la entropía condicional de $Y$ con respecto a $X$, $H(Y|X)$, se define como:
  \begin{align*}
    H(Y|X) &= \sum_{x \in \mathcal{X}} p(x) H(Y | X = x)\\
    &= - \sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y | x) \log p(y | x)\\
    &=  - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y | x)\\
    &=- E \log p(Y|X).
  \end{align*}
\end{definition}

En la definición anterior, la probabilidad condicionada $p(y|x)$ viene dada por la función masa de probabilidad $p(y|x) = \frac{p(x,y)}{p(x)}$.

\begin{theorem}[Regla de la cadena]\label{t:regla_cadena}
  Si $(X,Y)$ sigue una distribución $p(x,y)$, entonces se verifica:\[
H(X,Y) = H(X) + H(Y|X).
  \]
\end{theorem}

\begin{proof}
Para la prueba utilizaremos que $p(x,y) = p(x)p(y|x)$,
  \begin{align*}
    H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \left( p(x)p(y|x) \right )\\
    &= - \left(\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y|x) \right ),\\
  \end{align*}
usando que $\sum_{y\in \mathcal{Y}}p(x,y) = p(x)$, obtenemos
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) + H(Y|X)\\
    &= H(X) + H(Y|X).
  \end{align*}
\end{proof}

\begin{corollary} En las condiciones del Teorema \ref{t:regla_cadena},\[
H(X,Y|Z) = H(X|Z) + H(Y|X, Z).
  \]
\end{corollary}

\begin{lemma}
  Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces la entropía condicional cumple:\[
0 \leq H(Y|X) \leq H(Y).
\]
Donde la desigualdad de la derecha es una igualdad si, y solo si, $X$ e $Y$ son independientes.
\end{lemma}

\begin{proof}
  La desigualdad izquierda se obtiene de la definición de entropía condicional, \[H(Y|X) = - E \underbrace{\log \overbrace{p(Y|X)}^{\in [0,1]}}_{\leq 0} \ge 0.\]

  Aplicando el Teorema \ref{t:regla_cadena} y el Lema \ref{l:prop_ent_conj} se obtiene la desigualdad de la derecha:\[
H(X) + H(Y|X) = H(X,Y) \leq H(X) + H(Y),
\]
dándose la igualdad solo en el caso en que las variables $X$ e $Y$ sean independientes.
\end{proof}

\begin{example}\label{e:dist_conj}
Sea $(X,Y)$ una variable aleatoria con distribución conjunta:
  \begin{table}[H]
\centering
%\caption{}
\label{}
\begin{tabular}{r|llll}
  \toprule
\backslashbox{$Y$}{$X$} & 1 & 2 & 3 & 4\\ \hline\\[-10pt]
1 & $\frac{1}{8}$ & $\frac{1}{16}$ & $\frac{1}{32}$ & $\frac{1}{32}$\\[5pt]
2 & $\frac{1}{16}$ & $\frac{1}{8}$ & $\frac{1}{32}$ & $\frac{1}{32}$\\[5pt]
3 & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$\\[5pt]
4 & $\frac{1}{4}$ & 0 & 0 & 0\\[5pt]
\bottomrule
\end{tabular}
\end{table}

  La distribución marginal de $X$ es $\left ( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8} \right )$ y la distribución marginal de $Y$ es $\left ( \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4} \right )$. Calculamos las entropías de las variables marginales:
  \begin{align*}
    H(X) &= - \frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} - 2 \frac{1}{8} \log \frac{1}{8} = -\frac{1}{2} - \frac{1}{2} - \frac{6}{8} = \frac{7}{4} \text{ bits}.\\
    H(Y) &= - 4 \frac{1}{4} \log \frac{1}{4} = 2 \text{ bits}.
  \end{align*}
  Calculamos la entropía conjunta:
  \begin{align*}
    H(X,Y) &= - \frac{1}{4} \log \frac{1}{4} - \frac{2}{8} \log \frac{1}{8} - \frac{6}{16} \log \frac{6}{16} - \frac{4}{32} \log \frac{1}{32}\\
    &= \frac{1}{2} + \frac{3}{4} + \frac{6}{4} + \frac{5}{8} = \frac{27}{8} \text{ bits}.
  \end{align*}

  Vemos que, efectivamente, la entropía conjunta es mayor que cada una de las individuales, pero menor que su suma $\left( \frac{30}{8} \right )$. Calculamos las entropías condicionales:
  \begin{align*}
    H(Y|X) = &- \frac{1}{8} \log \frac{1}{4} - \frac{2}{16} \log\frac{1}{8} - \frac{1}{4} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\
    &- \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2} - \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2}\\
    =& \frac{1}{2} + \frac{6}{8} + \frac{3}{8} = \frac{13}{8} \text {bits}.\\
    H(X|Y) = & - \frac{1}{8} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4}- \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\
    &- \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4} - \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4}\\
    =& \frac{1}{4} + \frac{6}{8} + \frac{3}{8} = \frac{11}{8} \text{ bits}.
  \end{align*}
  Observamos que las entropías condicionales son positivas y menores que las entropías individuales, además $H(Y|X) \neq H(X|Y)$. Comprobamos que se verifica el Teorema \ref{t:regla_cadena}:
  \begin{align*}
    H(X,Y) &= H(X) + H(Y|X) = \frac{7}{4} + \frac{13}{8} = \frac{27}{8} \text{ bits}.\\
    H(X,Y) &= H(Y) + H(X|Y) = 2 + \frac{11}{8} = \frac{27}{8} \text{ bits}.
  \end{align*}
\end{example}

Pasamos a definir otra medida de información, la entropía relativa, esta será una medida de la distancia entre dos distribuciones. La podemos ver como una medida de ineficiencia al asumir que la distribución es $q$ cuando en realidad es $p$.

\begin{definition}[Entropía relativa]
  La entropía relativa (o ``distancia'' Kullback Leibler) entre dos funciones masa de probabilidad $p(x)$ y $q(x)$ se define como\[
D(p||q) = \sum_{x \in \mathcal{X}}p(x) \log \frac{p(x)}{q(x)} = E_p \log \frac{p(X)}{q(X)}.
  \]
\end{definition}
Las posibles indeterminaciones en la fórmula anterior se definen como: $0 \log \frac{0}{0} = 0$ y $p \log \frac{p}{0} = \infty$.

\begin{lemma}\label{l:ent_rel_pos}
  Para dos funciones masa de probabilidad $p(x)$ y $q(x)$ se tiene \[
  D(p||q) \ge 0,\]
  con igualdad si, y solo si, $p(x) = q(x) \quad \forall x \in \mathcal{X}$.
\end{lemma}
\begin{proof}
  Usando la definición de entropía relativa, se obtiene el resultado aplicando el Lema \ref{l:gibbs}.
\end{proof}

La entropía relativa no es realmente una distancia, ya que no es simétrica ni verifica la desigualdad triangular. Sin embargo, a veces es útil pensar en ella como una medida de distancia entre distribuciones.

\begin{example}
  Sea $\mathcal{X} = \{0,1\}$ y consideramos dos distribuciones $p$ y $q$ en $\mathcal{X}$. Tomamos $p(0) = 1-r$, $p(1) = r$ y $q(0) = 1-s,\ q(1) = s$.
  \begin{align*}
    D(p \Vert q) &= \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = (1 - r) \log \frac{1-r}{1-s} + r \log \frac{r}{s}.\\
    D(q \Vert p) &= \sum_{x \in \mathcal{X}}q(x) \log \frac{q(x)}{p(x)} = (1-s) \log \frac{1-s}{1-r} + s \log \frac{s}{r}.
  \end{align*}

  Si tuviéramos $r=s$, entonces $D(p \Vert q) = 0 = D(q \Vert p)$.
  Tomando $r = \frac{1}{2}$ y $s = \frac{1}{4}$ comprobaremos que no se da la simetría de la entropía relativa.
  \begin{align*}
    D(p \Vert q) &= \frac{1}{2} \log \frac{1/2}{3/4} + \frac{1}{2} \log \frac{1/2}{1/4} = 1 - \frac{1}{2} \log 6 + \frac{1}{2} = 1 - \frac{1}{2} \log 3 = 0.2075 \text{ bits}.\\
    D(q \Vert p) &= \frac{3}{4} \log \frac{3/4}{1/2} + \frac{1}{4} \log \frac{1/4}{1/2} = \frac{3}{4} \log 6 - \frac{3}{2} - \frac{1}{4} = -1 + \frac{3}{4} \log 3 = 0.1887 \text{ bits}. % \frac{2}{4} - \frac{3}{2} + \frac{3}{4} \log 3 =
  \end{align*}
Luego, $D(p\Vert q) \ne D(q \Vert p)$.
\end{example}

\begin{lemma}
  Si $X$ es una variable aleatoria con alfabeto $\mathcal{X}$, entonces \[
  H(X) \leq \log{\left\Vert\mathcal{X}\right\Vert}, 
  \]
donde $\left\Vert\mathcal{X}\right\Vert$ representa al cardinal de $\mathcal{X}$.
\end{lemma}
\begin{proof}
  Sea $u(x) = \frac{1}{\left\Vert\mathcal{X}\right\Vert}$ la función masa de probabilidad uniforme sobre $\left\Vert\mathcal{X}\right\Vert$, y sea $p(x)$ la función masa de probabilidad de $X$. Entonces,
  
 \begin{align*}
   D \left ( p \Vert u \right ) &= \sum_{x \in \mathcal{X}}
   p(x) \log \frac{p(x)}{u(x)} = \sum_{x \in \mathcal{X}} p(x) \log p(x) - \sum_{x \in \mathcal{X}} p(x) \log \frac{1}{\left\Vert\mathcal{X}\right\Vert}\\
  &= -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
 \end{align*}
 
    Usando el Lema \ref{l:ent_rel_pos}, obtenemos el resultado: \[
0 \leq D\left ( p \Vert u \right ) = -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
    \]
\end{proof}

\subsection{Información mutua}

La información mutua es una medida de la cantidad de información que una variable aleatoria contiene sobre otra. Se define como sigue.

\begin{definition}[Información mutua]
  Dadas dos variables aleatorias $X$ e $Y$ discretas con función masa de probabilidad conjunta $p(x,y)$ y funciones masa de probabilidad marginales $p(x)$ y $p(y)$. La información mutua, representada por $I(X;Y)$, es la entropía relativa entre la distribución conjunta y la distribución producto $p(x)p(y)$.
  \begin{align*}
  I(X;Y) &= D \left ( p(x,y) \Vert p(x)p(y) \right ) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\ &= E_{p(x,y)} \log \frac{p(X,Y)}{p(X)p(Y)}.
  \end{align*}
\end{definition}

Notamos que la información mutua, al estar definida como una entropía relativa, será no negativa (Lema \ref{l:ent_rel_pos}).

\begin{theorem}[Relación entre entropía e información mutua]\label{t:ent_im}
  La siguiente lista recoge algunas relaciones entre la entropía y la información mutua.
\begin{enumerate}[label={\alph*)}]
  \item $I(X;Y) = H(X) - H(X|Y)$.
  \item $I(X;Y) = H(Y) - H(Y|X)$.
  \item $I(X;Y) = H(X) + H(Y) - H(X,Y)$.
  \item $I(X;Y) = I(Y;X)$.
  \item $I(X;X) = H(X)$, la entropía es un caso particular de la información mutua.
  \end{enumerate}
\end{theorem}
\begin{proof}
   
  \begin{align*}
    \text{a) }I(X;Y) &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)} = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(y) p(x|y) \log \frac{p(y)p(x|y)}{p(x)p(y)}\\
    &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(x|y) - \sum_{x \in \mathcal{X}}\underbrace{\left ( \sum_{y \in \mathcal{Y}} p(y) p(x|y) \right)}_{p(x)} \log p(x)\\
    &= - H(X|Y) + H(X).
  \end{align*}
  b) Se obtiene de forma análoga.\\
  c) Se prueba aplicando la regla de la cadena sobre $H(X|Y)$, $I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)$.\\
  d) Se prueba usando el apartado c).\\
  e) $I(X;X) = H(X) - H(X|X) = H(X)$.
\end{proof}

\begin{example}
  Para las distribuciones consideradas en el Ejemplo \ref{e:dist_conj}, calculamos la información mutua:
  \begin{align*}
    I(X;Y) =& \frac{1}{8} \log \frac{1/8}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4}\\
    +& \frac{1}{8} \log \frac{1/8}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4}\\
    +& \frac{2}{16} \log \frac{1/16}{1/8 \cdot 1/4} + \frac{1}{4} \log \frac{1/4}{1/2 \cdot 1/4} = \frac{2}{16} \log \frac{1}{2} + \frac{1}{2} \log 2 = \frac{3}{8} \text{ bits}.
  \end{align*}

  Vemos que se cumplen las propiedades del Teorema \ref{t:ent_im}:
  \begin{align*}
    H(X) -H(X|Y) &= \frac{7}{4} - \frac{11}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(Y) -H(Y|X) &= 2 - \frac{13}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(X) + H(Y) - H(X,Y) &= \frac{7}{4} + 2 - \frac{27}{8} = \frac{3}{8} \text{ bits} = I(X;Y).
  \end{align*}
  
\end{example}

\subsection{Caso continuo}
--- Este apartado se puede agrandar si fuera de interés para incluir todas las definiciones del apartado anterior y algunas propiedades. Me limito a definir las definiciones utilizadas en el apartado de estimación.

Hasta ahora las definiciones dadas eran para variables aleatorias discretas, que solo tomaban un conjunto finito de valores. Podemos extender las definiciones estudiadas al caso continuo, para aplicarlas en variables aleatorias continuas. Trabajaremos en este caso con una variable aleatoria $X$ con función de distribución $F(x) = Pr[X \leq x]$, si $F$ es continua, la variable aleatoria se dice continua. Sea $f(x) = F'(x)$, cuando la derivada esté definida. Si $\int_{-\infty}^{\infty}f(x)=1,\ f(x)$ se llama la función de densidad de $X$. El conjunto donde $f(x) > 0$ es el soporte de $X$.

Las definiciones a continuación incluyen integrales, por lo que deberíamos añadirles un \textit{si existe} a todas ellas. Se pueden construir ejemplos de variables aleatorias cuya función de densidad no exista, o para la que no exista la integral a calcular.

La entropía pasa a llamarse entropía diferencial y podemos definirla como sigue.

\begin{definition}[Entropía diferencial]
  La entropía diferencial $h(X)$ de una variable aleatoria continua $X$ con función de densidad $f(x)$ se define como\[
h(X) = - \int_Sf(x)\log f(x)dx,
\]
donde $S$ es el soporte de $X$.
\end{definition}
Como en el caso discreto, la entropía diferencial solo depende de la probabilidad de la variable aleatoria, no de los valores que toma. 

\begin{example} Consideramos una variable aleatoria con una distribución uniforme sobre $[0,a]$, en este intervalo su densidad es $\frac{1}{a}$ y fuera de él es 0. La entropía es \[
h(x) = - \int_0^a\frac{1}{a}\log\frac{1}{a} = \log a.
  \]
Notamos que si $a<1, \log a < 0$, esto es, la entropía en el caso continuo puede ser negativa, no como en el caso discreto.
  
\end{example}

\begin{definition}[Información mutua]
  La información mutua, $I(X;Y)$ entre dos variables aleatorias con función de densidad conjunta $f(x,y)$ viene dada por\[
I(X;Y) = \int \int f(x,y) \log \frac{f(x,y)}{f(x)f(y)}dxdy.
  \]
\end{definition}

\begin{lemma}\label{l:ent_im}
  La información mutua de dos variables aleatorias $X$, $Y$ con funciones de densidad $f(x)$, $f(y)$ y función de densidad $f(x,y)$ conjunta verifican:\[
  I(X;Y) = h(X) + h(Y) - h(X,Y).
  \]
\end{lemma}

\begin{proof}
  Para realizar la prueba partiremos de la definición de información mutua y utilizaremos propiedades del logaritmo y el Teorema de Fubini para llegar al resultado.
  \begin{align*}
    I(X;Y) &= \int \int f(x,y) \log \frac{f(x,y)}{f(x)f(y)}dxdy \\
    &= \int \int f(x,y) \log f(x,y)dxdy - \int \int f(x,y) \log f(x)f(y)dxdy\\
    &= -h(x,y)-\int\int f(x,y) \log f(x) dy dx - \int \int f(x,y)\log f(y)dxdy\\
    &= -\int f(x) \log f(x)dx - \int f(y)\log f(y) dy -h(x,y)\\
    &= h(x) + h(y) - h(x,y).
  \end{align*}
\end{proof}

\subsection{Estimación de la información mutua}

Generalmente no conocemos las funciones de densidad de las variables aleatorias cuya información mutua queremos calcular, por ello, buscamos aproximar este valor a partir de la información que sí tenemos. El objetivo es estimar $I(X;Y)$ a partir de un conjunto $\{z_i=(x_i,y_i) : i = 1,\cdots, N\}$ sin conocer las densidades $f=f_z, f_x$ y $f_y$.

Nos centraremos en los estimadores de Kozachenko - Leonenko, aunque presentaremos brevemente otras dos posibles estimaciones.

\subsubsection{Forma más extendida de estimar}

Particionamos el soporte de $X$ e $Y$ en \textit{contenedores} de tamaño finito (no necesariamente el mismo) y aproximamos la información mutua por \[
I(X,Y) \approx I_{\textit{binned}}(X,Y) = \sum_{i,j} p(i,j) \log \frac{p(i,j)}{p_x(i)p_y(j)},
\]
donde $p_x(i) = \int_i f_x(x)dx$, $p_y(j) = \int_j f_y(y)dy$, $\int_i$ representa la integral sobre el i-ésimo contenedor, y $p(i,j) = \int_i\int_jf(x,y)dxdy$.

Notamos $n_x(i)$ al número de puntos en el i-ésimo contenedor de $X$, análogamente $n_y(j)$. $n(i,j)$ es el número de puntos en la intersección de $n_x(i)$ y $n_y(j)$.

Lo que hacemos es aproximar $p_x(i) \approx \frac{n_x(i)}{N}, p_y(j) \approx \frac{n_y(j)}{N}, p(i,j) \approx \frac{n(i,j)}{N}$.

Si todas las densidades son funciones \textit{proper}, cuando $N$ tienda a infinito y el tamaño de cada contenedor converja a cero, la estimación $I_{\textit{binned}}(X,Y) \to I(X;Y)$.

---Con \textit{proper function} entiendo que el artículo hace referencia a esto: \url{https://en.wikipedia.org/wiki/Proper\_convex\_function}. Es un concepto que no he visto y no sé cómo traducir.

\subsubsection{Estimar la entropía a partir de estadísticos de los $k$ vecinos más cercanos}

Si $X$ es una variable aleatoria unidimensional, podemos ordenar los distintos $x_i$ (realizaciones muestrales de $X$). Si, además, $x_{i+1} -x_i \to 0$ y $N\to \infty$, aproximamos:\[
H(X)\approx \frac{1}{N-1}\sum_{i=1}^N \log(x_{i+1}-x_i) + \psi(1) - \psi(N),\]
donde $\psi(x) = \frac{\Gamma'(x)}{\Gamma(x)}$ es la \textit{función digamma}. La función digamma verifica las siguientes propiedades:
\begin{itemize}
\item $\psi(x+1) = \psi(x) + \frac{1}{x}$.
\item $\psi(1) = -C = 0.5772156\dots$, la constante de Euler - Mascheroni.
  \item Para $x$ lo suficientemente grandes, $\psi(x) \approx \log x - \frac{1}{2x}$.
\end{itemize}

La estimación de la entropía propuesta es para $k=1$, existen fórmulas similares que utilizan $x_{i+k}-x_i$  en vez de $x_{i+1}-x_i$, con $k < N$.

Aunque la estimación dada, y sus generalizaciones a $k>1$, parezca ser la mejor para la entropía, $H(X)$, no sirve para calcular la información mutua, porque no es obvio cómo generalizarla a más dimensiones.

\subsubsection{Estimadores de Kozachenko - Leonenko}

Asumimos que tenemos unas métricas dadas para los espacios $X$, $Y$, $Z = (X,Y)$. Para cada punto $z_i = (x_i, y_i)$ sus vecinos según la distancia $d_{i,j}=\Vert z_i-z_j \Vert$ serían $d_{i,j_1} \leq d_{i,j_2} \leq \cdots$. Podríamos obtener los vecindarios de $X$ e $Y$ de manera similar.

Estimaremos la entropía de una variable $X$ promediando la distancia media de los $k$ vecinos más cercanos de todos los $x_i$. De esta forma, estimaremos $H(X), H(Y)$ y $H(X,Y)$. A continuación, estimaremos la información mutua utilizando el Lema \ref{l:ent_im}.\\

Sea $X$ una variable aleatoria continua con valores en un espacio métrico, es decir, hay definida una distancia, $\Vert x - x'\Vert$, entre dos realizaciones de $X$, y sea $f(x)$ la función de densidad una función $proper$. Nuestro objetivo es estimar la entropía $H(X)$ a partir de una muestra aleatoria simple $(x_1,\cdots,x_N)$ de $N$ realizaciones de $X$.

La ecuación de la entropía, $H(X) = - \int f(x) \log f(x) dx$, se podría entender (salvo el signo menos) como una media de $\log f(x)$. Si tuviéramos un estimador insesgado, $\reallywidehat{\log f}(x)$, del mismo, podríamos obtener un estimador insesgado de la entropía:\[
\widehat{H}(x) = - \frac{1}{N} \sum_{i=1}^N\reallywidehat{\log f}(x_i).\]

Para obtener el estimador $\reallywidehat{\log f}(x_i)$ consideramos la distribución de probabilidad $P_k(\varepsilon)$ de la distancia entre $x_i$ y su $k$-ésimo vecino más cercano. La probabilidad $P_k(\varepsilon)d\varepsilon$ ---no entiendo por qué aquí añade el $d\varepsilon$--- es igual a la probabilidad de que haya un punto a distancia $r\in \left[ \frac{\varepsilon}{2}, \frac{\varepsilon}{2} + \frac{d\varepsilon}{2} \right ]$ de $x_i$, de que haya otros $k-1$ puntos a menor distancia; y de que $N-k-1$ puntos se encuentren a mayor distancia de $x_i$.

Llamamos $p_i$ a la función masa de probabilidad  de la bola centrada en $x_i$ y radio $\varepsilon$, $p_i(\varepsilon) = \int_{\Vert s - x_i \Vert < \frac{\varepsilon}{2}}f(s)ds$. 

Usaremos la fórmula de la distribución multinomial para expresar $P_k(\varepsilon)d\varepsilon$. Recordamos que la distribución multinomial nos da la probabilidad de un número de éxitos $k$ en $N$ sucesos de Bernouilli (estos sucesos toman valor 1 para la probabilidad de éxito  y 0 para la de fracaso) independientes y equiprobables, tiene como función de densidad $\frac{N!}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}$. Luego,\[
P_k(\varepsilon)d\varepsilon = \frac{(N-1)!}{1!(k-1)!(N-k-1)!} \cdot \frac{d p_i(\varepsilon)}{d\varepsilon} d\varepsilon \cdot p_i^{k-1} \cdot (1-p_i)^{N-k-1}.
\]

--- En la ecuación anterior no entiendo el término $ \frac{d p_i(\varepsilon)}{d\varepsilon} $, entiendo que tiene que ser la probabilidad de que haya un punto justo a distancia $\varepsilon$ de $x_i$ pero no sé por qué se calcula así---

A partir de ahí, integrando, calculamos,\[
P_k(\varepsilon) = k\binom{N-1}{k}
\cdot \frac{d p_i(\varepsilon)}{d\varepsilon} \cdot p_i^{k-1} \cdot (1-p_i)^{N-k-1}.
\]

Se comprueba que $\int P_k(\varepsilon)d\varepsilon = 1$ ---no me salió ---, que debe cumplir para ser función de distribución.

Podemos calcular la esperanza de $\log p_i (\varepsilon)$, a partir de la posición de los $N-1$ puntos restantes, con $x_i$ fijo:

--- No sé por qué se verifican las igualdades siguientes---
\begin{align*}
\mathbb{E}(\log p_i) &= \int_0^\infty P_k(\varepsilon) \log p_i(\varepsilon) d\varepsilon =  k\binom{N-1}{k} \int_0^1 p^{k-1}(1-p)^{N-k-1}\log p\ dp\\ &= \psi (k) - \psi(N).
\end{align*}

Obtenemos un estimador de $\log f(x)$ asumiendo que $f(x)$ es constante en la bola de radio $\varepsilon$. --- ¿por qué es necesario asumir esto?, ¿cómo obtiene la aproximación siguiente?--- Esto nos da $p_i (\varepsilon) \approx c_d \varepsilon^d f(x_i)$, donde $d$ es la dimensión de $X$ y $c_d$ es el volumen de la bola unidad $d$-dimensional. La constante $c_d$ depende de la norma, para la del máximo tenemos $c_d=1$, mientras que para la euclídea $c_d=\frac{\pi^{d/2}}{\Gamma\left ( 1+ \frac{d}{2} \right ) / 2^d}$.

Tomando logaritmos sobre la aproximación de $p_i(\varepsilon)$, obtenemos:\[
\log p_i(\varepsilon) \approx \log c_d + d \log \varepsilon + \log f(x_i)
\]
Continuamos tomando esperanzas respecto de $\varepsilon$ y sustituyendo la esperanza del logaritmo ya calculada:\[
\log f(x_i) \approx \mathbb{E}(\log p_i) - \log c_d  - d \mathbb{E} (\log \varepsilon) = \psi (k) - \psi (N) - \log c_d  - d \mathbb{E} (\log \varepsilon).
\]

Hemos calculado el estimador $\reallywidehat{\log f} (x_i)$, sustituiremos de la forma propuesta para obtener un estimador de la entropía:
\begin{align}
  \widehat{H}(X) &= - \frac{1}{N} \sum_{i=1}^N\reallywidehat{\log f} (x_i) = - \frac{1}{N} \sum_{i=1}^N\left (\psi (k) - \psi (N) - \log c_d  - d\log \varepsilon \right ) \nonumber \\
  &= - \psi (k) + \psi (N) + \log c_d + \frac{d}{N} \sum_{i=1}^N \log \varepsilon (i), \label{eq:est_ent}
\end{align}
donde $\varepsilon(i)$ es dos veces la distancia de $x_i$ a su $k$-ésimo vecino más cercano.
---En la ecuación anterior no sé por qué desaparece la esperanza---


Por cómo lo hemos obtenido, $\widehat{H}(X)$ será insesgado si la densidad $f(x)$ es estrictamente constante. --- Esto no sé por qué ---

Consideremos ahora la variable aleatoria conjunta $Z=(X,Y)$ con la norma del máximo: $\Vert z - z' \Vert = \max \{\Vert x-x' \Vert, \Vert y-y'\Vert\}$, donde las normas en $X$ e $Y$ pueden ser cualesquiera, no necesariamente la misma.

Tomamos uno de los $N$ puntos, $z_i$, y notamos $\frac{\varepsilon(i)}{2}$ la distancia de este punto a su $k$-ésimo vecino. $\frac{\varepsilon_x(i)}{2}$ y $\frac{\varepsilon_y(i)}{2}$ reflejan la misma distancia proyectada en el subespacio $X$ e $Y$ respectivamente. $\varepsilon(i) = \max \{\varepsilon_x(i), \varepsilon_y(i)\}$.

Distinguiremos dos algoritmos diferentes para estimar la información mutua según si consideramos los vecinos que se encuentren en el cuadrado de lado $\varepsilon(i)$, o en el rectángulo de lados $\varepsilon_x(i)$, $\varepsilon_y(i)$. 

\paragraph{Primer algoritmo}

Este algoritmo se caracteriza por definir $n_x(i)$ como el número de puntos $x_j$ cuya distancia a $x_i$ es menor que $\frac{\varepsilon(i)}{2}$, de forma análoga se define $n_y(i)$. Como $\varepsilon(i)$ es una variable aleatoria, $n_x(i),\ n_y(i)$ también lo son. $\frac{\varepsilon(i)}{2}$ es una variable aleatoria con distribución dada por $P_k(\varepsilon)$, luego $\mathbb{E}(\log p_i) = \psi(k) - \psi(N)$ se cumple también en este caso. Para utilizar el estimador de la entropía calculado anteriormente debemos cambiar: $x_i$ por $z_i=(x_i,y_i)$, $d$ por $d_X,\ d_Y$ y $c_d$ por $c_{d_X}c_{d_Y}$. Con estas modificaciones obtenemos\[
\widehat{H}(X,Y) = - \psi(k) + \psi(N) + \log (c_{d_X}c_{d_Y}) + \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i)).
\]

Para obtener $I(X;Y)$ tenemos que restar este valor a las estimaciones de $H(X)$ y $H(Y)$. Podríamos usar el mismo $k$ para realizar estas estimaciones, pero entonces usaríamos diferentes escalas en el espacio conjunto que en los marginales. Para un $k$ fijo, la distancia al $k$-ésimo vecino en el espacio conjunto sería mayor que las distancias a los vecinos en los espacios marginales. El sesgo en el estimador de la entropía marcado por la no uniformidad de la densidad depende de estas distancias, los sesgos en $\widehat{H}(X)$, $\widehat{H}(Y)$, $\widehat{H}(X,Y)$ no se cancelarían. Para evitar eso, notamos que la estimación se cumple para cualquier valor de $k$ y que no tenemos porqué mantenerlo fijo.

Suponiendo que el $k$-ésimo vecino de $x_i$ se encuentra en uno de los lados verticales del cuadrado de lado $\varepsilon(i)$, si hay $n_x(i)$ puntos en la recta vertical $x = x_i \pm \frac{\varepsilon(i)}{2}$, entonces $\frac{\varepsilon(i)}{2}$ es la distancia al $n_x(i) + 1$ vecino de $x_i$ y estimamos su entropía mediante\[
\widehat{H}(X) = - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log c_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon(i).
\]
---En la ecuación anterior, ¿$x$ estaría fijo?---

Para la variable $Y$ podría no cumplirse exactamente así, ya que $\varepsilon(i)$ no tendría por qué ser dos veces la distancia al $(n_y(i)+1)$-ésimo vecino. Sin embargo, consideramos la ecuación anterior una buena aproximación para $H(Y)$ (cambiando $X$ por $Y$). Esta aproximación se vuelve exacta cuando $n_y(i) \to \infty$ y cuando $N\to \infty$.

Usando estas estimaciones en la ecuación correspondiente, obtenemos:
\begin{align*}
  I^{(1)}(X,Y) \approx & \widehat{H}(X) + \widehat{H}(Y) - \widehat{H}(X, Y)\\ =& - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log c_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & - \frac{1}{N}\sum_{i=1}^N\psi(n_Y(i)+1) + \psi(N) + \log c_{d_Y} + \frac{d_Y}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & + \psi(k) - \psi(N) - \log (c_{d_X}c_{d_Y}) - \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i))\\
  =& \psi(k) - \frac{1}{N}\sum_{i=1}^N\left( \psi(n_x(i)+1) + \psi(n_y(i)+1) \right ) + \psi(N).
\end{align*}

Notamos $\langle \cdots \rangle$ a la media sobre todos los $i\in\{1,\cdots,N\}$ y sobre todas las realizaciones de las muestras aleatorias,\[
\langle \cdots \rangle = \frac{1}{N} \sum_{i=1}^N\mathbb{E}[\cdots(i)].
\]

Utilizando esta notación la estimación de la entropía quedaría:\[
I^{(1)}(X,Y) \approx \psi(k) - \langle \psi(n_x+1) + \psi(n_y+1) \rangle + \psi(N).
\]
--- No entiendo cómo aplicamos $\langle \cdots \rangle$, ¿no faltaría una esperanza?---

El principal inconveniente de esta estimación es que solo usamos el estimador de Kozachenko - Leonenko correctamente en una dirección marginal. Esto parece inevitable si queremos usar (hiper)cubos en el espacio conjunto, como opción alternativa podríamos usar (hiper)rectángulos.

\paragraph{Segundo algoritmo} En este caso, $n_x(i)$ y $n_y(i)$ serán el número de puntos con $\Vert x_i - x_j \Vert \leq \frac{\varepsilon_x(i)}{2}$ y $\Vert y_i - y_j \Vert \leq \frac{\varepsilon_y(i)}{2}$ respectivamente. Tenemos que distinguir dos casos:

\begin{enumerate}[label={\alph*)}]
\item Los lados $\varepsilon_x(i)$ y $\varepsilon_y(i)$ quedan determinados por el mismo punto.
\item Los lados $\varepsilon_x(i)$ y $\varepsilon_y(i)$ quedan determinados por puntos diferentes.
\end{enumerate}

En ambos casos tenemos que cambiar $P_k(\varepsilon)$ por una probabilidad 2-dimensional:\[
P_k(\varepsilon_x, \varepsilon_y) = P_k^{a)}(\varepsilon_x, \varepsilon_y) + P_k^{b)}(\varepsilon_x, \varepsilon_y),
\]
---Esto no sé por qué---
con \[
P_k^{a)}(\varepsilon_x, \varepsilon_y) = \binom{N-1}{k} \frac{d^2[q_i^k]}{d\varepsilon_x d \varepsilon_y}(1-p_i)^{N-k-1}
\]
y\[
P_k^{b)}(\varepsilon_x, \varepsilon_y) = (k-1)\binom{N-1}{k} \frac{d^2[q_i^k]}{d\varepsilon_x d \varepsilon_y}(1-p_i)^{N-k-1},
\]
donde $q_i \equiv q_i(\varepsilon_x, \varepsilon_y)$ es la función de densidad del rectángulo de lados $\varepsilon_x \times \varepsilon_y$ centrado en el punto $(x_i, y_i)$, y $p_i$ es la función de densidad del cuadrado de lado $\varepsilon = \max \{\varepsilon_x, \varepsilon_y\}$. Esta última es necesario porque usando la norma del máximo garantizamos que no hay puntos en ese cuadrado que no estén en el cuadrado.

Se verifica que $\int P_k =1$. Calculamos la esperanza de $\log p_i$:
---No sé por qué se verifican las igualdades siguientes---
\[
\mathbb{E}(\log q_i) = \int \int_0^{\infty} P_k(\varepsilon_x, \varepsilon_y) \log q_i(\varepsilon_x, \varepsilon_y) d \varepsilon_x d\varepsilon_y
= \psi(k) - \frac{1}{k} - \psi(N)
\]
--- ¿Se cumple $pi\approx c_d\varepsilon^df(x)$ y podemos proceder de forma similar al caso anterior?---

La información mutua se aproximaría por\[
I^{(2)}(X,Y) \approx \psi(k) - \frac{1}{k} - \langle \psi(n_x) + \psi(n_y) \rangle + \psi(N).
\]


En general ambas fórmulas dan resultados similares. Para un mismo $k$, $I^{(1)}$ da ligeramente menores errores estadísticos (porque $n_x(i)$ y $n_y(i)$ tienden a ser más grandes y tener menores variaciones relativas), pero tienen mayores errores sistemáticos. Este último es grave solo si estamos interesados en variables con dimensiones muy altas, donde $\varepsilon(i)$ tiende típicamente a ser más grande que la marginal $\varepsilon_x(i)$. En ese caso parece preferible el segundo algoritmo, de resto se pueden usar los dos.

\section{Análisis de implementaciones}

En esta sección estudiaremos un par de implementaciones de conceptos anteriores como la entropía o la estimación mutua. En general, dado un número finito de muestras de una variable aleatoria no podemos calcular de forma exacta la entropía o información mutua, ya que no conocemos su distribución. Para implementarlas es necesario usar algún estimador, como los estudiados en el apartado anterior.

\subsection{Implementación 1 - GaelVaroquaux}

Comenzaremos revisando la implementación \cite{estimating}. Para ello leeremos el código y nos formaremos una idea de qué hace y cómo cada una de las funciones.

\begin{itemize}
\item \texttt{nearest\_distances}: para cada punto de un vector \texttt{X}, devuelve la distancia a su $k$-ésimo vecino más cercano. Se implementa haciendo uso de las funciones de \texttt{scikit-learn}.

\item \texttt{entropy\_gaussian}: entropía de una variable gaussiana, se utilizará para hacer \textit{tests}. Si la matriz de covarianzas \texttt{C} es escalar, se utiliza la siguiente fórmula para la entropía:\[
  H(X) = \frac{1}{2}\left ( 1 + \log(2\pi) + \frac{1}{2} \log C\right ).
  \]
% REVIEW
  En el Thomas dice que la entropía de la gaussiana con covarianza $\sigma^2$ es:\[
h(X) = \frac{1}{2}\log (2\pi e \sigma^2 ).
\]

Sabemos que $cov(X,X) = \sigma^2$, entonces no me cuadra el 1/2 que utilliza en el cálculo.

Para el caso en que la matriz \texttt{C} sea no degenerada, utiliza la expresión:\[
H(X_1,\cdots, X_n) = \frac{n}{2} \left (1 + \log(2\pi) \right) + \frac{1}{2}\log |C|,
\]

Coincidiendo con la fórmula del Thomas salvo porque hace el valor absoluto del determinante.

\item \texttt{entropy}: calcula la entropía de la variable \texttt{X} usando el estimador de Leonenko - Kozachenko \ref{eq:est_ent}. Comienza inicializando los elementos que se necesitan para aplicar la fórmula: vecinos más cercanos para todos los elementos del vector, dimensiones de \texttt{X}, volumen de la bola unidad ($c_d$). Notamos que en el cálculo de la constante $c_d$ (\texttt{volume\_unit\_ball}) falta un $\frac{1}{2^d}$. Otra diferencia es que en el estimador propuesto, $\varepsilon$ era dos veces la distancia del punto al vecino, mientras que en \texttt{r} tenemos la distancia del punto a su vecino más cercano una sola vez. Donde el estimador estudidado ponía $\varepsilon$ en esta implementación pone \texttt{r + eps}, donde \texttt{eps} es el menor valor que al ser sumado varía el resultado (de forma que  \texttt{1+eps $\neq$ 1}). Este detalle, presente en ambas implementaciones, se debe a que al tomar muestras empíricas se suelen usar pocos valores, lo que podría generar un conjunto en el que varios puntos tengan las mismas coordenadas y los números $n_x(i),\ n_y(i)$ no serían únicos (rompiendo la hipótesis de qe la muestra está distribuida de manera continua), para solucionarlo se añade un pequeñor error en los datos. Devuelve el resultado de sustituir estos valores en la fórmula \ref{eq:est_ent}.

\item \texttt{mutual\_information}: devuelve la información mutua entre cualquier número de variables. Para ello suma las entropías marginales de cada una de ellas y les resta la entropía conjunta (usando el mismo valor $k$ para ambas estimaciones). % REVIEW: Referenciar lema correspondiente

\item \texttt{mutual\_information\_2d}: calcula la información mutua entre dos variables unidimensionales a partir de su histograma conjunto. 

\item \texttt{test\_entropy}: permite probar la entropía comparando la entropía de una variable aleatoria gaussiana, que conocemos teóricamente y se calcula en \texttt{entropy\_gaussian}, con la entropía estimada por \texttt{entropy}. Si ambos valores difieren demasiado, saltará un \textit{assert}.

\item \texttt{test\_mutual\_information}: permite probar la información mutua de una variable aleatoria 2 dimensional estimando los resultados para una variable gaussiana y comparándolos con los resultados teóricos.

\item \texttt{test\_degenerate}: comprueba que los estimadores no dan valores degenerados.

\item \texttt{test\_mutual\_information\_2d}: es una función similar a \texttt{test\_mutual\_information}, pero para probar \texttt{mutual\_information\_2d}.
\end{itemize} 

% REVIEW
Si ejecutamos el código tal cual está, se producen como salida dos parejas. La primera es la estimación de la información mutua entre 2 variables normales correladas usando la función \texttt{mutual\_information}. La segunda muestra lo mismos resultados para la función \texttt{mutual\_information\_2d}.
Resultados: (0.1092329232318674, 0.11157177565710485)
(2.1173231240729757, 2.2033596236321267)

Añadimos a esta tupla la diferencia entre ambos valores:
(0.1092329232318674, 0.11157177565710485, 0.0023388524252374587)
(2.1173231240729757, 2.2033596236321267, 0.08603649955915094)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%       REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography

\end{document}
