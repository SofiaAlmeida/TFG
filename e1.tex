% article example for classicthesis.sty
\documentclass[10pt,a4paper]{article} % KOMA-Script article scrartcl
\usepackage[spanish]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{lipsum}
\usepackage{url}

% Cargamos amsmath antes que classicthesis que si no tenemos fallos de compilación
\usepackage{amsmath}
\usepackage[nochapters]{./classicthesis} % nochapters

% fuente del mathbb
\AtBeginDocument{%
  \let\mathbb\relax
  \DeclareMathAlphabet{\mathbb}{U}{msb}{m}{n}%
}


%%% Matemáticas
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[Symbol]{upgreek} %pi
\usepackage{mathtools}
%\allowdisplaybreaks
\AtBeginDocument{\renewcommand{\epsilon}{\varepsilon}}
\AtBeginDocument{\newcommand{\R}{\mathbb{R}}}
\AtBeginDocument{\newcommand{\E}{\mathbb{E}}}

\newcommand{\bias}{\text{bias}}
\newcommand{\Var}{\text{Var}}
\newcommand{\olsi}[1]{\,\overline{\!{#1}}} % overline short italic
%%% Tablas
\usepackage{tabularx}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{diagbox}
\renewcommand\spanishtablename{Tabla}

%%% Código
%\usepackage{listingsutf8}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  %frame=single,
  breaklines=true
}

%%% Gráficas
\usepackage{graphicx} % Required for including pictures
\graphicspath{{./fig/}}

%%% Bibliografía
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{spanish}{
  urlseen = {Último acceso}
}
\addbibresource{citations.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definición}[section]

\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{corollary}{Corolario}[theorem]

\begin{document}
%-------------------------- ÍNDICE--------------------------------------
\tableofcontents
\newpage

%-------------------------INTRODUCCIÓN----------------------------------
\section{Introducción}

---Hablar de: Shanon, teoría de la información, relevancia que ha tenido,...---
\subsection{Motivación del trabajo}
\subsection{Objetivos}
---(Si se opta por no ponerlos antes de la introducción)---

%--------------------------- FUNDAMENTOS--------------------------------
\section{Funtamentos y estado del arte}

---(De lo de estado del arte comentar que es un tema que se sigue investigando y poner referencias al respecto)---

\subsection{Conceptos previos}

Antes de entrar en materia, se recogen una serie de definiciones y resultados que se utilizarán a lo largo del trabajo.

\begin{definition}[$\sigma$-álgebra]
  Sea $\Omega$ un conjunto no vacío. Una $\sigma$-álgebra $\mathcal{A}$ sobre $\Omega$ es una familia de subconjuntos de $\Omega$ que verifica:
  \begin{itemize}
  \item  El conjunto vacío, $\emptyset$, pertenece a $\mathcal{A}$,
  \item  si $A$ pertenece a $\mathcal{A}$, entonces su complementario $\Omega\ \backslash\ A$ también pertenece a $\mathcal{A}$,
 \item si tomamos una serie de conjuntos $\{A_i\}_{i\in \mathbb{N}}$ de $\mathcal{A}$, entonces su unión numerable $\cup_{i\in \mathbb{N}}A_i$ también pertenece a $\mathcal{A}$.
  \end{itemize}
  
\end{definition}

\begin{example}
  La familia de conjuntos Borel $\mathcal{A} = \mathcal{B}(\mathbb{R}^n)$ es una $\sigma$-álgebra sobre $\mathbb{R}^n$.
  %Un conjunto Borel sobre un espacio topológico es aquel formado por todos los conjuntos abiertos con las operaciones de unión numerable, intersección numerable y complementario relativo.
\end{example}

\begin{definition}[Medida de probabilidad]
  Sea $\mathcal{A}$ una $\sigma$-álgebra sobre $\Omega$. Una medida de probabilidad $P$ es una función $P:\mathcal{A} \to [0,1]$ tal que
  \begin{itemize}
  \item $P(\Omega) = 1$,
   \item si  $\{A_i\}_{i\in \mathbb{N}}$ son conjuntos de $\mathcal{A}$ disjuntos dos a dos, entonces $P(\cup_{i\in \mathbb{N}}A_i) = \sum_{i\in\mathbb{N}}P(A_i)$. 
  \end{itemize}
\end{definition}

La tupla $(\Omega, \mathcal{A}, P)$ se llama espacio de probabilidad. Los conjuntos de $\mathcal{A}$ se llaman sucesos.\\% Se dice que un suceso$A$ ocurrirá casi seguramente cuando $P(A)=1$.\\

%% \begin{definition}[Variable aleatoria]
%%   Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad. Una variable aleatoria $X$ es una función medible \[
%% X:(\Omega, \mathcal{A}) \to (\mathbb{R}, \mathcal{B}).
%% \]
%% La condición de medibilidad quiere decir que $X^{-1}(B) \in \mathcal{A}\quad \forall B\in \mathcal{B}$.\\
%% \end{definition}

%% Podemos extender esta definición y obtener el concepto de vector aleatorio.

\begin{definition}[Variable aleatoria]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad. Un variable aleatoria $X=(X_1,\cdots, X_n)$ es una función medible \[
X:(\Omega, \mathcal{A}) \to (\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n)).
\]
La condición de medibilidad quiere decir que $X^{-1}(B) \in \mathcal{A}\quad \forall B\in \mathcal{B}(\mathbb{R}^n)$.
\end{definition}

Cuando $n=1$ diremos que la variable aleatoria es unidimensional, mientras que cuando $n >1$ diremos que la variable aleatoria es multidimensional, en ocasiones también denominada, vector aleatorio. En el caso de la variable aleatoria multidimensional podremos querer información sobre ella misma, $X=(X_1,\cdots, X_n)$, lo que llamaremos variable aleatoria conjunta, o sobre alguna de las variables aleatoria $X_i$, que denominaremos variable aleatoria marginal.\\

Cuando la imagen de la variable aleatoria $X$ sea numerable, diremos que la variable aleatoria es \textit{discreta}, será descrita por la función masa de probabilidad que asigna una probabilidad a cada valor de la imagen de $X$. Por otro lado, cuando la imagen de la variable aleatoria $X$ sea infinita no numerable, diremos que la variable aleatoria es \textit{continua}. Si además puede ser descrita por una función de densidad, será \textit{absolutamente continua}.\\

Cada variable aleatoria nos da una medida de probabilidad, su probabilidad inducida.

\begin{definition}[Probabilidad inducida]
  La probabilidad inducida por un variable aleatoria $X=(X_1,\cdots, X_n)$, o distribución de $X$, se define como la función \[
P_X[B]:= P[X^{-1}(B)],
  \]
para todo $B \in \mathcal{B}(\mathbb{R}^n)$.
\end{definition}

\begin{definition}[Función de distribución]
  La función de distribución de la variable aleatoria unidimensional $X$ es la aplicación $F_X:\mathbb{R}\to[0,1]$ definida como:\[
F_X(x) = P[X\leq x].
  \]
Para una variable aleatoria multidimensional,  $X=(X_1,\cdots,X_n)$, la función de distribución es $F_X:\mathbb{R}^n\to[0,1]$ y está dada por\[
F_X(x_1,\cdots,x_n) = P[X_1\leq x_1,\cdots, X_n\leq x_n].
  \]
\end{definition}

%La distribución de $X$ queda completamente determinada por su distribución.

\begin{proposition}
  La función de distribución $F_X$ de la variable aleatoria $X$ es no decreciente, continua a la derecha y $\lim_{x\to - \infty}F_X(x) = 0$, $\lim_{x\to +\infty}F_X(x) = 1$.\\
  Si la variable aleatoria $X$ es discreta, la función de distribución tendrá puntos de discontinuidad es los valores de la imagen de $X$. Mientras que si la variable aleatoria es continua, su función de distribución será continua.
\end{proposition}

\begin{definition}[Función Borel]
  Una función $f:\mathbb{R}^n\to \mathbb{R}^n$ será Borel si la imagen inversa $f^{-1}(B)$ de cualquier conjunto Borel $B\in \mathbb{R}^n$ es un conjunto Borel.
\end{definition}

\begin{definition}[Función de densidad y función masa de probabilidad]
  Si hay una función Borel $f_X:\mathbb{R}^n\to \mathbb{R}$ tal que para cualquier conjunto Borel $B\subset \mathbb{R}$\[
P[X\in B] = \int_B f_X(x) dx,
\]
se dice que $X$ es una variable aleatoria con distribución absolutamente continua y $f_X$ se llama función de densidad de $X$. Si hay una secuencia (finita o infinita) de vectores $x_1,x_2,\cdots$ tal que para cualquier conjunto Borel $B\subset \mathbb{R}^n$\[
P[X\in B] = \sum_{x_i\in B}P[X = x_i],
\]
entonces decimos que $X$ tiene una distribución discreta con valores $x_1,x_2\cdots$ y función masa de probabilidad $P[X=x_i]=p(x_i)$ en $x_i$.
\end{definition}

\begin{definition}[Soporte]
Llamamos soporte de una variable aleatoria $X$ al conjunto de puntos donde $f(x)>0$.
\end{definition}

%% \begin{definition}[Probabilidad conjunta]
%%   La probabilidad conjunta de varios vectores aleatorios $X_1,\cdots, X_n$ es una medida de probabilidad 
  
%% \end{definition}

\begin{definition}[Probabilidad condicionada]
  Para cualesquiera dos sucesos $A,B \in \mathcal{A}$ tal que $P(B)\neq 0$, la probabilidad condicionada de A por B se define como\[
P(A|B) = \frac{P(A\cap B)}{P(B)}.
  \]
\end{definition}

\begin{definition}[Función de densidad condicionada]
  Para cualesquiera dos variables aleatorias $X$,$Y$ con función de densidad $f_X$ y $f_Y$ respectivamente y función de densidad conjunta $f$, la densidad de $X$ condicionada a que $Y=y$ viene dada por\[
f_X(x|Y=y) = \frac{f(x,y)}{f_Y(y)}.
  \]
\end{definition}

\begin{definition}[Sucesos independientes]
  Los sucesos $A_1,\cdots,A_n\in \mathcal{A}$ se dicen independientes si\[
P\left (\cap_{j=1}^kA_{i_j} \right ) = \prod_{j=1}^kP\left (A_{i_j} \right),
\]
para cualesquiera índices $1\leq i_1\leq \cdots \leq i_k\leq n$.
\end{definition}

\begin{definition}[Variables aleatorias independientes]
  Dos variables aleatorias $X$ e $Y$ son independientes si para cualesquiera conjuntos de Borel $A, B \in \mathcal{B}(\mathbb{R}^n)$ los sucesos $[X\in A]$ e $[Y\in B]$ son independientes. De forma general, $k$ variables aleatorias $X_1,\cdots,X_k$ serán independientes si para cualesquiera conjuntos de Borel $B_1,\cdots,B_k \in \mathcal{B}(\mathbb{R}^n)$ los sucesos $[X_1 \in B_1],\cdots,[X_k\in B_k]$ son independientes.
\end{definition}

\begin{proposition}[Caracterizaciones de independencia]
  Dado un variable aleatoria $X =(X_1,\cdots,X_n)$ sus componentes son independientes si, y solo si, se da alguna de las siguientes condiciones:
  \begin{itemize}
  \item La función de distribución verifica $F_X(x) = F_{X_1}(x_1)\cdots F_{X_n}(x_n)$ para todo $x \in \mathbb{R}^n$.
  \item $X$ tiene función de densidad y $f_X(x) = f_{X_1}(x_1)\cdots f_{X_n}(x_n)$ para todo $x \in \mathbb{R}^n$, salvo, a lo sumo, en un conjunto de medida nula.
  \end{itemize}
\end{proposition}

\begin{definition}[Esperanza]
  La esperanza de una variable aleatoria $X$ unidimensional se define como:\[
\mu = \mathbb{E}[X] = \int_{\Omega}X(\omega)dP(\omega) = \int_{\mathbb{R}}xdF(x).
\]

En el caso discreto, si la variable aleatoria $X$ tiene un número finito de posibles salidas $x_1,\cdots,x_k$ cada una con probabilidad $p_1,\cdots, p_k$ respectivamente, la esperanza es\[
\mathbb{E}[X] = \sum_{i=1}^kx_ip_i.
\]

En el caso de que $X$ sea una variable aleatoria absolutamente continua y admita función de densidad, $f(x)$, la esperanza se calcula\[
\mathbb{E}[X] = \int_{\mathbb{R}}xf(x)dx.
\]
  
  Si  $X=(X_1,\cdots,X_n)^T$ es una variable aleatoria multidimensional. Se define la esperanza de $X$ como:\[
\mu_X = \mathbb{E}[X] = \begin{bmatrix}
           \mathbb{E}[X_1] \\
          \vdots \\
          \mathbb{E}[X_n]
         \end{bmatrix}
 = \begin{bmatrix}
           \mu_1 \\
          \vdots \\
          \mu_n
 \end{bmatrix},  \]
 siempre que existan todas las esperanzas unidimensionales.
\end{definition}

\begin{proposition}[Propiedades de la esperanza]
  Sea $X$ una variable aleatoria con función de densidad $f(x)$, entonces se cumplen las siguientes propiedades de la esperanza:
  \begin{itemize}
  \item Para una función $g(x)$, $g(X)$ será también una variable aleatoria y su esperanza será\[
\mathbb{E}[g(X)] = \int g(x)f(x)dx.
\]
Esta propiedad se conoce como \textit{ley del estadístico inconsciente}.
\item $\mathbb{E}\left [\sum_{j=1}^kc_jg_j(X) \right] = \sum_{j=1}^k c_j \mathbb{E} \left [ g_j(X_i) \right ]$.

\item Si $X_1,\cdots,X_n$ son variables aleatorias independientes, entonces\[
\mathbb{E} \left [ \prod_{i=1}^n X_i \right ] = \prod_{i=1}^k\mathbb{E}[X_i].
  \]
  \end{itemize}
  
\end{proposition}
\begin{definition}[Esperanza condicionada]
  Se define la esperanza condicionada de la variable aleatoria $Y$ dado $X$ como la variable aleatoria $\mathbb{E}[Y|X] = g(X)$ tal que \[
\mathbb{E} [ Y | X = x ] = \int y f_Y(y| X = x) dy.
  \] 
  
\end{definition}
\begin{definition}[Varianza y matriz de covarianzas]
  Sea $X$ una variable aleatoria unidimensional de esperanza $\mu_X$, definimos su varianza como\[
\sigma_X^2 = \Var(X) = \mathbb{E}\left [ (X- \mu_X)^2 \right ].
  \]
  Se define la matriz de covarianzas de una variable aleatoria multidimensional $X= (X_1,\cdots,X_n)^T$ como\[
\Sigma = Cov(X) = \mathbb{E}\left [(X-\mu_X)(X-\mu_X)^T \right ] =  \begin{bmatrix} \sigma_{11} & \dots & \sigma_{1n} \\ \vdots& \ddots & \vdots \\ \sigma_{n1} &  \dots & \sigma_{nn}\end{bmatrix}\,,
\]
donde $\sigma_{ij} = Cov(X_i, X_j) = \mathbb{E}\left[(X_i-\mu_i)(X_j-\mu_j)\right]=\sigma_{ji}$ es la covarianza de las variables aleatorias $X_i$ y $X_j$. La podremos definir cuando existan todas las covarianzas.  
\end{definition}

\subsubsection{Estimadores}

\begin{definition}[Muestra aleatoria simple]
  Una muestra aleatoria simple de tamaño $n$ de una variable aleatoria $X$ con distribución teórica $F$, son $n$ variables aleatorias $(X_1,\cdots,X_n)$, independientes e idénticamente distribuidas, con distribución común $F$.
%Consecuentemente la función de distribución conjunta de una muestra aleatoria simple $(X_1,\cdots, X_m)$, correspondiente a una distribución de la población $F$, es $F(x_1,\cdots,x_n)=F(x_1)\cdots F(x_n)$.
\end{definition}

Llamamos distribución teórica $F$ de la variable aleatoria a estudiar a la distribución de la variable aleatoria que, en principio, es desconocida. Notamos $\mathcal{F}$ a la familia de distribuciones candidatas a ser realmente la distribución (se obtiene a partir de información previa). A menudo es posible conocer la forma funcional de $F$ pero se desconocen uno o algunos parámetros de la misma. Para tratar de obtener información sobre este parámetro desconocido a partir de una muestra utilizaremos los \textit{estimadores}.

\begin{definition}[Estadístico]
  Se denomina estadístico a cualquier función $T:\mathbb{R}^n\to \mathbb{R}$ de una muestra aleatoria simple $X_1,\cdots,X_n$  que sea medible.
\end{definition}

\begin{example}
  Algunos estadísticos habituales son:

  \begin{itemize}
  \item $T(x_1,\dots,x_n) = \frac{1}{n}\sum_{i=1}^nx_i$, media aritmética o media muestral, denotada por $\bar{x}$.
  \item $T(x_1,\dots,x_n) = \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2$, varianza muestral, notada $s^2$.
  \item $T(x_1,\dots,x_n) = \min (x_1,\dots,x_n)$, el menor valor muestral.
  \item $T(x_1,\dots,x_n) = \max (x_1,\dots,x_n)$, el mayor valor muestral.
  \end{itemize}
\end{example}

\begin{definition}[Estimador]
Un estimador es un estadístico $\widehat{\theta}(X_1,\cdots,X_n)$ cuyos valores se utilizan para obtener la estimación puntual de un parámetro desconocido $\theta$.
\end{definition}

Dado un estimador queremos que sus valores sean cercanos al parámetro a predecir, sin embargo, esto no lo podemos medir directamente, así que utilizamos otras propiedades como medida de la calidad de un estimador. Además, podemos considerar diferentes estimadores para estimar un mismo parámetro, nos decidiremos por uno u otro en función a estas propriedades. Algunas propiedades deseables en un estimador son la insesgadez y consistencia, definidas a continuación.

\begin{definition}[Estimador insesgado]
  Si $\widehat{\theta}$ es un estimador del parámetro $\theta$, la diferencia \[
\bias\left( \widehat{\theta}\right) = \mathbb{E}\left [\widehat{\theta} \right ] - \theta
\]
se denomina sesgo del estimador $\widehat{\theta}$ como estimador de $\theta$. Cuando el sesgo es nulo para cualquier valor del parámetro, es decir, si\[
\mathbb{E} \left [\widehat{\theta}\right] = \theta\quad \forall\theta \in\Theta,
\]
el estimador $\widehat{\theta}$ se dice insesgado para $\theta$.
\end{definition}

\begin{example}\label{ej:medest}
  % Media muestral estimador insesgado
  Sean $X_1,\dots,X_n$ variables aleatorias con distribución $F$ y $\mu = \E\left[X_1\right]$. Si el parámetro de interés es la media $\mu$, podemos utilizar la media muestral para estimarlo. Calculamos su sesgo,\[
\bias\left[\olsi{X} \right] = \E\left[ \olsi{X}\right] - \mu = \frac{1}{n} \sum_{i=1}^n\E \left [ X_i\right] - \mu = \mu - \mu = 0,
\]
hemos comprobado que la media muestral es un estimador insesgado para $\mu$.
\end{example}

El sesgo de un estimador es una medida del error del estimador, nos da una idea de cuánto se aleja del parámetro $\theta$ a predecir. Podemos medir también la varianza de un estimador, una medida de la fluctuación estocástica. Intuitivamente, la varianza nos permite medir cuánto puede variar el estimador si lo calculamos utilizando una muestra diferente. El tamaño de la muestra, será por tanto, un factor que influya en la estimación. Por ello, otra propiedad deseable en un estimador es la consistencia. Para dar la definición de estimador consistente es necesario conocer la definición de convergencia en probabilidad.

%% \begin{example}
%%   % Varianza muestral estimador sesgado
%%   Sean $X_1,\dots,X_n$ variables aleatorias con distribución $F$ y $\sigma^2 = \Var(X_1)$. Si ahora el parámetro de interés es $\sigma^2$ y utilizamos como estimador la varianza muestral, veremos que este estimador es sesgado para $\sigma^2$.
%%   \[
%% \bias\left[S^2\right] = \E\left[ S^2\right] - \sigma^2 = \frac{1}{n} \sum_{i=1}^n\left(\E \left [ X_i - \olsi{X}\right]\right)^2 - \sigma^2 = 0 - \sigma^2 \neq 0,
%% \]
%% \end{example}



Existen diferentes nociones de convergencia de variables aleatorias, presentamos a continuación una de ellas, la convergencia en probabilidad.
\begin{definition}[Convergencia en probabilidad]
  Una sucesión de variables aleatorias $\{X_n\}$ converge en probabilidad a la variable $X$ si para todo $\varepsilon > 0$,\[
\lim_{n\to\infty}P\left[|X_n-X|>\varepsilon \right] = 0. 
  \] Lo notaremos $X_n\xrightarrow[]{P}X$.
\end{definition}


\begin{definition}[Estimador consistente]
  Sea $\hat{\theta}_n$ un estimador del parámetro $\theta$ para una muestra de tamaño $n$. Diremos que el estimador $\hat{\theta}_n$ es consistente si, cuando $n\to\infty$, se verifica\[
\hat{\theta}_n\xrightarrow[]{P}\theta.
  \]
\end{definition}

\begin{lemma}
  Sea $\hat{\theta}_n$ un estimador del parámetro $\theta$. Si $\bias\left(\hat{\theta}_n\right)\to 0$ y $\Var\left(\hat{\theta}_n\right)\to0$, entonces $\hat{\theta}_n\xrightarrow[]{P}\theta$, esto es, el estimador $\hat{\theta}_n$ es consistente.
\end{lemma}

\begin{example}
  % Estimadores consistentes
  Consideramos la media muestral, un estimador insesgado de la media de una variable aleatoria $X$. Veamos el comportamiento asintótico de su varianza.\[
\Var(\olsi{X}) = \E\left[(\olsi{X}-\mu)^2\right]
  \]
\end{example}

Como ejemplo de los conceptos anteriores podemos considerar la media muestral como estimador de la media teórica de una variable aleatoria $X$, la ley (débil) de los grandes números ...---no sé cómo quiero redactar eso---

\begin{theorem}[Ley (débil) de los grandes números]\label{t:lgn}
  Sean $X_1,\cdots,X_n$ variables aleatorias con distribución $F$ y $\mu =\mathbb{E}\left [X_1 \right]$. Si $\mu =\mathbb{E}\left [X_1 \right] < \infty$, entonces la media muestral\[
\olsi{X}_n = \frac{1}{n}\sum_{i=1}^nX_i
\]converge en probabilidad a $\mu$, esto es,\[
\olsi{X}_n\xrightarrow[]{P} \mu.
\]
  
\end{theorem}


\subsubsection{Algunas distribuciones discretas y continuas}

Durante el desarrollo del trabajo se utilizarán algunas distribuciones, se presentan a continuación.
%--------------------------------- Uniforme unidimensional--------------

\begin{definition}[Distribución uniforme discreta]
Si $X$ es una variable aleatoria con distribución uniforme discreta sobre el intervalo $[a,b]$. $X$ tomará $n$ posibles valores, $x_1,\cdots,x_n$ ($x_i\in [a,b]$ para $i=1,\cdots,n$), con igual probabilidad $\frac{1}{n}$. La función masa de probabilidad será $f(x_i) = \frac{1}{n}$ para todo $i = 1,\cdots, n$. 
\end{definition}

\begin{example}
  Si tiramos un dado no cargado hay seis posibles valores: 1, 2, 3, 4, 5 y 6; cada vez que lanzamos el dado hay probabilidad $\frac{1}{6}$ de que salga cualquiera de ellos.
\end{example}

\begin{definition}[Distribución uniforme continua]
  Una variable aleatoria $X$ que puede obtener cualquier valor en el intervalo $[a,b]$ con la misma probabilidad tendrá una distribución uniforme. Su función de densidad vendrá dada por \[
f(x) = 
\begin{cases}
  \frac{1}{b-a},\quad \text{si } x\in [a,b]\\
  0,\quad\quad \text{en otro caso}
\end{cases}.\\
  \]
\end{definition}
% ----------------------------- FIN uniforme----------------------------

%---------------------------------Distribución normal-------------------
\begin{definition}[Distribución normal unidimensional]
  Una variable aleatoria unidimensional seguirá una distribución normal, o distribución gaussiana,si su función de densidad es \[
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma}\right ) ^2}.
\]
\end{definition}

Generalizamos esta definición para el caso multivariante.
\begin{definition}[Distribución normal multivariante]
  Una variable aleatoria sigue una distribución normal multivariante si cualquier combinación de sus componentes tiene una distribución normal univariante. Su función de densidad es \[
f(x) = \frac{1}{\left (\sqrt{2\pi}\right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},
  \] donde $\Sigma$ es una matriz definida positiva.\\
\end{definition}
%--------------------------------Fin - normal---------------------------

%--------------Multinomial - Principio----------------------------------
La siguiente distribución, la distribución multinomial modela la probabilidad de un número determinado de éxitos para $n$ experimentos independientes, donde hay $k$ posibles éxitos, cada uno de ellos con probabilidad de éxito fija.

\begin{definition}[Distribución multinomial]
 Sean $n$ experimentos independientes donde cada uno produce uno de los sucesos $S_1,\dots,S_k$ para $k\geq 2$ (estos sucesos son mutuamente excluyentes) y cada suceso $S_i$ ocurre con probabilidad $p_i$ ($p_1+\cdots+p_k = 1$). Definimos las variables aleatorias $X_i,\ i=1,\dots,k$ como el número de experimentos en el que ocurre $S_i$. Entonces $X=(X_1,\cdots,X_k)$ tiene una distribución multinomial con parámetros $n$ y $p = (p_1,\dots, p_k)$. Su función masa de probabilidad viene dada por
  %Sea $k$ un número fijo y finito, tendremos $k$ posibles salidas mutuamente excluyentes con probabilidades $p_1\cdots p_k$. Si la variable aleatoria $X_i$ indica el número de veces que se observa la salida $i$ en los $n$ experimentos, entonces el vector $X = (X_1, \cdots, X_n)$ sigue una distribución multinomial con parámetros $n$ y $p = (p_1,\cdots, p_n)$. Su función masa de probabilidad viene dada por

\begin{align*}
f(x_1,\cdots,x_k; n, p_1,\cdots,p_k) &= P[X_1 = x_1\text{ y } \cdots \text{ y } X_k = x_k]\\ &=
\begin{cases}
    \frac{n!}{x_1!\cdots x_k!} p_1^{x_1} \cdots  p_k^{x_k},\quad\text{si } \sum_{i=1}^kx_i = n\\
    0, \qquad \qquad \qquad\text{en otro caso,}
\end{cases}
\end{align*}
donde $x_1,\cdots, x_k$ son enteros no negativos.
\end{definition}

%Notamos que aunque los sucesos sean independientes, sus salidas $X$ son dependientes, ya que deben sumar $n$.

\begin{example}
  Si tenemos un dado de $k$ lados y queremos calcular cuántas veces sale cada uno de ellos en $n$ lanzamientos, tendremos una distribución multinomial.\\
\end{example}

El valor esperado de observar la salida $i$ en $n$ sucesos es $\mathbb{E}(X_i) = np_i$, luego $\mathbb{E}(X) = np$.

Veamos cómo son los elementos de la matriz de covarianzas. Los elementos de la diagonal son la varianza de una variable aleatoria con distribución binomial, luego $\Var(X_i) = np_i(1-p_i)$. Las covarianzas de $X_i, X_j$ para $i\neq j$ son $Cov(X_i, X_j) = - n p_i p_j$. Por tanto, $\Var(X) = n [\text{diag}(p) - p p^T]$, donde diag$(p)$ representa la matriz diagonal que tiene el elemento $p_i$ en la posición $i,i$.
%----------------------MULTINOMIAL-FIN----------------------------------

%--------------------------Estudio de la dependencia--------------------
\subsubsection{Estudio de la dependencia}

Dadas dos variables aleatorias nos podemos preguntar si son o no independientes, es decir, si la realización de una de ellas afecta o no a la realización de la otra. En caso de ser dependientes nos gustaría medir qué tipo de relación existe entre ellas. Hay numerosos métodos que nos permiten realizar esta medida, por ejemplo, el \textit{coeficiente de correlación de Pearson} o el \textit{coeficiente de correlación de Spearman}.

Medir la dependencia entre variables puede ser útil porque nos indica relaciones entre variables que se pueden aprovechar para hacer predicciones de una a partir de la otra. Encontramos infinidad de aplicaciones en el mundo real a conocer una dependencia entre variables. Por ejemplo, para conocer qué efecto tiene un fenómeno sobre otro. También, si conocemos la dependencia entre variables, podemos utilizar aquellas de las que tenemos información más precisa o fiable para explicar aquellas de las que no.

\begin{definition}[Coeficiente de correlación de Pearson]
  Se define el coeficiente de correlación de Pearson para un par de variables aleatorias $(X,Y)$ como\[
r_{X,Y} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y} = \frac{\mathbb{E}\left [ (X- \mu_X)(Y-\mu_Y)\right ]}{\sigma_X\sigma_Y}.
\]
El mismo coeficiente para una muestra $\{(x_1,y_1),\dots,(x_n,y_n)\}$ se define como
%B) El mismo coeficiente para una muestra $(X_1,Y_1),\dots, (X_n,Y_n)$ en la que se ha obtenido la realización muestral  $\{(x_1,y_1),\dots,(x_n,y_n)\}$ se define como
\[
r_{xy}=\frac{\sum_{i=1}^n(x_i-\olsi{x})(y_i-\olsi{y})}{\sqrt{\sum_{i=1}^n(x_i-\olsi{x})^2(y_i-\olsi{y})^2}},
\] donde $\olsi{x} = \frac{1}{n}\sum_{i=1}^nx_i$ la media muestral, de forma análoga para $\olsi{y}$.
\end{definition}

Este coeficiente mide la correlación lineal entre las variables $X,Y$. Su valor se encuentra entre +1 y -1, donde +1 indica una correlación lineal positiva total, 0 que no hay correlación lineal y -1 correlación lineal total negativa. Si el coeficiente de correlación de Pearson de dos variables aleatorias es 0, sabremos que no existe una dependencia lineal entre ellas, pero no podremos asegurar nada acerca de la independencia, pues podría existir otro tipo de dependencia entre ellas.

Este coeficiente es paramétrico, depende de las distribuciones marginales, el que presentamos a continuación será no paramétrico.

El coeficiente de correlación de Spearman mide la relación entre dos variables más allá de si es lineal o no. Atiende a si ambas variables tienen la misma monotonía, esto es, si a medida que los valores de una de las variables crecen, los de la otra también lo hacen (o al revés). Para ello, tendremos que ordenar los datos de cada variable de menor a mayor.


--- No encuentro la definición del coeficiente teórico ---
\begin{definition}[Coeficiente de correlación de Spearman]
  Para una muestra $S = \{(x_1,y_1),\dots,(x_n,y_n)\}$ de dos variables aleatorias $X$ e $Y$, ordenamos los datos de ambas variables de forma que $S_x = (x_{(1)},\dots,x_{(n)})$ es la ordenación de los $x_i$ y $S_y = (y_{(1)},\dots,y_{(n)})$ la de los $y_i$. El coeficiente de correlación de Spearman entre $X$ e $Y$ viene dado por\[
\rho_{XY}=r_{S_xS_y}=\frac{Cov(S_x,S_y)}{\sigma_{S_x}\sigma_{S_y}},
\] donde $r$ denota el coeficiente de correlación de Pearson. Si todos los puntos son distintos, se puede utilizar la fórmula:\[
\rho_{XY} = 1 - \frac{6\sum_i d_i^2}{n(n^2-1)},
\]donde $d_i$ es la diferencia entre el puesto de $x_i$ e $y_i$ al realizar la ordenación.
\end{definition}

Este coeficiente también toma valores entre -1 y +1. Si los valores son cercanos a cero la correlación monótona será muy débil, mientras que si son cercanos a +1 o -1 será muy fuerte, el signo nos indicará si a medida que una crece la otra decrece o viceversa. Al igual que coeficiente de Pearson, el de Spearman no caracteriza la independencia. Aunque ambos coeficientes, si las variables aleatorias son independientes, valdrán 0.

Así, hemos visto un par de ejemplos de medidas de dependencia que caracterizan adecuadamente tipos concretos de dependencia, pero no caracterizan la independencia.

La información mutua es otra forma de medir la dependencia entre dos variables aleatorias capaz de determinar otros tipos de dependencias, además de las lineales o monótonas. Podríamos decir que es una medida de la dependencia global. Además, esta medida también nos dará información sobre la independencia de las variables aleatorias en estudio (véanse Teorema \ref{t:im_indd} y \ref{c:im_indc}).

%-----------------------------------------------------------------------
%-----------------------TEORÍA DE LA INFORMACIÓN------------------------
%-----------------------------------------------------------------------
    \subsection{Teoría de la información}

    El concepto de información es demasiado amplio como para ser recogido en una única definición. Sin embargo, podemos definir el concepto de entropía, que mide la cantidad de información necesaria para describir una variable aleatoria, o el de información mutua, que refleja la información que una variable aleatoria contiene sobre otra.\\

    Intuitivamente, la entropía es una medida de la información o incertidumbre de una variable aleatoria. El término entropía viene del griego, donde significa \textit{transformación} y tiene relevancia también en otras áreas, como en la física donde se define como el logaritmo del cociente entre temperatura final e inicial de un sistema.\\

    La idea de relacionar el número de estados de un sistema con una medida física viene del siglo XIX, en el que Rudolph Clausius sugirió la denominación ``entropía'' para esta medida. En 1928, Ralph Vinton Lyon Hartley define el término en el contexto de la teoría de la información, básicamente como el logaritmo del tamaño del alfabeto. Aunque no es hasta 1948 cuando Claude Shannon da una definición matemática del concepto de información como lo conocemos hoy en día,  sentando las bases de la teoría de la información, que pasó a formar parte de la teoría de la probabilidad. En su concepto de entropía no todos los símbolos de un alfabeto tienen porqué ser equiprobables. Esto nos permite medir la capacidad de comunicación de un canal. Andrei N. Kolmogorov, en la década de los 60, desarrolló su teoría de la complejidad, otra forma de teoría de la información.\\

A continuación, se explicarán estos conceptos, y algunos relacionados, junto a demostraciones de sus propiedades y algunos ejemplos, tanto para el caso discreto como para el caso continuo. Las principales fuentes consultadas para ello han sido \cite{thomas} y \cite{gray}. Se han estudiado y comprendido los capítulos correspondientes a entropía y entropía diferencial para posteriormente exponer aquí una síntesis de lo aprendido.

\subsubsection{Caso discreto}
    
Sea una variable aleatoria discreta, $X$, que toma valores en el alfabeto $\mathcal{X} = \{x_1, \cdots, x_N \}$, con función masa de probabilidad $P_X(x_i) = p(x_i)$.

\begin{definition}[Entropía]
  La entropía de una variable aleatoria discreta $X$ viene dada por \[H(X) = - \sum_{x\in \mathcal{X}} p(x) \log p(x).\]
\end{definition}

En la fórmula anterior tomaremos, por continuidad, $0 \log 0 = 0$. Así, la entropía queda definida por un funcional de la distribución de $X$, es decir, que no depende de los valores que tome la variable sino de sus probabilidades.

Según cuál sea la base del logaritmo utilizado en la fórmula de la entropía utilizaremos unas unidades u otras. Notamos $H_b(X) = - \sum_{x\in \mathcal{X}} p(x) \log_b p(x)$. Si el logaritmo tiene base 2, la entropía se medirá en bits, mientras que si la base es $e$, las unidades serán \textit{nats} (unidad natural de información).\\

Dada la variable aleatoria discreta $X$ con distribución $p(x)$, podemos considerar la variable aleatoria $g(X)$, cuya esperanza, $E$, viene dada por:\[
E_pg(X) = \sum_{x\in \mathcal{X}}g(x)p(x).
\]
Tomando $g(X) = \log \frac{1}{p(X)}$ obtenemos otra definición de entropía, la usada en termodinámica:\[
H(X) = E_p \log \frac{1}{p(X)}.
\]

La fórmula anterior coincide, efectivamente, con la definición de entropía dada:\[
H(X) = E_p \log \frac{1}{p(X)} = \sum_{x\in \mathcal{X}}\left ( \log \frac{1}{p(x)} \right ) p(x) = - \sum_{x \in \mathcal{X}}p(x)\log p(x).
\]

\begin{lemma} La entropía de una variable aleatoria discreta es no negativa, $H(X) \ge 0$.
\end{lemma}
\begin{proof}
  \[H(X) = - \sum_{x \in \mathcal{X}}\underbrace{p(x)}_{\in [0,1]}\underbrace{\log p(x)}_{\leq 0} \ge 0.\]

  %% El producto de ambos factores es negativo para cualquier $x \in \mathcal{X}$, luego el signo menos en la fórmula de la entropía nos permite concluir que $H(X) = - \sum_{x \in \mathcal{X}}p(x)\log p(x) \ge 0$.
\end{proof}

El siguiente lema, mediante propiedades del logaritmo, nos permitirá cambiar la base con la que calculemos la entropía.\\

\begin{lemma}\[H_b(X) = (\log_ba) H_a(X).\]
\end{lemma}
\begin{proof}
  Partiremos de la definición alternativa de $H_b$ y usaremos que $\log_bp = \log_ba\ \log_ap$.
  \[
  H_b(X) = E_p \left(\log_b \frac{1}{p(X)}\right ) = E_p \left [ \left ( \log_b a \right )\log_a \frac{1}{p(X)} \right ] = \left ( \log_b{a} \right ) H_a(X).
  \]
\end{proof}

En el siguiente ejemplo calculamos la entropía de una variable aleatoria con probabilidades fijadas.
\begin{example}
  Sea  \[X = 
     \begin{cases}
       a, \quad\text{con probabilidad } \frac{1}{2}\\
       b, \quad\text{con probabilidad } \frac{1}{4} \\
       c, \quad\text{con probabilidad } \frac{1}{8} \\
       d, \quad\text{con probabilidad } \frac{1}{8} \\
     \end{cases}.\]

     Utilizamos la fórmula de la entropía y obtenemos \[H(X) = - \left (\frac{1}{2} \log \frac{1}{2} + \frac{1}{4} \log \frac{1}{4} + \frac{1}{4} \log \frac{1}{8} \right ) = - \left (-\frac{1}{2} - \frac{1}{2} - \frac{3}{4} \right ) = 1 + \frac{3}{4} = \frac{7}{4} \text{ bits}.\]\\
\end{example}

\begin{example}
  Sea \[X = 
     \begin{cases}
       1, \quad\text{con probabilidad } p\\
       0, \quad\text{con probabilidad } 1-p \\
     \end{cases}.\]
     Vamos a analizar la entropía de la variable aleatoria $X$ como función de $p$, es decir, $H:[0,1]\to \mathbb{R}$ estará dada por $H(p)$. Se calcula como sigue: $ H(p) := H(X) = - \left ( p \log p + (1-p) \log (1-p) \right )$.
     %La entropía de la variable aleatoria $X$ la notaremos $H(p)$
     Cuando $p=0$ o $p=1$, $H(p) = 0$, luego no hay incertidumbre, se conoce el valor que tomará la variable $X$. Calcularemos la primera y segunda derivada de $H(p)$ para conocer su comportamiento en el intervalo $(0,1)$.
     \begin{align*}
       H'(p) &= - \left ( \log p + \frac{p}{p} - \log(1-p) - \frac{1-p}{1-p} \right ) = - \left ( \log \left (\frac{p}{1-p} \right) \right ).\\
     H''(p) &= - \left ( \frac{1}{p} + \frac{1}{1-p} \right ) \leq 0 \quad \forall p \in (0,1).
     \end{align*}
     
     Hemos comprobado que la función $H(p)$ es cóncava. La incertidumbre máxima se alcanzará en $p=\frac{1}{2}$: \[H\left (\frac{1}{2}\right ) = - \left ( \frac{1}{2}\log\frac{1}{2} + \frac{1}{2} \log\frac{1}{2} \right ) = - \log \frac{1}{2} = 1.\]\\
\end{example}


La definición de entropía de una variable aleatoria se puede aplicar a un par de variables aleatorias dando lugar a la entropía conjunta.

\begin{definition}[Entropía conjunta]
  La entropía conjunta, $H(X,Y)$, de un par de variables aleatorias discretas $(X,Y)$ con distribución conjunta $p(x,y)$ se define como \[
  H(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y), \]
  que podemos expresar también como \[
  H(X,Y) = - E \log p(X,Y).\]
\end{definition}
Presentamos el siguiente lema que será útil para la demostración de propiedades en lo sucesivo.

\begin{lemma}[Desigualdad de Gibbs]\label{l:gibbs}
  Dadas dos funciones masa de probabilidad $\{p_i\}$ y $\{q_i\}$, entonces \[
\sum_i p_i \log \frac{p_i}{q_i} \ge 0,
\]
dándose la igualdad si, y solo si, $q_i = p_i$ para todo $i$.
\end{lemma}

\begin{proof}
  Sea $I = \{i : p_i > 0\}$, entonces\[
- \sum_{i \in I} p_i \log \frac{p_i}{q_i} = \sum_{i \in I} p_i \log \frac{q_i}{p_i},
  \]
  usando $\log x \le x - 1$, con igualdad si, y solo si, x = 1, obtenemos\[
  \sum_{i \in I} p_i \log \frac{q_i}{p_i} \leq
  \sum_{i\in I}p_i \left ( \frac{q_i}{p_i} - 1 \right )
  = \sum_{i\in I}q_i - \underbrace{\sum_{i \in I}p_i}_{= 1} \leq 0.\]

  Tendremos la igualdad cuando $\frac{q_i}{p_i} = 1$ para todo $i$.
\end{proof}

En el siguiente lema recogemos algunas propiedades de la entropía conjunta.

\begin{lemma}\label{l:prop_ent_conj} Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifican:
  \begin{enumerate}
  \item $H(X) \le H(X, Y)$.
  \item $H(X,Y) \le H(X) + H(Y)$. La igualdad se dará si, y solo si, $X$ e $Y$ son independientes.
  \end{enumerate}
\end{lemma}

\begin{proof}\hfill
  \begin{enumerate}
  \item Para cualesquiera $x \in \mathcal{X}, y \in \mathcal{Y}$ se tiene que $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y $p(x,y) \le p(x)$, luego:
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \ge - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x)\\
   &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) = H(X).
\end{align*}

\item Usando la definición, $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y propiedades del logaritmo:
  \begin{align*}
    &H(X,Y) - (H(X) + H(Y)) \\ &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} p(x)\log p(x) + \sum_{y \in \mathcal{Y}} p(y)\log p(y)\\
    &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(y)\\
    &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x)p(y)}{p(x,y)} \leq 0,
  \end{align*}
  donde la última desigualdad viene de aplicar el Lema \ref{l:gibbs} sobre las funciones masa de probabilidad $p(x,y)$ y $p(x)p(y)$. Esta última sería la función masa de probabilidad conjunta de dos variables aleatorias $X,Y$ con distribuciones $p(x), p(y)$ independientes. La igualdad ocurre si, y solo si, $p(x)p(y) = p(x,y)$, esto es si $X$ e $Y$ son independientes.
  \end{enumerate}
\end{proof}

Al considerar dos distribuciones distintas, además de su entropía conjunta podemos definir la entropía condicional de una respecto de la otra.

\begin{definition}[Entropía condicional]
  Si $(X,Y)$ sigue una distribución $p(x,y)$, la entropía condicional de $Y$ con respecto a $X$, $H(Y|X)$, se define como:
  \begin{align*}
    H(Y|X) &= \sum_{x \in \mathcal{X}} p(x) H(Y | X = x)\\
    &= - \sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y | x) \log p(y | x)\\
    &=  - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y | x)\\
    &=- E \log p(Y|X).
  \end{align*}
\end{definition}

%En la definición anterior, la probabilidad condicionada $p(y|x)$ viene dada por la función masa de probabilidad $p(y|x) = \frac{p(x,y)}{p(x)}$.

\begin{theorem}[Regla de la cadena]\label{t:regla_cadena}
  Si $(X,Y)$ sigue una distribución $p(x,y)$, entonces se verifica:\[
H(X,Y) = H(X) + H(Y|X).
  \]
\end{theorem}

\begin{proof}
Para la prueba utilizaremos que $p(x,y) = p(x)p(y|x)$,
  \begin{align*}
    H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \left( p(x)p(y|x) \right )\\
    &= - \left(\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y|x) \right ),\\
  \end{align*}
usando que $\sum_{y\in \mathcal{Y}}p(x,y) = p(x)$, obtenemos
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) + H(Y|X)\\
    &= H(X) + H(Y|X).
  \end{align*}
\end{proof}

\begin{corollary} En las condiciones del Teorema \ref{t:regla_cadena},\[
H(X,Y|Z) = H(X|Z) + H(Y|X, Z).
  \]
\end{corollary}

\begin{lemma}
  Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces la entropía condicional cumple:\[
0 \leq H(Y|X) \leq H(Y).
\]
Donde la desigualdad de la derecha es una igualdad si, y solo si, $X$ e $Y$ son independientes.
\end{lemma}

\begin{proof}
  La desigualdad izquierda se obtiene de la definición de entropía condicional, \[H(Y|X) = - E \underbrace{\log \overbrace{p(Y|X)}^{\in [0,1]}}_{\leq 0} \ge 0.\]

  Aplicando el Teorema \ref{t:regla_cadena} y el Lema \ref{l:prop_ent_conj} se obtiene la desigualdad de la derecha:\[
H(X) + H(Y|X) = H(X,Y) \leq H(X) + H(Y),
\]
dándose la igualdad solo en el caso en que las variables $X$ e $Y$ sean independientes.
\end{proof}

\begin{example}\label{e:dist_conj}
Sea $(X,Y)$ una variable aleatoria con distribución conjunta:
  \begin{table}[H]
\centering
%\caption{}
\label{}
\begin{tabular}{r|llll|l}
 \toprule
 \backslashbox{$Y$}{$X$} & 1 & 2 & 3 & 4&\\ %\hline\\[-10pt]
 \cline{1-5} \\[-10pt]
1 & $\frac{1}{8}$ & $\frac{1}{16}$ & $\frac{1}{32}$ & $\frac{1}{32}$  & $\frac{1}{4}$ \\[5pt]
2 & $\frac{1}{16}$ & $\frac{1}{8}$ & $\frac{1}{32}$ & $\frac{1}{32}$ & $\frac{1}{4}$ \\[5pt]
3 & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{4}$ \\[5pt]
4 & $\frac{1}{4}$ & 0 & 0 & 0 & $\frac{1}{4}$ \\[5pt]
%\hline\\[-10pt]
\cline{2-5}\\[-10pt]
& $\frac{1}{2}$ &  $\frac{1}{4}$ &  $\frac{1}{8}$ &  $\frac{1}{8}$ \\[5pt]   
\bottomrule
\end{tabular}
\end{table}

  %  La distribución marginal de $X$ es $\left ( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8} \right )$ y la distribución marginal de $Y$ es $\left ( \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4} \right )$.
  Calculamos las entropías de las variables marginales:
  \begin{align*}
    H(X) &= - \frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} - 2 \frac{1}{8} \log \frac{1}{8} = -\frac{1}{2} - \frac{1}{2} - \frac{6}{8} = \frac{7}{4} \text{ bits}.\\
    H(Y) &= - 4 \frac{1}{4} \log \frac{1}{4} = 2 \text{ bits}.
  \end{align*}
  Calculamos la entropía conjunta:
  \begin{align*}
    H(X,Y) &= - \frac{1}{4} \log \frac{1}{4} - \frac{2}{8} \log \frac{1}{8} - \frac{6}{16} \log \frac{6}{16} - \frac{4}{32} \log \frac{1}{32}\\
    &= \frac{1}{2} + \frac{3}{4} + \frac{6}{4} + \frac{5}{8} = \frac{27}{8} \text{ bits}.
  \end{align*}

  Vemos que, efectivamente, la entropía conjunta es mayor que cada una de las individuales, pero menor que su suma $\left( \frac{30}{8} \right )$. Calculamos las entropías condicionales:
  \begin{align*}
    H(Y|X) = &- \frac{1}{8} \log \frac{1}{4} - \frac{2}{16} \log\frac{1}{8} - \frac{1}{4} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\
    &- \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2} - \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2}\\
    =& \frac{1}{2} + \frac{6}{8} + \frac{3}{8} = \frac{13}{8} \text {bits}.\\
    H(X|Y) = & - \frac{1}{8} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4}- \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\
    &- \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4} - \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4}\\
    =& \frac{1}{4} + \frac{6}{8} + \frac{3}{8} = \frac{11}{8} \text{ bits}.
  \end{align*}
  Observamos que las entropías condicionales son positivas y menores que las entropías individuales, además $H(Y|X) \neq H(X|Y)$. Comprobamos que se verifica el Teorema \ref{t:regla_cadena}:
  \begin{align*}
    H(X,Y) &= H(X) + H(Y|X) = \frac{7}{4} + \frac{13}{8} = \frac{27}{8} \text{ bits}.\\
    H(X,Y) &= H(Y) + H(X|Y) = 2 + \frac{11}{8} = \frac{27}{8} \text{ bits}.\\
  \end{align*}
\end{example}

Pasamos a definir otra medida de información, la entropía relativa, esta será una medida de la distancia entre dos distribuciones. La podemos ver como una medida de inteficiencia al asumir que la distribución es $q$ cuando en realidad es $p$. Será una medida de ineficiencia en el sentido de cuánta información perderíamos si utilizáramos la distribución $q$ para aproximar a la distribución $p$.

\begin{definition}[Entropía relativa]
  La entropía relativa (o ``distancia'' Kullback Leibler) entre dos funciones masa de probabilidad $p(x)$ y $q(x)$ se define como\[
D(p||q) = \sum_{x \in \mathcal{X}}p(x) \log \frac{p(x)}{q(x)} = E_p \log \frac{p(X)}{q(X)}.
  \]
\end{definition}
Las posibles indeterminaciones en la fórmula anterior se definen como: $0 \log \frac{0}{0} = 0$ y $p \log \frac{p}{0} = \infty$.

\begin{lemma}\label{l:ent_rel_pos}
  Para dos funciones masa de probabilidad $p(x)$ y $q(x)$ se tiene \[
  D(p||q) \ge 0,\]
  con igualdad si, y solo si, $p(x) = q(x) \quad \forall x \in \mathcal{X}$.
\end{lemma}
\begin{proof}
  Usando la definición de entropía relativa, se obtiene el resultado aplicando el Lema \ref{l:gibbs}.
\end{proof}

La entropía relativa no es realmente una distancia, ya que no es simétrica ni verifica la desigualdad triangular. Sin embargo, a veces es útil pensar en ella como una medida de distancia entre distribuciones.

\begin{example}
  Sea $\mathcal{X} = \{0,1\}$ y consideramos dos distribuciones $p$ y $q$ en $\mathcal{X}$. Tomamos $p(0) = 1-r$, $p(1) = r$ y $q(0) = 1-s,\ q(1) = s$.
  \begin{align*}
    D(p \Vert q) &= \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = (1 - r) \log \frac{1-r}{1-s} + r \log \frac{r}{s}.\\
    D(q \Vert p) &= \sum_{x \in \mathcal{X}}q(x) \log \frac{q(x)}{p(x)} = (1-s) \log \frac{1-s}{1-r} + s \log \frac{s}{r}.
  \end{align*}

  Si tuviéramos $r=s$, entonces $D(p \Vert q) = 0 = D(q \Vert p)$.
  Tomando $r = \frac{1}{2}$ y $s = \frac{1}{4}$ comprobaremos que no se da la simetría de la entropía relativa.
  \begin{align*}
    D(p \Vert q) &= \frac{1}{2} \log \frac{1/2}{3/4} + \frac{1}{2} \log \frac{1/2}{1/4} = 1 - \frac{1}{2} \log 6 + \frac{1}{2} = 1 - \frac{1}{2} \log 3 = 0.2075 \text{ bits}.\\
    D(q \Vert p) &= \frac{3}{4} \log \frac{3/4}{1/2} + \frac{1}{4} \log \frac{1/4}{1/2} = \frac{3}{4} \log 6 - \frac{3}{2} - \frac{1}{4} = -1 + \frac{3}{4} \log 3 = 0.1887 \text{ bits}. % \frac{2}{4} - \frac{3}{2} + \frac{3}{4} \log 3 =
  \end{align*}
Luego, $D(p\Vert q) \ne D(q \Vert p)$.\\
\end{example}

\begin{lemma}
  Si $X$ es una variable aleatoria con alfabeto $\mathcal{X}$, entonces \[
  H(X) \leq \log{\left\Vert\mathcal{X}\right\Vert}, 
  \]
donde $\left\Vert\mathcal{X}\right\Vert$ representa al cardinal de $\mathcal{X}$.
\end{lemma}
\begin{proof}
  Sea $u(x) = \frac{1}{\left\Vert\mathcal{X}\right\Vert}$ la función masa de probabilidad uniforme sobre $\left\Vert\mathcal{X}\right\Vert$, y sea $p(x)$ la función masa de probabilidad de $X$. Entonces,
  
 \begin{align*}
   D \left ( p \Vert u \right ) &= \sum_{x \in \mathcal{X}}
   p(x) \log \frac{p(x)}{u(x)} = \sum_{x \in \mathcal{X}} p(x) \log p(x) - \sum_{x \in \mathcal{X}} p(x) \log \frac{1}{\left\Vert\mathcal{X}\right\Vert}\\
  &= -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
 \end{align*}
 
    Usando el Lema \ref{l:ent_rel_pos}, obtenemos el resultado: \[
0 \leq D\left ( p \Vert u \right ) = -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
    \]
\end{proof}

%\subsubsection{Información mutua}

La información mutua es una medida de la dependencia entre dos variables aleatorias. Mide la cantidad de información que tenemos sobre una de ellas si hemos observado la otra. Se define como sigue.

\begin{definition}[Información mutua]
  Dadas dos variables aleatorias $X$ e $Y$ discretas con función masa de probabilidad conjunta $p(x,y)$ y funciones masa de probabilidad marginales $p(x)$ y $p(y)$. La información mutua, representada por $I(X;Y)$, es la entropía relativa entre la distribución conjunta y la distribución producto $p(x)p(y)$.
  \begin{align*}
  I(X;Y) &= D \left ( p(x,y) \Vert p(x)p(y) \right ) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\ &= E_{p(x,y)} \log \frac{p(X,Y)}{p(X)p(Y)}.
  \end{align*}
\end{definition}

Notamos que la información mutua, al estar definida como una entropía relativa, será no negativa (Lema \ref{l:ent_rel_pos}).

\begin{theorem}[Relación entre entropía e información mutua]\label{t:ent_im}
  La siguiente lista recoge algunas relaciones entre la entropía y la información mutua.
\begin{enumerate}[label={\alph*)}]
  \item $I(X;Y) = H(X) - H(X|Y)$.
  \item $I(X;Y) = H(Y) - H(Y|X)$.
  \item $I(X;Y) = H(X) + H(Y) - H(X,Y)$.
  \item $I(X;Y) = I(Y;X)$.
  \item $I(X;X) = H(X)$, la entropía es un caso particular de la información mutua.
  \end{enumerate}
\end{theorem}
\begin{proof}
   
  \begin{align*}
    \text{a) }I(X;Y) &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)} = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(y) p(x|y) \log \frac{p(y)p(x|y)}{p(x)p(y)}\\
    &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(x|y) - \sum_{x \in \mathcal{X}}\underbrace{\left ( \sum_{y \in \mathcal{Y}} p(y) p(x|y) \right)}_{p(x)} \log p(x)\\
    &= - H(X|Y) + H(X).
  \end{align*}
  b) Se obtiene de forma análoga.\\
  c) Se prueba aplicando la regla de la cadena sobre $H(X|Y)$, $I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)$.\\
  d) Se prueba usando el apartado c).\\
  e) $I(X;X) = H(X) - H(X|X) = H(X)$.
\end{proof}

\begin{example}
  Para las distribuciones consideradas en el Ejemplo \ref{e:dist_conj}, calculamos la información mutua:
  \begin{align*}
    I(X;Y) =& \frac{1}{8} \log \frac{1/8}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4}\\
    +& \frac{1}{8} \log \frac{1/8}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4}\\
    +& \frac{2}{16} \log \frac{1/16}{1/8 \cdot 1/4} + \frac{1}{4} \log \frac{1/4}{1/2 \cdot 1/4} = \frac{2}{16} \log \frac{1}{2} + \frac{1}{2} \log 2 = \frac{3}{8} \text{ bits}.
  \end{align*}

  Vemos que se cumplen las propiedades del Teorema \ref{t:ent_im}:
  \begin{align*}
    H(X) -H(X|Y) &= \frac{7}{4} - \frac{11}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(Y) -H(Y|X) &= 2 - \frac{13}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(X) + H(Y) - H(X,Y) &= \frac{7}{4} + 2 - \frac{27}{8} = \frac{3}{8} \text{ bits} = I(X;Y).
  \end{align*}
  
\end{example}

\begin{theorem}\label{t:im_indd}
  Sean $X$, $Y$ variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifica que $I(X;Y)\ge 0$, dádonse la igualdad si, y solo si, $X,Y$ son independientes.
\end{theorem}
\begin{proof}
%  Supongamos que $I(X;Y)=0$, entonces, por el apartado c) del Teorema \ref{t:ent_im}, tenemos que $0 = H(X)+H(Y)-H(X,Y)$, esto es, $H(X,Y) =H(X)+H(Y)$, en el Lema \ref{l:prop_ent_conj} probamos que esto ocurre si, y solo si, $X,Y$ son independientes.

El apartado c) del Teorema \ref{t:ent_im} nos dice que $I(X;Y) = H(X)+H(Y)-H(X,Y)$, usando el Lema \ref{l:prop_ent_conj}.2 tendremos que $I(X;Y)\ge 0$, dándose la igualdad si, y solo si, $X,Y$ son independientes.
\end{proof}
%-----------------------------------------------------------------------
%----------------------- CASO CONTINUO ---------------------------------
\subsubsection{Caso continuo}
Hasta ahora las definiciones dadas eran para variables aleatorias discretas, que solo tomaban un conjunto finito de valores. Podemos extender las definiciones estudiadas al caso continuo, para aplicarlas en variables aleatorias continuas. Trabajaremos en este apartado con una variable aleatoria $X$ absolutamente continua con función de distribución $F$, función de densidad $f$ y conjunto soporte $S$.
%Trabajaremos en este caso con una variable aleatoria $X$ con función de distribución $F(x) = Pr[X \leq x]$, si $F$ es continua, la variable aleatoria se dice continua. Sea $f(x) = F'(x)$, cuando la derivada esté definida. Si $\int_{-\infty}^{\infty}f(x)=1,\ f(x)$ se llama la función de densidad de $X$. El conjunto donde $f(x) > 0$ es el soporte de $X$.

Las definiciones a continuación incluyen integrales, por lo que entenderemos en la redacción que todas ellas existen.\\

%deberíamos añadirles un \textit{si existe} a todas ellas. Se pueden construir ejemplos de variables aleatorias cuya función de densidad no exista, o para la que no exista la integral a calcular.

La entropía en el caso continuo pasa a llamarse entropía diferencial y podemos definirla como sigue.

\begin{definition}[Entropía diferencial]
  La entropía diferencial $h(X)$ de una variable aleatoria continua $X$ viene dada por \[
h(X) = - \int_S\log \left(f(x)\right) f(x) dx,
\]
donde $S$ es el soporte de $X$.
\end{definition}
Como en el caso discreto, la entropía diferencial solo depende de la probabilidad de la variable aleatoria, no de los valores que toma. Notamos que en la definición anterior la variable aleatoria $X$ podría ser, o bien, unidimensional, o bien, multidimensional.

\begin{example}\label{ej:uni} Consideramos una variable aleatoria $X$ absolutamente continua con una distribución uniforme sobre $[0,a]$. En este intervalo su densidad es $f(x) = \frac{1}{a}$ para $x\in[0,a]$ y para el resto de puntos es 0. La entropía es \[
h(X) = - \int_0^a\log\left(\frac{1}{a}\right)\frac{1}{a}dx = \log a.
  \]
Notamos que si $a<1, \log a < 0$, esto es, la entropía en el caso continuo puede ser negativa, no como en el caso discreto.\\
\end{example}

\begin{example}\label{ej:norm_uni}
  Consideramos una variable aleatoria $X$ con distribución normal univariante de media $\mu$ y varianza $\sigma^2$, su función de densidad es $f(x) = \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}$, queremos calcular su entropía.
  \begin{align*}
    h(X) &= - \int_{\mathbb{R}} \log \left(f(x)\right) f(x)  dx = - \int_{\mathbb{R}}  \log \left ( \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \right ) \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}dx\\
    &= -  \frac{1}{\sigma \sqrt{2\pi}} \int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \left [ \log \left ( e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \right ) - \log \left(\sigma \sqrt{2\pi}\right ) \right ] dx\\
    &= -  \frac{1}{\sigma \sqrt{2\pi}}\left [ \int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \left ( -\frac{(x-\mu)^2}{2\sigma ^2} \right )dx - \int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \log \left ( \sigma \sqrt{2\pi}\right ) dx \right ].\\
  \end{align*}

  Ahora, utilizando la ley del estadístico inconsciente ($E[g(X)]=  \int g(x)f(x)dx$), tenemos,
  \begingroup
\allowdisplaybreaks
  \begin{align*}
    h(X) &= \frac{\mathbb{E}\left [ (X-\mu) ^2\right ] }{2\sigma^2} + \log\left( \sigma\sqrt{2\pi} \right )\underbrace{\int_{\mathbb{R}} \frac{1}{\sigma\sqrt{2\pi}}e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}dx}_{1}\\
    &= \frac{\sigma^2}{2\sigma^2} + \log \sqrt{\sigma^2 2 \pi} = \frac{1}{2} + \frac{1}{2}\log\left(  \sigma^2 2 \pi\right)\\
    &= \frac{1}{2} \log e + \frac{1}{2} \log\left( \sigma^2 2 \pi\right)\\ &= \frac{1}{2}\log\left({2\pi e\sigma^2} \right) \text{ (nats).}\\
  \end{align*}
  \endgroup
  
\end{example}

%% No siempre tendremos variables unidimensionales, es por ello que definimos también el concepto de entropía para un conjunto de variables.
%% --- No se si hace falta esta definición o si con la anterior sería suficiente al estar considerando variables aleatorias multidimensionales.---
%% \begin{definition}[Entropía diferencial]
%%   La entropía diferencial de un conjunto de variables aleatorias $X_1,\cdots,X_n = X$ con función de densidad $f(x_1,\cdots,x_n) = f(x)$ se define como\[
%% h(X_1,\cdots,X_n) = h(X) = - \int f(x)\log f(x) dx.
%%   \]
%% \end{definition}

\begin{example}\label{ej:norm_multi}
  Tomamos una variable aleatoria $X = (X_1,\cdots,X_n)$ con distribución normal multivariante de vector de medias $\mu$ y matriz de covarianzas $\Sigma$. Su densidad viene dada por $f(x) = \frac{1}{\left (\sqrt{2\pi}\right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$.

  \begin{align*}
    h(X) &= - \int_{\mathbb{R}^n} \log\left( f(x)\right)f(x) dx \\
    &= - \int_{\mathbb{R}^n} \log \left ( \frac{1}{\left ( \sqrt{2\pi} \right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \right )f(x) dx\\
    &= - \int_{\mathbb{R}^n}\left (- \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) - \log\left [ \left (\sqrt{2\pi} \right )^n |\Sigma|^{\frac{1}{2}} \right ]  \right ) f(x) dx\\
    &= \frac{1}{2} \mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(x-\mu) \right ] + \frac{1}{2}\log\left [(2\pi)^n|\Sigma|\right ].
  \end{align*}

  Continuamos desarrollando la expresión $\mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(x-\mu) \right ]$,

  \begin{align*}
    \mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(x-\mu) \right ] &= \mathbb{E}\left [ \sum_{i,j}(X_i - \mu)(\Sigma^{-1})_{ij}(X_j-\mu)\right ]\\
    &= \mathbb{E}\left[ \sum_{i,j}(X_j-\mu)(X_i-\mu)(\Sigma^{-1})_{ij}\right]\\
    &= \sum_{i,j}\underbrace{\mathbb{E}\left[ \sum_{i,j}(X_j-\mu)(X_i-\mu)\right]}_{Cov(X_j,X_i)}(\Sigma^{-1})_{ij}\\
    &= \sum_j\sum_i(\Sigma)_{ji}(\Sigma^{-1})_{ij} = \sum_j (\Sigma\Sigma^{-1})_{jj} = \sum_j (I_n)_{jj} = n.
  \end{align*}

  Sustituyendo este valor en la expresión de la entropía, obtenemos,

  \begin{align*}
    h(X) &= \frac{n}{2} + \frac{1}{2}\log\left [ (2\pi)^n|\Sigma|\right ] = \frac{1}{2} \left (\log e^n + \log \left[(2\pi)^n|\Sigma|\right] \right)\\
    &= \frac{1}{2}\log\left[(2\pi e)^n|\Sigma|\right] \text{ (nats).}\\
  \end{align*}

  
\end{example}

\begin{theorem}
  Sean $X_1,\cdots, X_n$ una serie de variables aleatorias independientes e idénticamente distribuidas con función de densidad $f(x)$. Entonces\[
-\frac{1}{n}\log f(X_1,\cdots, X_n) \xrightarrow[]{P} \mathbb{E}\left [ - \log f(X) \right ] = h(X).
  \]
\end{theorem}

\begin{proof}
  Como las variables son independientes se verifica\[
-\frac{1}{n}\log f(X_1,\cdots, X_n) = - \frac{1}{n} \sum_{i=1}^n \log f(X_i),
\]
por el Teorema \ref{t:lgn} tenemos que la expresión anterior converge en probabilidad a $\mathbb{E}\left [ - \log f(X) \right ] = h(X)$.
\end{proof}


\begin{definition}[Entropía diferencial condicionada]
  Si $X,Y$ son variables aleatorias con función de densidad conjunta $f(x,y)$, definimos la entropía diferencial condicionada $h(X|Y)$ como\[
h(X|Y) = -\int\int  \log \left(f(x|y)\right) f(x,y) dxdy.
\]
Como $f(x|y) = \frac{f(x,y)}{f(y)}$, también podemos definirla como\[
h(X|Y) = h(X,Y) - h(Y).
\]
Siempre que las entropías diferenciales sean finitas.
\end{definition}

\begin{definition}[Entropía relativa]
  La entropía relativa (o ``distancia'' de Kullback-Leibler) $D(f||g)$ entre dos densidades $f$ y $g$ se define como \[
D(f||g) = \int_{\mathbb{R}^n} \log\left( \frac{f(x)}{g(x)}\right)f(x) dx.
  \]
\end{definition}

Notamos que $D(f||g)$ es finita si, y solo si, el conjunto soporte de $f$ está contenido en el conjunto soporte de $g$. 
Al igual que en el caso discreto y motivados por la continuidad consideraremos $0\log\frac{0}{0} = 0$.\\

\begin{definition}[Información mutua]
  La información mutua, $I(X;Y)$ entre dos variables aleatorias con función de densidad conjunta $f(x,y)$ viene dada por\[
I(X;Y) = \int \int \log \left(\frac{f(x,y)}{f(x)f(y)}\right)f(x,y) dxdy = D(f(x,y)||f(x)f(y)).
  \]
\end{definition}

\begin{lemma}\label{l:ent_im}
  La información mutua de dos variables aleatorias $X$, $Y$ verifica:\[
  I(X;Y) = h(X) + h(Y) - h(X,Y).
  \]
\end{lemma}

\begin{proof}
  Supongamos que las funciones de densidad marginales de $X$ e $Y$ son $f(x)$, $f(y)$, respectivamente, y la conjunta es $f(x,y)$.
  Para realizar la prueba partiremos de la definición de información mutua y utilizaremos propiedades del logaritmo y el Teorema de Fubini para llegar al resultado.
  \begin{align*}
    I(X;Y) &= \int \int  \log \left(\frac{f(x,y)}{f(x)f(y)}\right)f(x,y)dxdy \\
    &= \int \int  \log \left(f(x,y)\right)f(x,y)dxdy - \int \int  \log \left(f(x)f(y)\right) f(x,y)dxdy\\
    &= -h(X,Y)-\int\int \log \left(f(x)\right)f(x,y)  dy dx - \int \int \log \left(f(y)\right)f(x,y)dxdy\\
    &= -\int  \log f(x)f(x)dx - \int \log f(y) f(y)dy -h(X,Y)\\
    &= h(X) + h(Y) - h(X,Y).
  \end{align*}
\end{proof}

\begin{lemma}
  La información mutua de dos variables aleatorias $X$, $Y$ se relaciona con la entropía condicional de la siguiente forma:\[
I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X).
  \]
\end{lemma}

\begin{proof}
  Sean $f(x)$, $f(y)$ las funciones de densidad de $X$ e $Y$ y $f(x,y)$ la función de densidad conjunta.
  De manera similar al Lema anterior, partiremos de la definición de información mutua y utlizaremos propiedades del logaritmo, el teorema de Fubini y la definición de función de densidad condicionada.
  \begin{align*}
    I(X;Y)&= \int \int  \log\left( \frac{f(x,y)}{f(x)f(y)}\right)f(x,y)dxdy \\
    &= - \int\int \log \left(f(x)\right)f(x,y)  dx dy + \int\int\log \left(\frac{f(x,y)}{f(y)}\right)  f(x,y)dx dy\\
    &= - \int  \log \left(f(x)\right)f(x) dx + \int\int \log \left(f(x|y)\right)f(x,y) dx dy\\  
    &= h(X) - h(X|Y).
  \end{align*}
De manera análoga se obtiene la segunda igualdad.
\end{proof}

\begin{definition}[Variable aleatoria cuantificada]
  Sea $X$ una variable aleatoria con función de densidad $f$. Si dividimos la imagen de $X$ en intervalos de longitud $\epsilon$ y asumimos que la densidad es constante en cada intervalo, el teorema del valor medio nos dice que, para cada intervalo $\left[i\epsilon, (i+1)\epsilon\right]$ existe un valor $x_i\in \left[i\epsilon, (i+1)\epsilon\right]$ tal que $f(x_i)\epsilon = \int_{i\epsilon}^{(i+1)\epsilon}f(x)dx$.
  La variable aleatoria cuantificada, $X^{\epsilon}$, se define de la siguiente forma\[
X^{\epsilon} = x_i, \quad \text{ si } i\epsilon \leq X  < (i+1)\epsilon.
\]Su función masa de probabilidad viene dada por \[
P[X^\epsilon = x_i] = p_i =  \int_{i\epsilon}^{(i+1)\epsilon}f(x)dx = f(x_i)\epsilon.
\]
\end{definition}

\begin{theorem}
  Si la densidad $f$ de una variable aleatoria $X$ es integrable Riemann, entonces\[
H(X^{\varepsilon}) + \log \epsilon \xrightarrow[]{\epsilon \to 0}  h(X).
  \] 
\end{theorem}
\begin{proof}
  Partimos de la entropía de $X^{\epsilon}$,
  \begin{align*}
    H(X^{\epsilon}) &= - \sum_{-\infty}^{+\infty} p_i \log p_i = - \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i)\epsilon \right )\\
    &= -  \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right ) - \underbrace{\sum_{-\infty}^{+\infty}f(x_i)\epsilon}_{\int_{-\infty}^{+\infty}f(x)dx=1} \log\left ( \epsilon \right ),
  \end{align*}
  luego, tenemos que $H(X^{\epsilon}) + \log \epsilon  = \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right )$.

  Como $f$ es integrable Riemann, tomando $\epsilon \to 0$ obtenemos \[\lim_{\epsilon \to 0}\sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right )  = - \int_{-\infty}^{+\infty}\log\left( f(x)\right) f(x)dx = h(X).\]

  Por tanto,\[
H(X^{\varepsilon}) + \log \epsilon \xrightarrow[]{\epsilon \to 0}  h(X).
  \] 
\end{proof}

Podemos calcular la información mutua de dos variables aleatorias cuantificadas, 
\begin{align}\label{eq:imq}
  I(X^{\epsilon};Y^{\epsilon}) &= H(X^{\epsilon}) - H(X^{\epsilon}|Y^{\epsilon})\nonumber\\&\approx h(X) - \log\epsilon - (h(X|Y) - \log\epsilon) = I(X;Y).
\end{align}

De forma más general, podemos definir la información mutua en términos de particiones finitas de la imagen de la variable aleatoria. Sea $\mathcal{X}$ la imagen de la variable aleatoria $X$. Una partición $\mathcal{P}$ de $\mathcal{X}$ es una colección finita de conjuntos disjuntos $P_i$ de forma que $\cup_i P_i = \mathcal{X}$. La cuantificación de $X$ dada la partición $\mathcal{P}$, que notaremos $[X]_{\mathcal{P}}$, es la variable aleatoria discreta con función masa de probabilidad\[
P\left[[X]_{\mathcal{P}} = i\right ] = P [X\in P_i] = \int_{P_i}dF(x).
\]
Utilizando esto para dos variables aleatorias con sendas particiones, podremos calcular la información mutua de las variables cuantificadas utilizando \ref{eq:imq}. De esta forma, obtenemos una nueva definición de la información mutua.

\begin{definition}[Información mutua]
  La información mutua de dos variables aleatorias $X$ e $Y$ viene dada por \[
I(X;Y) = \sup_{\mathcal{P},\mathcal{Q}}I\left ( [X]_{\mathcal{P}}, [Y]_{\mathcal{Q}} \right),
  \]donde el supremo es sobre todas las particiones finitas $\mathcal{P}$ y $\mathcal{Q}$.
\end{definition}
Notamos que esta definición será válida cuando las variables aleatorias no tengan definida su función de densidad.

%REVIEW
%---Lo que viene a continuación no lo tengo del todo claro---
%De la misma forma, podemos obtener otra definición de la entropía relativa. Considerando para una función de densidad, $f$, su cuantificación dada por una partición, $\mathcal{Q}$, de manera análoga a como lo hicimos para una variable aleatoria. Particionamos el dominio de $f$ 

\begin{theorem}Si $f$ y $g$ son dos funciones de densidad, entonces\[
D\left ( f || g \right ) \ge 0
\]
con igualdad si, y solo si, $f = g$ en casi todo punto.
\end{theorem}
\begin{proof}
%REVIEW  
\end{proof}

\begin{corollary}\label{c:im_indc}
$I(X;Y)\ge 0$ con igualdad si, y solo si, $X$ e $Y$ son independientes.
\end{corollary}

\begin{corollary}
$h(X|Y) \le 0$ con igualdad si, y solo si, $X$ e $Y$ son independientes.
\end{corollary}

\begin{theorem}[Regla de la cadena para la entropía diferencial] Sean $X_1,\dots, X_n$ variables aleatorias, entonces
  \[
h(X_1,\cdots,X_n) = \sum_{i=1}^n h(X_i|X_1,\cdots, X_{i-1}).
  \]
\end{theorem}

\begin{proof}
  Realizaremos la prueba por inducción sobre el número de variables.
  \begin{itemize}
  \item Si $n=2$, entonces $h(X_1,X_2) = h(X_1) + h(X_2|X_1)$ por definición de la entropía diferencial condicionada.
  \item Suponiendo cierto el caso $n-1$, veamos que también se cumple para $n$, utilizando otra vez la definición de entropía diferencial condicionada:
    \begin{align*}
h(X_1,\dots,X_n) &= h(X_n|X_1,\dots, X_{n-1}) + h(X_1,\dots, X_{n-1})\\ &= h(X_n|X_1,\dots,X_{n-1}) + \sum_{i=1}^{n-1}h(X_i|X_1,\dots,X_{i-1}).
    \end{align*}
  \end{itemize}
\end{proof}

\begin{corollary}
  \[h(X_1,\cdots, X_n) \leq \sum_i h(X_i),\]
  dándose la igualdad si, y solo si, $X_1,\cdots, X_n$ son independientes.
\end{corollary}
% EN la prueba se podría usar el leman 2.21 para ver que h(X) >= h(X|Y)

\begin{theorem}
  Sea $X$ una variable aleatoria absolutamente continua y $c$ un vector de escalares, entonces
  \[
  h(X+c) = h(X).
  \]
  La entropía diferencial no cambia por traslaciones.
\end{theorem}

\begin{proof}
  Sea $f_X$ la función de densidad de $X$. Llamemos $Y = X+ c$, esta será una variable aleatoria cuya función de densidad se relaciona con la de $X$ como sigue: $f_Y(y)=f_X(y-c)$. Calculamos su entropía diferencial usando su definición y aplicando el cambio de variable $y = x + c$.
  \begin{align*}
    h(X+c) &= h(Y) = - \int \log \left(f_Y(y)\right)f_Y(y) dy\\
    &= - \int  \log \left(f_X(y-c)\right) f_X(y-c) dy\\
    &= - \int  \log \left(f_X(x)\right) f_X(x) dx = h(X).
  \end{align*}
\end{proof}

\begin{theorem}
  Sea $X$ una variable aleatoria unidimensional y $a$ un escalar no nulo, entonces\[
h(aX) = h(X) + \log |a|.
  \]
\end{theorem}

\begin{proof}
  Llamemos $Y = aX$, entonces $f_Y(y) = \frac{1}{|a|}f_X\left (\frac{y}{a} \right )$. Usaremos la definición de entropía, la función de densidad recién calculada y un cambio de variables ($x = \frac{y}{a}$):
  \begin{align*}
    h(aX) &= h(Y) = -\int \log \left(f_Y(y)\right) f_Y(y)dy\\
    &=  -\int \log \left ( \frac{1}{|a|}f_X\left (\frac{y}{a} \right ) \right ) \frac{1}{|a|}f_X\left (\frac{y}{a} \right ) dy\\
    &= -\int  \log \left ( \frac{1}{|a|}f_X\left (x \right ) \right )\frac{|a|}{|a|}f_X\left (x \right ) dx\\
    &= -\int \log \left (f_X\left (x \right ) \right ) f_X\left (x \right ) dx + \int\log\left({|a|}\right) f_X(x)dx\\
    &= h(X) + \log{|a|}.
  \end{align*}
\end{proof}

\begin{corollary}
  Si $X$ es una variable aleatoria multidimensional y $A$ una matriz entonces,\[
h(AX) = h(X) + \log|det(A)|.
  \]
\end{corollary}

%% Veremos que la distribución normal multivariante maximiza la entropía sobre todas las distribuciones con la misma covarianza.

%% \begin{theorem}
%%   Sea una variable aleatoria $X\in \mathbb{R}^n$ con media cero y covarianza $\Sigma = \mathbb{E}\left[XX^T\right ]$ (esto es, $\Sigma_{ij} = \mathbb{E}\left[X_iX_j\right ],\ 1\leq i,j \leq n$). Entonces \[
%%   h(X) \leq \frac{1}{2}\log\left(2\pi e\right ) ^n |\Sigma |,\]
%%   con igualdad si, y solo si, $X \sim \mathcal{N}(0,\Sigma)$.
%% \end{theorem}

\subsection{Estimación de la entropía y de la información mutua}

Generalmente no conocemos las funciones de densidad de las variables aleatorias cuya entropía o información mutua queremos calcular, por ello, tratamos de aproximar este valor a partir de información que sí conocemos, como realizaciones muestrales de las variables. El objetivo es, para dos variables aleatorias $X,Y$, estimar $I(X;Y)$ a partir de un conjunto de realizaciones muestrales, $\{z_i=(x_i,y_i) : i = 1,\cdots, N\}$, sin conocer las funciones de densidad $f=f_Z, f_X$ y $f_Y$, donde $Z = (X,Y$ es la distribución conjunta de $X$ e $Y$.

Nos centraremos en los estimadores de Kozachenko - Leonenko, aunque presentaremos brevemente otras dos posibles estimaciones.

\subsubsection{Estimador basado en particiones}

Particionamos el soporte de $X$ e $Y$ en \textit{contenedores} de tamaño finito (no necesariamente el mismo) y aproximamos la información mutua por \[
I(X,Y) \approx I_{\textit{binned}}(X,Y) = \sum_{i,j} p(i,j) \log \frac{p(i,j)}{p_x(i)p_y(j)},
\]
donde $p_x(i) = \int_i f_x(x)dx$, $p_y(j) = \int_j f_y(y)dy$, $\int_i$ representa la integral sobre el i-ésimo contenedor, y $p(i,j) = \int_i\int_jf(x,y)dxdy$.

Notamos $n_x(i)$ al número de puntos en el i-ésimo contenedor de $X$, análogamente $n_y(j)$. $n(i,j)$ es el número de puntos en la intersección de $n_x(i)$ y $n_y(j)$.

Lo que hacemos es aproximar $p_x(i) \approx \frac{n_x(i)}{N}, p_y(j) \approx \frac{n_y(j)}{N}, p(i,j) \approx \frac{n(i,j)}{N}$.

Si todas las densidades son funciones \textit{proper}, cuando $N$ tienda a infinito y el tamaño de cada contenedor converja a cero, la estimación $I_{\textit{binned}}(X,Y) \to I(X;Y)$.

---Con \textit{proper function} entiendo que el artículo hace referencia a esto: \url{https://en.wikipedia.org/wiki/Proper\_convex\_function}. Es un concepto que no he visto y no sé cómo traducir.

\subsubsection{Estimadores de la entropía basados en los $k$ vecinos más cercanos}

Los estimadores estudiados a continuación, realizan una estimación de la función de densidad de la variable aleatoria $X$ a partir de los $k$ vecinos más cercanos, un tipo de estimación no paramétrica. Prosiguen viendo la entropía como la esperanza del $\log(X)$ negada y utilizando el estimador de la densidad de $X$ obtenido anteriormente para estimar la entropía.\\

Sea $X$ una variable aleatoria $d$-dimensional con función de densidad $f$ y $x_1,\dots,x_n$ una realización muestral, $x_i\in \mathbb{R}^d$. Para cada punto, $x$, ordenamos todas las observaciones según la distancia al punto dado. Notaremos $\epsilon_k(x)$ a la distancia entre $x$ y el $k$-ésimo punto más cercano a él, al que llamaremos su $k$-ésimo vecino más cercano (\textit{$k$-nearest neighbour}, $k$-nn).\\

Consideramos la bola de centro $x$ y radio $\epsilon_k(x)$,\[
B(x, \epsilon_k(x)) = \{y \in \R^d: \Vert x - y\Vert \leq \epsilon_k(x)\},
\]que verifica \begin{align}\label{eq:knn-ind}
\frac{k}{n} = \frac{1}{n} \sum_{i=1}^nI\left((x_i\in B\left(x,\epsilon_k(x)\right)\right),
\end{align}
donde $I$ es la función indicadora. La cantidad $\frac{1}{n} \sum_{i=1}^nI\left((x_i\in B(x,\epsilon_k(x))\right)$ la podemos ver como un estimador de \[
P[x_i\in B(x,\epsilon_k(x))] \approx \int_{B(x,\epsilon_k(x))}f(y)dy.
\]

Cuando $n$ es grande y $k$ es relativamente pequeño en comparación, $\epsilon_k(x)$ será pequeño porque $\frac{k}{n}$ lo es.
---Esto no sé por qué---

Luego, la densidad $f(y)$ en la bola $B(x,\epsilon_k(x))$ no cambiará demasiado. Para cualquier punto de la bola consideraremos que su densidad coincide con la del centro de la misma, $f(y)\approx f(x)$. Tenemos entonces\[
P[x_i\in B(x,\epsilon_k(x))] \approx \int_{B(x,\epsilon_k(x))}f(y)dy\approx f(x) \int_{B(x,\epsilon_k(x))}dy = f(x)\cdot V_d \cdot \epsilon^d_k(x),
\]
donde $V_d\cdot \epsilon^d_k(x)$ es el volumen de la bola $d$ dimensional de radio $\epsilon_k(x)$.

Utilizando \ref{eq:knn-ind} para estimar $P[x_i\in B(x,\epsilon_k(x))]$, tendremos,\[
f(x)\cdot V_d \cdot \epsilon_k(x) \approx P[x_i\in B(x,\epsilon_k(x))]\approx  \frac{1}{n} \sum_{i=1}^nI\left((x_i\in B\left(x,\epsilon_k(x)\right)\right) = \frac{k}{n}.
\]
Considerando\[
f(x)\cdot V_d \cdot \epsilon^d_k(x) \approx \frac{k}{n}
\]
se obtiene el estimador de la función de densidad, \[
\hat{f}(x) = \frac{k}{n}\frac{1}{V_d \cdot \epsilon^d_k(x)}.
\]

Este estimador verifica,
\begin{align*}
  \bias\left(\hat{f}(x)\right) &= O \left(\left(\frac{k}{n}\right)^{\frac{2}{d}} + \frac{1}{k}\right),\\
  \Var\left(\hat{f}(x)\right) &= O \left(\frac{1}{k}\right).
\end{align*}

Su comportamiento asintótico no depende solo del tamaño de la muestra sino también de la dimensión $d$ y del valor $k$ elegido. De hecho, la varianza depende exclusivamente del valor de $k$, la variabilidad del estimador viene dada por el número de vecinos más cercanos a considerar. 

-----

Si $X$ es una variable aleatoria unidimensional, podemos ordenar los distintos $x_i$ (realizaciones muestrales de $X$). Si, además, $x_{i+1} -x_i \to 0$ y $N\to \infty$, aproximamos:\[
H(X)\approx \frac{1}{N-1}\sum_{i=1}^N \log(x_{i+1}-x_i) + \psi(1) - \psi(N),\]
donde $\psi(x) = \frac{\Gamma'(x)}{\Gamma(x)}$ es la \textit{función digamma}. La función digamma verifica las siguientes propiedades:
\begin{itemize}
\item $\psi(x+1) = \psi(x) + \frac{1}{x}$.
\item $\psi(1) = -C = 0.5772156\dots$, la constante de Euler - Mascheroni.
  \item Para $x$ lo suficientemente grandes, $\psi(x) \approx \log x - \frac{1}{2x}$.
\end{itemize}

La estimación de la entropía propuesta es para $k=1$, existen fórmulas similares que utilizan $x_{i+k}-x_i$  en vez de $x_{i+1}-x_i$, con $k < N$.

Aunque la estimación dada, y sus generalizaciones a $k>1$, parezca ser la mejor para la entropía, $H(X)$, no sirve para calcular la información mutua, porque no es obvio cómo generalizarla a más dimensiones.

\subsubsection{Estimadores de Kozachenko - Leonenko}

Asumimos que tenemos unas métricas dadas para los espacios $X$, $Y$, $Z = (X,Y)$. Para cada punto $z_i = (x_i, y_i)$ sus vecinos según la distancia $d_{i,j}=\Vert z_i-z_j \Vert$ serían $d_{i,j_1} \leq d_{i,j_2} \leq \cdots$. Podríamos obtener los vecindarios de $X$ e $Y$ de manera similar.

Estimaremos la entropía de una variable $X$ promediando la distancia media de los $k$ vecinos más cercanos de todos los $x_i$. De esta forma, estimaremos $H(X), H(Y)$ y $H(X,Y)$. A continuación, estimaremos la información mutua utilizando el Lema \ref{l:ent_im}.\\

Sea $X$ una variable aleatoria continua con valores en un espacio métrico, es decir, hay definida una distancia, $\Vert x - x'\Vert$, entre dos realizaciones de $X$, y sea $f(x)$ la función de densidad una función $proper$. Nuestro objetivo es estimar la entropía $H(X)$ a partir de una muestra aleatoria simple $(x_1,\cdots,x_N)$ de $N$ realizaciones de $X$.

La ecuación de la entropía, $H(X) = - \int f(x) \log f(x) dx$, se podría entender (salvo el signo menos) como una media de $\log f(x)$. Si tuviéramos un estimador insesgado, $\widehat{\log f}(x)$, del mismo, podríamos obtener un estimador insesgado de la entropía:\[
\widehat{H}(x) = - \frac{1}{N} \sum_{i=1}^N\widehat{\log f}(x_i).\]

Para obtener el estimador $\widehat{\log f}(x_i)$ consideramos la distribución de probabilidad $P_k(\varepsilon)$ de la distancia entre $x_i$ y su $k$-ésimo vecino más cercano. La probabilidad $P_k(\varepsilon)d\varepsilon$ ---no entiendo por qué aquí añade el $d\varepsilon$--- es igual a la probabilidad de que haya un punto a distancia $r\in \left[ \frac{\varepsilon}{2}, \frac{\varepsilon}{2} + \frac{d\varepsilon}{2} \right ]$ de $x_i$, de que haya otros $k-1$ puntos a menor distancia; y de que $N-k-1$ puntos se encuentren a mayor distancia de $x_i$.

Llamamos $p_i$ a la función masa de probabilidad  de la bola centrada en $x_i$ y radio $\varepsilon$, $p_i(\varepsilon) = \int_{\Vert s - x_i \Vert < \frac{\varepsilon}{2}}f(s)ds$. 

Usaremos la fórmula de la distribución multinomial para expresar $P_k(\varepsilon)d\varepsilon$. Recordamos que la distribución multinomial nos da la probabilidad de un número de éxitos $k$ en $N$ sucesos de Bernouilli (estos sucesos toman valor 1 para la probabilidad de éxito  y 0 para la de fracaso) independientes y equiprobables, tiene como función de densidad $\frac{N!}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}$. Luego,\[
P_k(\varepsilon)d\varepsilon = \frac{(N-1)!}{1!(k-1)!(N-k-1)!} \cdot \frac{d p_i(\varepsilon)}{d\varepsilon} d\varepsilon \cdot p_i^{k-1} \cdot (1-p_i)^{N-k-1}.
\]

--- En la ecuación anterior no entiendo el término $ \frac{d p_i(\varepsilon)}{d\varepsilon} $, entiendo que tiene que ser la probabilidad de que haya un punto justo a distancia $\varepsilon$ de $x_i$ pero no sé por qué se calcula así---

A partir de ahí, integrando, calculamos,\[
P_k(\varepsilon) = k\binom{N-1}{k}
\cdot \frac{d p_i(\varepsilon)}{d\varepsilon} \cdot p_i^{k-1} \cdot (1-p_i)^{N-k-1}.
\]

Se comprueba que $\int P_k(\varepsilon)d\varepsilon = 1$ ---no me salió ---, que debe cumplir para ser función de distribución.

Podemos calcular la esperanza de $\log p_i (\varepsilon)$, a partir de la posición de los $N-1$ puntos restantes, con $x_i$ fijo:

--- No sé por qué se verifican las igualdades siguientes---
\begin{align*}
\mathbb{E}(\log p_i) &= \int_0^\infty P_k(\varepsilon) \log p_i(\varepsilon) d\varepsilon =  k\binom{N-1}{k} \int_0^1 p^{k-1}(1-p)^{N-k-1}\log p\ dp\\ &= \psi (k) - \psi(N).
\end{align*}

Obtenemos un estimador de $\log f(x)$ asumiendo que $f(x)$ es constante en la bola de radio $\varepsilon$. --- ¿por qué es necesario asumir esto?, ¿cómo obtiene la aproximación siguiente?--- Esto nos da $p_i (\varepsilon) \approx c_d \varepsilon^d f(x_i)$, donde $d$ es la dimensión de $X$ y $c_d$ es el volumen de la bola unidad $d$-dimensional. La constante $c_d$ depende de la norma, para la del máximo tenemos $c_d=1$, mientras que para la euclídea $c_d=\frac{\pi^{d/2}}{\Gamma\left ( 1+ \frac{d}{2} \right ) / 2^d}$.

Tomando logaritmos sobre la aproximación de $p_i(\varepsilon)$, obtenemos:\[
\log p_i(\varepsilon) \approx \log c_d + d \log \varepsilon + \log f(x_i)
\]
Continuamos tomando esperanzas respecto de $\varepsilon$ y sustituyendo la esperanza del logaritmo ya calculada:\[
\log f(x_i) \approx \mathbb{E}(\log p_i) - \log c_d  - d \mathbb{E} (\log \varepsilon) = \psi (k) - \psi (N) - \log c_d  - d \mathbb{E} (\log \varepsilon).
\]

Hemos calculado el estimador $\widehat{\log f} (x_i)$, sustituiremos de la forma propuesta para obtener un estimador de la entropía:
\begin{align}
  \widehat{H}(X) &= - \frac{1}{N} \sum_{i=1}^N\widehat{\log f} (x_i) = - \frac{1}{N} \sum_{i=1}^N\left (\psi (k) - \psi (N) - \log c_d  - d\log \varepsilon \right ) \nonumber \\
  &= - \psi (k) + \psi (N) + \log c_d + \frac{d}{N} \sum_{i=1}^N \log \varepsilon (i), \label{eq:est_ent}
\end{align}
donde $\varepsilon(i)$ es dos veces la distancia de $x_i$ a su $k$-ésimo vecino más cercano.
---En la ecuación anterior no sé por qué desaparece la esperanza---


Por cómo lo hemos obtenido, $\widehat{H}(X)$ será insesgado si la densidad $f(x)$ es estrictamente constante. --- Esto no sé por qué ---

Consideremos ahora la variable aleatoria conjunta $Z=(X,Y)$ con la norma del máximo: $\Vert z - z' \Vert = \max \{\Vert x-x' \Vert, \Vert y-y'\Vert\}$, donde las normas en $X$ e $Y$ pueden ser cualesquiera, no necesariamente la misma.

Tomamos uno de los $N$ puntos, $z_i$, y notamos $\frac{\varepsilon(i)}{2}$ la distancia de este punto a su $k$-ésimo vecino. $\frac{\varepsilon_x(i)}{2}$ y $\frac{\varepsilon_y(i)}{2}$ reflejan la misma distancia proyectada en el subespacio $X$ e $Y$ respectivamente. $\varepsilon(i) = \max \{\varepsilon_x(i), \varepsilon_y(i)\}$.

Distinguiremos dos algoritmos diferentes para estimar la información mutua según si consideramos los vecinos que se encuentren en el cuadrado de lado $\varepsilon(i)$, o en el rectángulo de lados $\varepsilon_x(i)$, $\varepsilon_y(i)$. 

\paragraph{Primer algoritmo}

Este algoritmo se caracteriza por definir $n_x(i)$ como el número de puntos $x_j$ cuya distancia a $x_i$ es menor que $\frac{\varepsilon(i)}{2}$, de forma análoga se define $n_y(i)$. Como $\varepsilon(i)$ es una variable aleatoria, $n_x(i),\ n_y(i)$ también lo son. $\frac{\varepsilon(i)}{2}$ es una variable aleatoria con distribución dada por $P_k(\varepsilon)$, luego $\mathbb{E}(\log p_i) = \psi(k) - \psi(N)$ se cumple también en este caso. Para utilizar el estimador de la entropía calculado anteriormente debemos cambiar: $x_i$ por $z_i=(x_i,y_i)$, $d$ por $d_X,\ d_Y$ y $c_d$ por $c_{d_X}c_{d_Y}$. Con estas modificaciones obtenemos\[
\widehat{H}(X,Y) = - \psi(k) + \psi(N) + \log (c_{d_X}c_{d_Y}) + \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i)).
\]

Para obtener $I(X;Y)$ tenemos que restar este valor a las estimaciones de $H(X)$ y $H(Y)$. Podríamos usar el mismo $k$ para realizar estas estimaciones, pero entonces usaríamos diferentes escalas en el espacio conjunto que en los marginales. Para un $k$ fijo, la distancia al $k$-ésimo vecino en el espacio conjunto sería mayor que las distancias a los vecinos en los espacios marginales. El sesgo en el estimador de la entropía marcado por la no uniformidad de la densidad depende de estas distancias, los sesgos en $\widehat{H}(X)$, $\widehat{H}(Y)$, $\widehat{H}(X,Y)$ no se cancelarían. Para evitar eso, notamos que la estimación se cumple para cualquier valor de $k$ y que no tenemos porqué mantenerlo fijo.

Suponiendo que el $k$-ésimo vecino de $x_i$ se encuentra en uno de los lados verticales del cuadrado de lado $\varepsilon(i)$, si hay $n_x(i)$ puntos en la recta vertical $x = x_i \pm \frac{\varepsilon(i)}{2}$, entonces $\frac{\varepsilon(i)}{2}$ es la distancia al $n_x(i) + 1$ vecino de $x_i$ y estimamos su entropía mediante\[
\widehat{H}(X) = - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log c_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon(i).
\]
---En la ecuación anterior, ¿$x$ estaría fijo?---

Para la variable $Y$ podría no cumplirse exactamente así, ya que $\varepsilon(i)$ no tendría por qué ser dos veces la distancia al $(n_y(i)+1)$-ésimo vecino. Sin embargo, consideramos la ecuación anterior una buena aproximación para $H(Y)$ (cambiando $X$ por $Y$). Esta aproximación se vuelve exacta cuando $n_y(i) \to \infty$ y cuando $N\to \infty$.

Usando estas estimaciones en la ecuación correspondiente, obtenemos:
\begin{align*}
  I^{(1)}(X,Y) \approx & \widehat{H}(X) + \widehat{H}(Y) - \widehat{H}(X, Y)\\ =& - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log c_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & - \frac{1}{N}\sum_{i=1}^N\psi(n_Y(i)+1) + \psi(N) + \log c_{d_Y} + \frac{d_Y}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & + \psi(k) - \psi(N) - \log (c_{d_X}c_{d_Y}) - \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i))\\
  =& \psi(k) - \frac{1}{N}\sum_{i=1}^N\left( \psi(n_x(i)+1) + \psi(n_y(i)+1) \right ) + \psi(N).
\end{align*}

Notamos $\langle \cdots \rangle$ a la media sobre todos los $i\in\{1,\cdots,N\}$ y sobre todas las realizaciones de las muestras aleatorias,\[
\langle \cdots \rangle = \frac{1}{N} \sum_{i=1}^N\mathbb{E}[\cdots(i)].
\]

Utilizando esta notación la estimación de la entropía quedaría:\[
I^{(1)}(X,Y) \approx \psi(k) - \langle \psi(n_x+1) + \psi(n_y+1) \rangle + \psi(N).
\]
--- No entiendo cómo aplicamos $\langle \cdots \rangle$, ¿no faltaría una esperanza?---

El principal inconveniente de esta estimación es que solo usamos el estimador de Kozachenko - Leonenko correctamente en una dirección marginal. Esto parece inevitable si queremos usar (hiper)cubos en el espacio conjunto, como opción alternativa podríamos usar (hiper)rectángulos.

\paragraph{Segundo algoritmo} En este caso, $n_x(i)$ y $n_y(i)$ serán el número de puntos con $\Vert x_i - x_j \Vert \leq \frac{\varepsilon_x(i)}{2}$ y $\Vert y_i - y_j \Vert \leq \frac{\varepsilon_y(i)}{2}$ respectivamente. Tenemos que distinguir dos casos:

\begin{enumerate}[label={\alph*)}]
\item Los lados $\varepsilon_x(i)$ y $\varepsilon_y(i)$ quedan determinados por el mismo punto.
\item Los lados $\varepsilon_x(i)$ y $\varepsilon_y(i)$ quedan determinados por puntos diferentes.
\end{enumerate}

En ambos casos tenemos que cambiar $P_k(\varepsilon)$ por una probabilidad 2-dimensional:\[
P_k(\varepsilon_x, \varepsilon_y) = P_k^{a)}(\varepsilon_x, \varepsilon_y) + P_k^{b)}(\varepsilon_x, \varepsilon_y),
\]
---Esto no sé por qué---
con \[
P_k^{a)}(\varepsilon_x, \varepsilon_y) = \binom{N-1}{k} \frac{d^2[q_i^k]}{d\varepsilon_x d \varepsilon_y}(1-p_i)^{N-k-1}
\]
y\[
P_k^{b)}(\varepsilon_x, \varepsilon_y) = (k-1)\binom{N-1}{k} \frac{d^2[q_i^k]}{d\varepsilon_x d \varepsilon_y}(1-p_i)^{N-k-1},
\]
donde $q_i \equiv q_i(\varepsilon_x, \varepsilon_y)$ es la función de densidad del rectángulo de lados $\varepsilon_x \times \varepsilon_y$ centrado en el punto $(x_i, y_i)$, y $p_i$ es la función de densidad del cuadrado de lado $\varepsilon = \max \{\varepsilon_x, \varepsilon_y\}$. Esta última es necesario porque usando la norma del máximo garantizamos que no hay puntos en ese cuadrado que no estén en el cuadrado.

Se verifica que $\int P_k =1$. Calculamos la esperanza de $\log p_i$:
---No sé por qué se verifican las igualdades siguientes---
\[
\mathbb{E}(\log q_i) = \int \int_0^{\infty} P_k(\varepsilon_x, \varepsilon_y) \log q_i(\varepsilon_x, \varepsilon_y) d \varepsilon_x d\varepsilon_y
= \psi(k) - \frac{1}{k} - \psi(N)
\]
--- ¿Se cumple $pi\approx c_d\varepsilon^df(x)$ y podemos proceder de forma similar al caso anterior?---

La información mutua se aproximaría por
\begin{align}
I^{(2)}(X,Y) \approx \psi(k) - \frac{1}{k} - \langle \psi(n_x) + \psi(n_y) \rangle + \psi(N). \label{eq:est2}
\end{align}


En general ambas fórmulas dan resultados similares. Para un mismo $k$, $I^{(1)}$ da ligeramente menores errores estadísticos (porque $n_x(i)$ y $n_y(i)$ tienden a ser más grandes y tener menores variaciones relativas), pero tienen mayores errores sistemáticos. Este último es grave solo si estamos interesados en variables con dimensiones muy altas, donde $\varepsilon(i)$ tiende típicamente a ser más grande que la marginal $\varepsilon_x(i)$. En ese caso parece preferible el segundo algoritmo, de resto se pueden usar los dos.

\subsection{Implementaciones de estimadores de la entropía y de la información mutua}
En esta sección estudiaremos un par de implementaciones de conceptos anteriores como la entropía o la estimación mutua. En general, dado un número finito de muestras de una variable aleatoria no podemos calcular de forma exacta la entropía o información mutua, ya que no conocemos su distribución. Para implementarlas es necesario usar algún estimador, como los estudiados en el apartado anterior.

\subsubsection{Implementación 1 - GaelVaroquaux}

Comenzaremos revisando la implementación \cite{estimating}. Para ello leeremos el código y nos formaremos una idea de qué hace y cómo cada una de las funciones.

\begin{itemize}
\item \texttt{nearest\_distances}: para cada punto de un vector \texttt{X}, devuelve la distancia a su $k$-ésimo vecino más cercano. Se implementa haciendo uso de las funciones de \texttt{scikit-learn}.

\item \texttt{entropy\_gaussian}: entropía de una variable gaussiana, se utilizará para hacer \textit{tests}. Si la matriz de covarianzas \texttt{C} es escalar, se utiliza la siguiente fórmula para la entropía:\[
  H(X) = \frac{1}{2}\left ( 1 + \log(2\pi) \right )+ \frac{1}{2} \log C.
  \]
Vimos en \ref{ej:norm_uni} que la entropía de la gaussiana con covarianza $\sigma^2$ es:\[
h(X) = \frac{1}{2}\log (2\pi e \sigma^2 ),
\]
coincidiendo con la implementación.

Para el caso en que la matriz \texttt{C} sea no degenerada, utiliza la expresión:\[
H(X_1,\cdots, X_n) = \frac{n}{2} \left (1 + \log(2\pi) \right) + \frac{1}{2}\log |C|,
\]

Coincidiendo con la fórmula vista en \ref{ej:norm_multi}.

\item \texttt{entropy}: calcula la entropía de la variable \texttt{X} usando el estimador de Leonenko - Kozachenko \ref{eq:est_ent}. Comienza inicializando los elementos que se necesitan para aplicar la fórmula: vecinos más cercanos para todos los elementos del vector, dimensiones de \texttt{X}, volumen de la bola unidad ($c_d$). Notamos que en el cálculo de la constante $c_d$ (\texttt{volume\_unit\_ball}) falta un $\frac{1}{2^d}$. Otra diferencia es que en el estimador propuesto, $\varepsilon$ era dos veces la distancia del punto al vecino, mientras que en \texttt{r} tenemos la distancia del punto a su vecino más cercano una sola vez. Donde el estimador estudiado ponía $\varepsilon$ en esta implementación pone \texttt{r + eps}, donde \texttt{eps} es el menor valor que al ser sumado varía el resultado (de forma que  \texttt{1+eps $\neq$ 1}). En ambas implementaciones se añade un ínfimo ruido a los datos, para evitar que varios puntos tengan exactamente las mismas coordenadas. Devuelve el resultado de sustituir estos valores en la fórmula \ref{eq:est_ent}.

\item \texttt{mutual\_information}: devuelve la información mutua entre cualquier número de variables. Para ello suma las entropías marginales de cada una de ellas y les resta la entropía conjunta (usando el mismo valor $k$ para ambas estimaciones).---Esta fórmula habría que probarla y añadirla a la parte de teoría (la tenemos para 2 variables)--- % REVIEW: Referenciar lema correspondiente

\item \texttt{mutual\_information\_2d}: calcula la información mutua entre dos variables unidimensionales a partir de su histograma conjunto. 

\item \texttt{test\_entropy}: permite probar la entropía comparando la entropía de una variable aleatoria gaussiana, que conocemos teóricamente y se calcula en \texttt{entropy\_gaussian}, con la entropía estimada por \texttt{entropy}. Si ambos valores difieren demasiado, saltará un \textit{assert}.

\item \texttt{test\_mutual\_information}: permite probar la información mutua de una variable aleatoria 2 dimensional estimando los resultados para una variable gaussiana y comparándolos con los resultados teóricos.

\item \texttt{test\_degenerate}: comprueba que los estimadores no dan valores degenerados.

\item \texttt{test\_mutual\_information\_2d}: es una función similar a \texttt{test\_mutual\_information}, pero para probar \texttt{mutual\_information\_2d}.
\end{itemize} 

Si ejecutamos el código como está, se producen como salida dos parejas. La primera es la estimación de la información mutua entre 2 variables normales correladas usando la función \texttt{mutual\_information}. La segunda muestra lo mismos resultados para la función \texttt{mutual\_information\_2d}. En ambos casos a la izquierda está el valor obtenido y a la derecha el valor teórico. Se obtienen los siguientes resultados:
\begin{lstlisting}
(0.1092329232318674, 0.11157177565710485)
(2.1173231240729757, 2.2033596236321267)
\end{lstlisting}

Añadimos a esta tupla la diferencia entre ambos valores:
\begin{lstlisting}
(0.1092329232318674, 0.11157177565710485, 0.0023388524252374587)
(2.1173231240729757, 2.2033596236321267, 0.08603649955915094)
\end{lstlisting}

Observamos una pequeña diferencia entre los valores estimados y los teóricos.

\subsubsection{Implementación 2 -  gregversteeg}

Pasamos a revisar la segunda implementación, que encontramos en \cite{npeet}. Como en el caso anterior, comenzamos leyendo el código función por función. 
\begin{itemize}
\item \texttt{entropy}: Leonenko - Kozachenko \ref{eq:est_ent} basado en los $k$ vecinos. \texttt{x} debe ser una lista de vectores. En primer lugar, hay un \textit{assert} para que el $k$ sea mayor que el número de muestras. Se convierte la lista \texttt{x} en un \textit{array}, almacena el número de elementos y de características, se añade ruido a los elementos de \texttt{x} (en la función \texttt{add\_noise}), se obtiene el árbol de vecinos usando la función correspondiente de scikit-learn. Se obtienen los $k$ vecinos más cercanos de \texttt{x} usando este árbol y la función \texttt{query\_neighbors} que llama a la función \texttt{query} correspondiente de scikit-learn, esta función devolverá la distancia a los vecinos más cercanos y sus índices, pero nos quedamos solo con las distancias. La función devuelve el siguiente valor:\[
  \frac{\psi(N) - \psi(k) + d \cdot \log 2 + \frac{d}{N}\sum \varepsilon(i)}{\log(base)}.
  \]

  Este $\log (base)$ se utiliza para que la fórmula pase a estar en base 2. La fórmula coincide con la estudiada salvo porque la constante $\log c_d$ es sustituida por $d \cdot \log 2$.

\item \texttt{centropy}: estima la entropía de $X|Y$, para ello usa la función anterior para calcular las entropías de $(X,Y)$ e $Y$. Estima la entropía condicionada como $H(X|Y) = H(X,Y)-H(Y)$.
\item \texttt{tc}: calcula la entropía de cada característica, devuelve la suma de las entropías de las características menos la entropía de la variable.
\item \texttt{ctc}: similar al anterior pero con entropías condicionadas.
\item \texttt{corex}: similar a \texttt{tc} pero en vez de entropías calcula la información mutua.
\item \texttt{im}: calcula la información mutua entre \texttt{x, y}, utilizando el estimador \ref{eq:est2}. Comienza comprobando que ambos vectores tienen la misma longitud y que $k$ es menor que la misma. Hace las transformaciones con los datos correspondientes y les añade ruido. Si se ha pasado como parámetro también la  \texttt{z}, la información mutua a calcular será condicionada a este valor así que añadimos el valor z al conjunto de puntos. De forma similar a como lo hizo en \texttt{entropy}, construye el árbol de los vecinos y almacena la distancia de cada elemento al $k$-ésimo vecino más cercano. A continuación, utiliza la función \texttt{avgdigamma} para encontrar el número de puntos a distancia menor que el vecino $k$-ésimo (\texttt{num\_points}, en el espacio marginal) y devuelve la media de $\psi(num\_points)$. Si el parámetro \texttt{alpha $>$ 0} se suma a $\psi(N)$ el valor de \texttt{lnc\_correction} (estudiado más adelante). La función devuelve:\[
\frac{- \langle \psi(n_x) \rangle - \langle \psi(n_y) \rangle + \psi(k) + \psi(N)}{\log (base)}.
\]
Para que la fórmula coincidiera de forma exacta con la de \ref{eq:est2} faltaría un $-\frac{1}{k}$.
\item \texttt{cmi}: calcula la información mutua de \texttt{x} e \texttt{y} condicionado a \texttt{z}, hace uso de la anterior función, que en este caso devolvería \[
  \frac{- \langle\psi(n_{xz})\rangle - \langle\psi(n_{yz})\rangle + \langle\psi(n_z)\rangle + \psi(k)}{\log(base)}.
\]
\item \texttt{kldiv}: devuelve la distancia de Kullback Leibler o entropía relativa entre \texttt{x} y \texttt{xp}. Comienza comprobando que las distribuciones tienen la misma dimensión y que el $k$ escogido no supera al número de muestras. La implementación es similar a las ya vistas. En este caso se devuelve\[
\frac{\log len(xp) - \log(len(x) -1) + d \cdot (\mathbb{E}( nnp) - \mathbb{E} (nn))}{\log(base)},
\]

donde \texttt{nn} y \texttt{nnp} son las distancias a los vecinos más cercanos de los puntos de \texttt{x} y \texttt{xp} respectivamente.
%En la fórmula anterior aparece $\log base$ en el denominador, además no me termica de cuadrar qué es exactamente \texttt{nnp}, entiendo que pretende ser la distancia del k-ésimo vecino de cada elemento de xp, pero utiliza el árbol de xp y le pasa el vector x, no entiendo por qué. Tampoco sé por qué en ese caso utiliza k-1.

\item \texttt{lnc\_correction}: ---esta función no la entiendo bien---.

\item \texttt{entropyd}: con esta comienza una serie de funciones para los estimadores discretos. Se estima la probabilidad contando el número de apariciones de un elemento y dividiendo entre el total de elementos. Se devuelve la suma de las probabilidades multiplicadas por el logaritmo de 1 partido por la probabilidad, todo ello dividido entre $\log base$:\[
\frac{\sum p \log \frac{1}{p}}{log(base)}.
\]

Si las probabilidades son correctas, esta fórmula coincide con la entropía teórica.

\item \texttt{midd}: calcula la información mutua en el caso discreto restando a la entropía de \texttt{x}, la entropía de $x|y$.

\item \texttt{cmidd}: calcula la entropía de \texttt{x,y} condicionada a \texttt{z}. La implementación es similar a la del caso continuo pero llamando a las funciones correspondientes para variables discretas.

\item \texttt{centropyd}: entropía de $x|y$ para variables discretas. Se calcula como entropía conjunta menos entropía de y.

\item \texttt{tcd}: a la entropía por características le resta la entropía total.

\item \texttt{ctcd}:  a la entropía condicionada por características le resta la entropía condicionada.

\item \texttt{corexd}: la suma de la información mutua por columnas menos la información mutua.

\item \texttt{micd}: con este empiezan los estimadores mixtos, calcula la información mutua cuando \texttt{x} sea continua e \texttt{y} discreta. Primero se asegura de que tengamos el mismo número de elementos de ambas variables. Calcula la entropía de \texttt{x} y la probabilidad de cada valor de \texttt{y} (como en el caso discreto, contando el número de veces que aparece cada elemento y dividiendo entre el total). A continuación, para cada valor que toma \texttt{y} se calcula la entropía de \texttt{x} dado \texttt{y} (este valor comienza siendo 0 y se le va sumando la entropía calculada). Para hacer este cálculo, en primer lugar, se toma el vector de elementos de \texttt{x} dado un \texttt{y}, si tiene menos elementos que el $k$ considerado se suma la probabilidad de \texttt{y} por la entropía de \texttt{x}, añadiendo un \textit{warning} de que en este caso se está asumiendo la entropía máxima. Si el número de elementos es suficiente, se calcula la entropía de este vector y se multiplica por la probabilidad de \texttt{y}. La función devolverá el valor absoluto de la diferencia entre la entropía de \texttt{x} y la entropía de \texttt{x} dado \texttt{y}.

\item \texttt{midc}: este estimador se utilizará cuando \texttt{x} sea discreta e \texttt{y} continua, consiste en llamar al anterior con los argumentos intercambiados.

\item \texttt{centropycd}: calculamos la entropía de \texttt{x} (continua) condicionada a \texttt{y} (discreta). Devuelve la entropía de \texttt{x} menos la información mutua (utilizando el estimador mixto correspondiente) de \texttt{x, y}.

\item \texttt{centropydc}: utiliza la función anterior con los parámetros intercambiados.

\item \texttt{ctcdc}: devuelve la suma de la entropía condicionada por columnas menos la entropía condicionada cuando \texttt{x} es discreta e \texttt{y} continua.

\item \texttt{ctccd}: llama a la función anterior para \texttt{x} continua e \texttt{y} discreta.

\item \texttt{corexcd}: devuelve el valor de \texttt{corexdc} intercambiando los parámetros.

\item \texttt{corexdc}: devuelve la diferencia entre \texttt{tcd(xs)} y \texttt{ctcdc(xs, ys, k)}.

\item Las funciones auxiliares \texttt{add\_noise}, \texttt{query\_neighbors}, \texttt{count\_neighbors}, \texttt{avgdigamma} y \texttt{build\_tree} se han ido comentando a medida que aparecían.

\item \texttt{shuffle\_test}: es una función a la que pasamos la medida a utilizar (entropía, información mutua, condicionadas, \dots) y los vectores \texttt{x} e \texttt{y}. Repite \texttt{ns} veces la medida barajando el vector \texttt{x}, devuelve la media y el intervalo de confianza de los resultados. Dice en un comentario que la información mutua (y la información mutua condicionada) deberían tener una media cercana a cero, ---¿esto por qué?---
\end{itemize}

Además del archivo con los estimadores, en esta implementación se incluye un segundo archivo \texttt{test.py} en el que se realizan varias pruebas sobre los mismos. Si lo ejecutamos devuelve:

\begin{lstlisting}
For a uniform distribution with width alpha, the differential entropy is log_2 alpha, setting alpha = 2
and using k=1, 2, 3, 4, 5
result: [0.9675938258710816, 1.0283110450873976, 0.9611523946690828, 0.9875186838882535, 1.0917394218821124]

Gaussian random variables

Conditional Mutual Information
covariance matrix
[[4 3 1]
 [3 4 1]
 [1 1 2]]
true CMI(x:y|x) 0.5148736716970265
samples used [10, 25, 50, 100, 200]
estimated CMI [0.2253106080187056, 0.3892879127835147, 0.4398035226533797, 0.49760670969901555, 0.512074966143492]
95% conf int. (a, b) means (mean - a, mean + b)is interval
 [(0.297216360850314, 0.4346078735815755), (0.3998450496742388, 0.41093314959735017), (0.32034107567533604, 0.34254631457985824), (0.23963575516458396, 0.2875658611969877), (0.17492887513865663, 0.18358453574822897)]
Mutual Information
true MI(x:y) 0.5963225389711981
samples used [10, 25, 50, 100, 200]
estimated MI [0.36751110421959715, 0.508806849561461, 0.5987137761055255, 0.6182090681075484, 0.6174309289477204]
95% conf int.
 [(0.39751000110157275, 0.5830073784740326), (0.46139075928905027, 0.43094430206983014), (0.31222200485617396, 0.3922391298958362), (0.250926144320218, 0.35084718632752154), (0.18857541697637004, 0.19108560308455347)]

IF you permute the indices of x, e.g., MI(X:Y) = 0
samples used [10, 25, 50, 100, 200]
estimated MI [-0.005738147775725934, -0.02720997970818474, 0.0010112811193075707, -0.008517516095735011, 0.0015400222685101536]
95% conf int.
 [(0.24707698319910223, 0.43997790512266205), (0.26641749338269727, 0.2214406621687565), (0.1889408437170837, 0.21236801559420707), (0.18470276983064027, 0.19294766099193206), (0.14993655857703755, 0.16121527948941508)]


Test of the discrete entropy estimators

For z = y xor x, w/x, y uniform random binary, we should get H(x)=H(y)=H(z) = 1, H(x:y) etc = 0, H(x:y|z) = 1
H(x), H(y), H(z) 1.0 1.0 1.0
H(x:y), etc 0.0 0.0 0.0
H(x:y|z), etc 1.0 1.0 1.0


Kl divergence estimator (not symmetric, not required to have same num samples in each sample set
should be 0 for same distribution
result: -0.03587664487774093
should be infinite for totally disjoint distributions (but this estimator has an upper bound like log(dist) between disjoint prob. masses)
result: 7.818662829432822

Test discrete.
random: I(X; Y) = 0.0279 ± 0.0048 (maximum possible 2.3194)
deterministic: I(X; Y) = 3.3168 ± 0.0019 (maximum possible 3.3168)
noisy: I(X; Y) = 2.7575 ± 0.0264 (maximum possible 3.3168)
\end{lstlisting}

En primer lugar, se calcula la entropía de variables con una distribución uniforme en un intervalo $[0,\alpha]$. En el ejemplo \ref{ej:uni} calculamos la entropía teórica, $h(x) = \log \alpha$. En este caso se tomó $\alpha = 2$, luego la entropía teórica vale 1. Observamos que para los valores $k$ siempre hay una pequeña diferencia entre el valor obtenido y la entropía teórica. A continuación, calcula la información mutua (condicionada y sin condicionar) de una variable aleatoria con distribución normal, de forma teórica y usando el estimador. Por último, prueba los estimadores para el caso discreto. 


%-------------------------COMPARACIÓN-----------------------------------
\section{Metodología para la comparación de estimadores}
\subsection{Tecnología utilizada}
---python, bibliotecas usadas, git, planificación (también se podría poner en objetivos) -> hacer un diagrama de Gantt (revisar bibliografía, adquirir conocimmiento, escribir memoria, búsqueda de implementaciones, escribir código)---


\section{Resultados}

\subsection{Análisis comparativo}

Pasamos a implementar el programa \texttt{prueba.py} para poder comparar ambas implementaciones y realizar las modificaciones que consideremos.

Comenzaremos comparando las estimaciones de la entropía. Para ello, creamos la función \texttt{rd\_ent} en la que obtenemos dos variables aleatorias y calculamos su entropía a partir de los estimadores dados por las dos implementaciones.

\begin{lstlisting}
Entropía de X:
Estimador 1:  29.423303907819346
Estimador 2:  43.499531309829294
Entropía de Y:
Estimador 1:  4.232705400287453
Estimador 2:  6.084047562318489
\end{lstlisting}

Los estimadores obtienen resultados notablemente diferentes. Esto se debe a que uno de ellos nos está dando la entropía en bits (al pasarlo a base 2) y el otro en nats. Modificaremos el primer estimador, dividiendo su resultado entre $\log(2)$, para que los dos estén en las mismas unidades.

Pasamos a comparar estas entropías con una que conozcamos teóricamente, la distribución uniforme, vista en el Ejemplo \ref{ej:uni}, cuya entropía teórica es $\log(a)$. El código utilizado se encuentra en la función \texttt{example\_entu}, los resultados obtenidos son los siguientes:

\begin{lstlisting}
Entropía x: (a = 2)
Teórica:  1.0
Estimador 1:  1.0261633399126422
Estimador 2:  1.0261633406219435

Entropía x: (a = 100)
Teórica:   6.643856189774725
Estimador 1:   6.683574884777654
Estimador 2:   6.68357488477827
\end{lstlisting}

Las estimaciones son parecidas, coincidiendo con la entropía teórica en un decimal y entre ellas unos 11. Notamos que en este caso la variable aleatoria $X$ es unidimensional.

Para comparar las estimaciones de la entropía en una variable multidimensional, aprovechamos la función \texttt{test\_entropy} de la primera implementación y la adaptamos para calcular también la entropía con la segunda implementación. Además, añadimos el parámetro \texttt{d}, con el que podremos variar la dimensión de la variable considerada, y el parámetro \texttt{k} que marcará el número de vecinos a utilizar. La matriz \texttt{P}, utilizada para generar la matriz de covarianzas, ya no está definida de forma fija, sino que se define de forma aleatoria según la dimensión a utilizar. Comenzamos a estudiar el caso d = 2, las estimaciones son muy parecidas entre sí (suelen coincidir en varios decimales), si ejecutamos la función en bucle, en general se obtiene una pequeña diferencia con el estimador real, pero no llega a saltar el \textit{assert}. Este salta algunas veces, cuando la entropía es negativa, si los estimadores dan un resultado menor que la entropía real.

Queremos ser realmente conscientes de la magnitud del error que se está cometiendo, para ello, implementamos la función \texttt{err\_entropy}. Esta función no es más que una modificación de \texttt{test\_entropy} en la que se eliminan los \textit{assert} y se calcula la diferencia entre la entropía gaussiana teórica y las estimadas. Notamos que si aumentamos el valor de $d$, ambas estimaciones obtienen valores parecidos, difiriendo de la entropía teórica.

Creamos la función \texttt{exp\_err\_ent}. En ella, se pretende obtener un idea más precisa del error obtenido para cada dimensión con ambos estimadores. Para ello, se mide el error un número (\texttt{reps}) determinado de veces y se calcula la media del error obtenido para cada dimensión. Finalmente, se dibuja una gráfica del error obtenido. Llamando a esta función con \texttt{reps = 20}, \texttt{k = 3}, \texttt{d = 2, ..., 10}, se obtienen los errores que podemos observar en la Tabla \ref{tab:err_ent}, vemos cómo a medida que aumenta la dimensión, aumenta el error de forma similar para ambos estimadores.

\begin{table}[H]
%\large
\centering
\caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica.}
\label{tab:err_ent}
\begin{tabular}{lll}
\toprule
$d$ & Error estimador 1 & Error estimador 2\\ \midrule
2 & 0.014212339128172302 & 0.014680402744924015\\
3 & 0.11105323234117655 & 0.1228230352418274\\
4 & 0.16456589959078247 & 0.1634349916264736\\
5 & 0.699756197647123 & 0.7335137192470153\\
6 & 1.11385415187634 & 1.1622332193590927\\
7 & 1.2283705866481953 & 1.2922942070634487\\
8 & 1.5855188720644953 & 1.6860491894433907\\
9 & 2.262297509187 & 2.436619705610153\\
10 & 2.433512217901498 & 2.660198051294997\\
\bottomrule
\end{tabular}
\end{table}

Dibujando una gráfica de los valores anteriores compararemos más fácilmente el error obtenido con ambos estimadores. En la Figura \ref{fig:err_ent} encontramos esta representación. Vemos que los errores obtenidos con los dos estimadores eran prácticamente 0 para $d=2$ y que estos van aumentando con la dimensión, siendo ligeramente superior el error obtenido con el segundo estimador. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{err_ent}
    \caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica.}
    \label{fig:err_ent}
\end{figure}

El tiempo de cómputo a medida que aumentamos la dimensión se incrementa considerablemente por lo que para hacernos una idea de qué ocurre si la aumentamos un poco más, repetimos el experimento con menos repeticiones por dimensión y llegamos hasta $d = 15$. Se obtienen los errores mostrados en la Figura \ref{fig:err_ent15}. Notamos que los errores no superan al anterior en todos los casos, se puede deber a que en las 10 repeticiones se obtengan estimaciones de la entropía que fueran mejores y ayuden a disminuir el error. Además es destacable cómo, a medida que aumenta la dimensión, crece la diferencia entre las dos estimaciones.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{err_ent_2_15_10reps}
    \caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica, 10 repeticiones por iteración.}
    \label{fig:err_ent15}
\end{figure}


Tratamos de mejorar la primera implementación añadiendo los detalles que la diferenciaban de la estimación estudiada teóricamente. En la función \texttt{entropy\_mod} se añade el $\frac{1}{2^d}$ que faltaba en el cálculo del volumen de la bola unidad y la distancia de cada punto a su vecino más cercano se multiplica por 2. Los resultados para $d = 2$ no son muy prometedores, por lo que abandonamos esta vía.

\begin{lstlisting}
k = 3, d = 2
Error estimador 1:  0.013887003356169863
Error estimador 1_mod:  1.9861129966437974
\end{lstlisting}


Pasamos entonces a comparar los valores devueltos para la información mutua. Las implementaciones realizan la estimación de la información mutua de forma diferente. La implementación 1 utiliza la fórmula que calcula la información mutua como suma de las entropías marginales de las variables y resta de la entropía conjunta. Los errores encontrados en los cálculos individuales podrían afectar al cálculo total. La implementación 2 utiliza el estimador \ref{eq:est_ent} para realizar su estimación de la entropía, es posible que los errores cometidos al calcular la entropía no se vean arrastrados.

Realizamos pruebas comparando los resultados de los estimadores con los de una gaussiana (en la función \texttt{test\_mutual\_information\_mod}) y calculamos el error como la diferencia entre información mutua teórica y estimada, calculamos la media tras repetir el experimento 100 veces, con $d=2$ y $k=3$. Se obtienen los siguientes resultados:

\begin{lstlisting}
Error estimador 1:  0.010671581846025138
Error estimador 2:  0.007051668015342052
\end{lstlisting}

El error en el primer estimador es ligeramente superior al error en el segundo.

Al pasar a dimensión 3, los estimadores comienzan a diferir notablemente de la entropía teórica. Revisando el código notamos que no se adapta la al cambio de dimensión directamente, el cálculo de la entropía teórica de la gaussiana se hace sumando las entropías marginales y restándosela a la conjunta (en este caso como la entropía se calcula correctamente no se arrastran errores), pero la función estaba hecha para el caso $d = 2$. Se modifica utilizando un bucle sobre la dimensión para que, efectivamente, sume todas las entropías marginales. Una vez hecha la modificación correspondiente, realizamos la prueba para $k = 3,\ d = 3$ y 100 repeticiones, con lo que obtenemos:

\begin{lstlisting}
Error estimador 1:  1.6417490587527461
Error estimador 2:  1.6426795618610046
\end{lstlisting}

Al igual que en el caso anterior, crearemos una función, \texttt{exp\_err\_im}, en la que repetiremos la prueba para las dimensiones dadas. Repitiendo el experimento para $d=2,...,15$; 25 veces por dimensión (calculando la media del error), con $k=3$ se obtienen los errores mostrados en la Tabla \ref{tab:err_im}. Notamos que los errores van aumentando al ir añadiendo más dimensiones. Visualizamos los errores obtenidos en la Figura \ref{fig:err_im}. Destaca que los errores obtenidos con ambas estimaciones son tan similares que se superponen en la gráfica. El error en la estimación es creciente en la dimensión.

\begin{table}[H]
\centering
\caption{Errores obtenidos con ambas estimaciones al calcular la información mutua de variables aleatorias con distribución normal y compararla con la información mutua teórica.}
\label{tab:err_im}
\begin{tabular}{lll}
\toprule
$d$ & Error estimador 1 & Error estimador 2\\ \midrule
2 & 0.0077895466646103586 & 0.005879135481256652\\
3 & 1.852406411791869 & 1.8495568835932743\\
4 & 2.328067362081687 & 2.330375193324987\\
5 & 4.022186430366867 & 4.022166466891652\\
6 & 4.552915119298815 & 4.555859073764772\\
7 & 5.325584640708861 & 5.327399887079806\\
8 & 5.696270621561146 & 5.696928152061884\\
9 & 6.86921976358911 & 6.871685971363305\\
10 & 7.492106258014951 & 7.4935380268611915\\
11 & 8.401680241357042 & 8.400306763900506\\
12 & 9.010038140658828 & 9.011324360858382\\
13 & 9.983816029579954 & 9.986518788178701\\
14 & 10.470383330007241 & 10.468634751932003\\
15 & 11.476246581716687 & 11.47645425484027\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{err_im_25reps}
    \caption{Errores obtenidos con ambas estimaciones al calcular la información mutua de variables aleatorias con distribución normal y compararla con la información mutua teórica.}
    \label{fig:err_im}
\end{figure}


\subsection{Experimentos}
\subsection{Discusión}

\section{Conclusiones}

\section{Estimación del coste del TFG}
---Ordenador, horas a precio de junior---

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%       REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography

\end{document}
