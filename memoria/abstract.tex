\chapter*{Abstract \markboth{Abstract}{}}

This project is developed in the context of concepts related to information theory, such as entropy and mutual information. Properties and proposals about how to estimate such concepts are studied. In addition, an analysis from a practical perspective for some implementations of these estimators is presented.\\

Many concepts associated with the information provided by random
variables were born in the scope of information theory. Information theory searches for theoretical limits to the message communication capacity, moreover, it develops schemes that allow to achive an optimal capacity for message exchanging. Some authors consider information theory as a branch of probability theory, although it can also be viewed as a branch of theory of dynamical systems.\\

Concepts related to information have been around for a long time, but it is with Shannon's work, in the 40s decade, when they have been formalized and given a meaning. Shannon defined two measures associated with information, that will be the basic definitions in this project. On the one hand, Shannon defined \textit{entropy}, starting from the definition that Hartley had previously given in the thermodynamics field, as a measure of the ``quantity of information'' that a random variable provides about itself. Formally, for a discrete random variable $X$ with probability mass function $p(x)$, entropy, $H(X)$, is defined as\[
H(X) = - \sum_{x\in\mathcal{X}}p(x)\log p(x).
\]
On the other hand, he defined the \textit{mutual information}, a measure of the quantity of information which a random variable provides about another random variable. Analytically, for two discrete random variables $X$ and $Y$ with joint probability mass function $p(x,y)$ and marginals probability mass functions $p(x), p(y)$ respectively, mutual information, $I(X;Y)$, is defined as\[
I(X;Y) = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log \frac{p(x,y)}{p(x)p(y)}.
\]
These measures can be defined also in the continuous case and, moreover, there are more information measures, for example, relative entropy, proposed by Kullback.\\

From a statistical point of view, we can see these measures related to information as dependency measures between two random variables. In fact, if we compare mutual information with Pearson's correlation coefficient, for example, we will see that the first one gives us more information than the second one, because it does not measure a specific type of dependency and it characterizes independence.\\

In practice, it is not always possible to calculate entropy or mutual information from a random variable, because it is necessary to know its probability mass function or probability density function. In this context, it appears the problem about how to estimate entropy or mutual information from observations of that random variable. The problem of obtaining precise estimations of entropy and mutual information is important because of its several applications. This problem constitutes the core objective of this project.\\

Obtaining entropy or mutual information is useful not just in the information theory field or in thermodynamics, but also in many other fields such as economy, where it can be used to study economic growth, or biology, where it can be used to predict the secondary structure of RNA. In the machine learning field, estimates of entropy are used to measure how good a model is or to estimate its hiperparameters. Mutual information can be used as a rule to feature selection, clustering analysis or causality detection.\\

In this project, we consider the objective of analysing the notions of entropy and mutual information, some related concepts, such as conditional entropy, and the properties of these notions, in addition to the relations between the different concepts. Since entropy and mutual information depend on the probability mass function or probability density function of the random variables, which are not always known, we will focus on analyzing how to estimate them. This analysis will be developed in two levels: the theoretical level, in which we will compare different proposals of estimators, and the practical level, in which we will compare some implementations of the estimations studied in the theory. In the theoretical level, we will briefly introduce binned estimators and we will describe estimators of type $k$ nearest neighbours and also estimators of type Kozachenko - Leonenko. These estimators will allow us, for observations of a random variable that we do not know its theoretical distribution, approximate its entropy and mutual information. In the practical level, we will analyse experimentally two implementations of some of the estimators studied theoretically and we will make a comparative analysis between them. This analysis will consider the quality of the solution through the observation of the error and by the difference between mean performance provided by the \textit{T-Test}. The efficiency in terms of the execution time through profiling by developing a performance analysis is also provided.\\

After the theoretical study and the practical experimentation we have deduced some conclusions. The analysis of the performance of the estimators when data came from a normal distribution allow us to affirm that the errors in the estimators grow with the dimension of the data. When we estimate entropy, the error obtained by the implementation \cite{estimating} is slightly smaller than the error obtained by the implementation \cite{npeet}. However, when we estimate mutual information, the errors of both estimations are similar.\\

The realization of the \textit{T-Test} on the entropy implementations allow us to conclude that the performance of the estimators is similar for small dimensions, but as dimension grows, the differences between the estimators become significant. In the case of entropy, the tests made comparing the estimators with the theoretical estimator for data following a normal distribution allow us to conclude that the estimator \cite{estimating} presents smaller errors and, therefore, would be preferable. In the case of mutual information, however, we can not conclude anything in this sense, because the errors obtained by both estimators were similar.\\

Paying attention to the efficiency, the quickest implementation for calculating entropy is \cite{npeet}, except when the number of observations and the dimension are large, in which case it is faster to use the implementation \cite{estimating}. In the study of mutual information, if we have to chose the quickest version it will depend on the size of the sample and its dimension. For small samples, the implementation \cite{npeet} is faster, just like for medium-sized samples and large dimension. In the rest of situations, large samples and medium-sized samples with small and medium-sized dimensions are the fastest in the implementation \cite{estimating}. We notice that for large dimensions and large sample sizes the execution times increase considerably. Times near to some seconds of execution when the dimension is small, became times between 45 minutes and one hour for large dimensions and large sample sizes. It is necessary, therefore, to use parallelism on execution level if we would like to obtain results in acceptable running times.\\

\paragraph{Key words:} entropy, mutual information, estimators based on the $k$ nearest neighbours, estimators of type Kozachenko - Leonenko, Python.
