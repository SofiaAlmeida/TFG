% memoria.tex
    %% Copyright (C) 2020  Sofía Almeida

    %% This program is free software: you can redistribute it and/or modify
    %% it under the terms of the GNU Affero General Public License as published
    %% by the Free Software Foundation, either version 3 of the License, or
    %% (at your option) any later version.

    %% This program is distributed in the hope that it will be useful,
    %% but WITHOUT ANY WARRANTY; without even the implied warranty of
    %% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    %% GNU Affero General Public License for more details.

    %% You should have received a copy of the GNU Affero General Public License
    %% along with this program.  If not, see <https://www.gnu.org/licenses/>.
% article example for classicthesis.sty
\documentclass[12pt,a4paper]{report} % KOMA-Script article scrartcl
\usepackage[british, spanish]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{lipsum}
\usepackage{url}

% Cargamos amsmath antes que classicthesis que si no tenemos fallos de compilación
\usepackage{amsmath}
\usepackage{./classicthesis}

\usepackage[left=1.5in,right=1.5in,top=1in,bottom=1in]{geometry}

% fuente del mathbb
\AtBeginDocument{%
  \let\mathbb\relax
  \DeclareMathAlphabet{\mathbb}{U}{msb}{m}{n}%
}

%%% Cabeceras
\cleardoublepage
\pagestyle{scrheadings}
\automark{chapter}


%%% Matemáticas
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[Symbol]{upgreek} %pi
\usepackage{mathtools}
%\allowdisplaybreaks
\AtBeginDocument{\renewcommand{\epsilon}{\varepsilon}}
\AtBeginDocument{\newcommand{\R}{\mathbb{R}}}
\AtBeginDocument{\newcommand{\E}{\mathbb{E}}}

\newcommand{\bias}{\text{bias}}
\newcommand{\Var}{\text{Var}}
\newcommand{\olsi}[1]{\,\overline{\!{#1}}} % overline short italic

%%% Tablas
\usepackage{tabularx}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{diagbox}
\renewcommand\spanishtablename{Tabla}
\usepackage{subcaption}
\usepackage{chngpage} % allows for temporary adjustment of side margins

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

%%% Código
%\usepackage{listingsutf8}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  %frame=single,
  breaklines=true
}
\usepackage[htt]{hyphenat} % Permite salto de líneas en texttt

%%% Gráficas
\usepackage{graphicx} % Required for including pictures
\graphicspath{{../fig/}}
\usepackage[section]{placeins} % Floats no pasen de sección

%%% Bibliografía
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{spanish}{
  urlseen = {Último acceso}
}
\addbibresource{citations.bib}
\appto{\bibsetup}{\sloppy}


\theoremstyle{definition}
\newtheorem{definition}{Definición}[section]

\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{corollary}{Corolario}[theorem]

% Título
\usepackage{scrextend}
\newcommand{\myTitle}{Estimadores de la entropía e información mutua}
\newcommand{\myName}{Sofía Almeida Bruno}
\newcommand{\myDegree}{Doble Grado en Ingeniería Informática y Matemáticas}
\newcommand{\myFaculty}{ETSIIT -- Facultad de Ciencias}
\newcommand{\myUni}{Universidad de Granada}
\newcommand{\myYear}{Curso 2019-2020}
\newcommand{\myProf}{Alberto Guillén Perales\\\textit{Departamento de Arquitectura y Tecnología de Computadores}\\ETSIIT}
\newcommand{\myOtherProf}{Ana I. Garralda Guillem\\\textit{Departamento de Matemática Aplicada}\\Facultad de Ciencias}

\begin{document}
%-------------------------- ÍNDICE--------------------------------------
\include{Titlepage}
\include{Titleback}
\include{agradecimientos}

\input{resumen.tex}

\begin{otherlanguage}{british}
  \input{abstract.tex}
\end{otherlanguage}

\tableofcontents
\newpage


%-------------------------INTRODUCCIÓN----------------------------------
\chapter{Introducción}

Este trabajo se desarrolla en el contexto de conceptos asociados a la teoría de la información, como son la entropía y la información mutua. Se estudiarán sus propiedades y algunas propuestas sobre cómo estimarlos. Además, se analizarán desde un punto de vista práctico algunas implementaciones de estos estimadores, profundizando en la calidad de las estimaciones así como en el rendimiento computacional de las mismas.\\

\section{Motivación y antecedentes}

Es en el ámbito de la teoría de la información donde surgen conceptos relativos a la información que nos proporcionan las variables aleatorias. La teoría de la información busca límites teóricos a la capacidad de intercambiar mensajes, así como desarrollar esquemas que alcancen una capacidad óptima para el intercambio de mensajes. Algunos autores consideran la teoría de la información como una rama de la teoría de la probabilidad, aunque también se puede ver como una rama de la teoría de los sistemas dinámicos \cite{gray}.\\

Aunque ya existían conceptos relacionados con la información, es con los trabajos de Shannon en la década de los 40 cuando realmente se formalizan y se les aporta significado \cite{shannon}. Shannon define dos medidas asociadas a la información, que serán las definiciones centrales de este trabajo. Por un lado, define la \textit{entropía}, a partir de la definición que Hartley había dado en el campo de la termodinámica \cite{hartley}, como una medida de la ``cantidad de información'' que una variable aleatoria aporta sobre sí misma. Formalmente, para una variable aleatoria discreta $X$ con función masa de probabilidad $p(x)$ se define la entropía, $H(X)$, como\[
H(X) = -\sum_{x\in \mathcal{X}}p(x)\log p(x).
\] Por otro lado, define la \textit{información mutua}, una medida de la cantidad de información que una variable aleatoria nos proporciona sobre otra. Analíticamente, para dos variables aleatorias $X$ e $Y$ discretas con función masa de probabilidad conjunta $p(x,y)$ y funciones masa de probabilidad marginales $p(x), p(y)$ respectivamente, se define la información mutua $I(X;Y)$, como\[
I(X;Y) = \sum_{x\in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}.
\]

Estas medidas se pueden definir también en el caso continuo y, además, no son las únicas, podemos encontrar también la entropía relativa propuesta por Kullback, por ejemplo \cite{thomas}.\\

Desde un punto de vista estadístico, podemos ver estas medidas asociadas a la información como medidas de dependencia entre dos variables. De hecho, si comparamos la información mutua con el coeficiente de correlación de Pearson, por ejemplo, veremos que la primera nos proporciona más información que la segunda, ya que no mide un tipo concreto de dependencia y caracteriza la independencia.\\

En la práctica, no siempre es posible calcular la entropía o información mutua de una variable aleatoria, puesto que es necesario conocer su función masa de probabilidad o función de densidad. Por ello, surge el problema de cómo estimarla a partir de una realización muestral de dicha variable. El problema de obtener estimaciones precisas de la entropía y la información mutua es importante debido a las múltiples aplicaciones que hacen uso de ellas. Este problema constituye uno de los objetivos centrales de este proyecto.\\

Calcular la entropía o la información mutua resulta de interés no solo en el ámbito de la teoría de la información o en la termodinámica, ya que se les ha encontrado utilidad en numerosos campos como la economía, donde se puede utilizar para estudiar el crecimiento económico \cite{economy}, o la biología, para predecir la estructura secundaria del ARN \cite{freyhult}. En el campo del aprendizaje automático se utilizan estimaciones de la entropía para medir la bondad de un modelo \cite{anewclass} o para estimar sus hiperparámetros. La información mutua se puede utilizar como criterio para seleccionar variables \cite{feature}, hacer un análisis clúster \cite{cluster} o detectar causalidades \cite{causality}.\\


\section{Objetivos}
\begin{enumerate}
\item \textit{Revisión teórica sobre entropía, información mutua y estimadores de las mismas}.

  En este trabajo nos planteamos el objetivo de analizar las nociones de entropía e información mutua, algunos conceptos relacionados, como la entropía condicionada, y sus propiedades más importantes, además de las relaciones entre los diferentes conceptos. Puesto que estos conceptos dependen de la función masa de probabilidad o función de densidad de las variables aleatorias, que no siempre resulta conocida, se continuará analizando cómo estimarlas. Este análisis se desarrollará en dos niveles: a nivel teórico, estudiando diferentes propuestas de estimadores, y a nivel práctico, comparando varias implementaciones de los mismos. En el nivel teórico, se introducirán brevemente los estimadores basados en particiones y se desarrollarán estimadores basados en los $k$ vecinos más cercanos y de tipo de Kozachenko - Leonenko \cite{kraskov}. Estos estimadores nos permitirán, para una observación de una variable de la que no conocemos su distribución teórica, aproximar la entropía y la información mutua.
  
\item \textit{Analizar y comparar implementaciones de estimadores de la entropía y de la información mutua.}
  
 El segundo objetivo a alcanzar se corresponde con el análisis de los estimadores desde un punto de vista práctico. En este nivel, se analizarán experimentalmente implementaciones de algunos de los estimadores estudiados teóricamente y se realizará una análisis comparativo entre ellas. Este análisis contemplará la calidad de la solución a través de la observación del error, diferencia entre comportamientos medios mediante \textit{T-Test} y el rendimiento en términos de tiempo de ejecución mediante \textit{profiling}.\\
\end{enumerate}


Esta memoria se ha estructurado en cuatro capítulos, este primer capítulo introductorio y tres capítulos adicionales.\\

En el Capítulo 2 se estudian los conceptos básicos necesarios para el desarrollo del proyecto. Entre otras, se presenta la definición de estimador y sus propiedades, se describen algunas distribuciones que se usarán a lo largo de este trabajo de fin de grado y se definen dos medidas de la dependencia como motivación para la definición de la información mutua. Asimismo, se presentan los conceptos de teoría de la información a estudiar: entropía e información mutua, así como una serie de propiedades y relaciones entre ellas. Este estudio se realizará tanto para el caso discreto como para el continuo. Se continúa estudiando estimadores de los conceptos de entropía e información mutua. Se estudiará brevemente un estimador basado en particiones y en más profundidad un estimador de la entropía basado en los $k$ vecinos más cercanos. También se estudiarán estimadores del tipo de Kozachenko - Leonenko, tanto para la entropía como para la información mutua. Para cerrar este capítulo se analizarán dos implementaciones en Python de los estimadores estudiados anteriormente: las implementaciones presentadas en \cite{estimating} y \cite{npeet}.\\

En el Capítulo 3 se establece la metodología utilizada para la validación computacional de los conceptos teóricos y el estudio experimental de los estimadores. Se describen brevemente las herramientas a utilizar, el lenguaje de programación y las bibliotecas utilizadas, centrándonos posteriormente en aquellas necesarias para llevar a cabo el análisis de rendimiento. Durante el estudio de las implementaciones adquiere interés conocer cómo funciona el paralelismo en Python y las herramientas que proporciona la biblioteca \texttt{scikit-learn} para calcular los vecinos más cercanos. Además, el cálculo de los vecinos más cercanos resultará fundamental para realizar las estimaciones, por ello, se describen las clases en que la biblioteca \texttt{scikit-learn} tienen implementado este algoritmo. Se discuten los fundamentos teóricos del \textit{T-Test} que será posteriormente incluido como parte de la metodología de análisis y validación computacional. La última etapa del análisis de las implementaciones es el análisis de rendimiento, se incluye en este capítulo una explicación de las bibliotecas utilizadas para la realización del mismo.\\

El Capítulo 4 recoge los resultados derivados del análisis experimental desarrollado. Esta experiencia computacional, en primer lugar, analiza el comportamiento de los estimadores cuando los datos siguen una distribución normal, conocida teóricamente. Continúa realizando un \textit{T-Test} para saber si las diferencias en media de los estimadores son estadísticamente significativas. Asimismo, se desarrolla el análisis de rendimiento de las distintas versiones desarrolladas intentando extraer conclusiones acerca de los requerimientos computacionales de cada método.\\

La memoria finaliza con un apartado de conclusiones, así como un apartado en el que se especifica el presupuesto necesario para su desarrollo.\\

Del análisis experimental se concluye que los estimadores estudiados incrementan sus errores al aumentar la dimensión de los datos de muestra. Además, el aumento de la dimensión, así como el aumento del tamaño de muestra, provoca un incremento notable en los tiempos de ejecución. Tras el \textit{T-Test} concluimos que las implementaciones de la entropía en media resultan similares para dimensiones pequeñas, pero cuando esta aumenta las diferencias entre ellas también lo hacen, preferiremos en este caso la implementación \cite{estimating} de la entropía, pues es la que obtiene menores errores en la comparación con una variable aleatoria con distribución normal. En el caso de la información mutua, las diferencias entre los estimadores resultan significativas en todos los casos, pero las comparaciones con variables aleatorias con distribución normal no nos permiten decantarnos por ninguno. Atendiendo a los tiempos de ejecución, la elección de la implementación dependerá tanto de la dimensión como del tamaño de muestra.\\


%--------------------------- FUNDAMENTOS--------------------------------
\chapter{Fundamentos}

\section{Conceptos previos}

En esta sección se incluyen algunas definiciones y resultados sobre probabilidad y estadística estudiados a lo largo del grado que utilizamos en el cuerpo del trabajo.\\

\begin{definition}[$\sigma$-álgebra]
  Sea $\Omega$ un conjunto no vacío. Una $\sigma$-álgebra $\mathcal{A}$ sobre $\Omega$ es una familia de subconjuntos de $\Omega$ que verifica:
  \begin{itemize}
  \item  el conjunto vacío, $\emptyset$, pertenece a $\mathcal{A}$,
  \item  si $A$ pertenece a $\mathcal{A}$, entonces su complementario $\Omega\ \backslash\ A$ también pertenece a $\mathcal{A}$,
 \item si tomamos una serie de conjuntos $\{A_i\}_{i\in \mathbb{N}}$ de $\mathcal{A}$, entonces su unión numerable $\cup_{i\in \mathbb{N}}A_i$ también pertenece a $\mathcal{A}$.\\
  \end{itemize}
  
\end{definition}

\begin{example}
  La familia de conjuntos Borel $\mathcal{A} = \mathcal{B}(\mathbb{R}^n)$ es una $\sigma$-álgebra sobre $\mathbb{R}^n$.\\

\end{example}

\begin{definition}[Medida de probabilidad]
  Sea $\mathcal{A}$ una $\sigma$-álgebra sobre un conjunto no vacío $\Omega$. Una medida de probabilidad $P$ es una función $P:\mathcal{A} \to [0,1]$ tal que
  \begin{itemize}
  \item $P[\Omega] = 1$,
  \item si $\{A_i\}_{i\in \mathbb{N}}$  son conjuntos de $\mathcal{A}$ disjuntos dos a dos, entonces\[
    P\left[\bigcup_{i\in \mathbb{N}}A_i\right] = \sum_{i\in\mathbb{N}}P[A_i].\]\\[-10pt]
  \end{itemize}
\end{definition}

La tupla $(\Omega, \mathcal{A}, P)$ se llama espacio de probabilidad. Los conjuntos de $\mathcal{A}$ se llaman sucesos.\\

\begin{definition}[Variable aleatoria]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad. Un variable aleatoria $X=(X_1,\dots, X_n)$ es una función medible \[
X:(\Omega, \mathcal{A}) \to (\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n)).
\]
La condición de medibilidad quiere decir que $X^{-1}(B) \in \mathcal{A}\quad \forall B\in \mathcal{B}(\mathbb{R}^n)$.\\
\end{definition}

Cuando $n=1$ diremos que la variable aleatoria es unidimensional, mientras que cuando $n >1$ diremos que la variable aleatoria es multidimensional, en ocasiones también denominada, vector aleatorio. En el caso de la variable aleatoria multidimensional podremos querer información sobre ella misma, $X=(X_1,\dots, X_n)$, lo que llamaremos variable aleatoria conjunta, o sobre alguna de las variables aleatoria $X_i$, que denominaremos variable aleatoria marginal.\\

Cada variable aleatoria nos da una medida de probabilidad, su probabilidad inducida.\\

\begin{definition}[Probabilidad inducida]
  La probabilidad inducida por un variable aleatoria $X=(X_1,\dots, X_n)$, o distribución de $X$, se define como la función \[
P_X[B]:= P[X^{-1}(B)],
  \]
para todo $B \in \mathcal{B}(\mathbb{R}^n)$.\\
\end{definition}

\textbf{Nota:} en adelante por comodidad, omitiremos el subíndice $X$ cuando no haya lugar a confusión.\\

\begin{definition}[Función de distribución]
  La función de distribución de una variable aleatoria unidimensional $X$ es la aplicación $F_X:\mathbb{R}\to[0,1]$ definida como:\[
F_X(x) = P[X\leq x].
  \]\\[-5pt]
Para una variable aleatoria multidimensional,  $X=(X_1,\dots,X_n)$, la función de distribución es $F_X:\mathbb{R}^n\to[0,1]$ y está dada por\[
F_X(x_1,\dots,x_n) = P[X_1\leq x_1,\dots, X_n\leq x_n].
  \]\\[-10pt]
\end{definition}

\begin{proposition}
  La función de distribución $F_X$ de la variable aleatoria $X$ es no decreciente, continua a la derecha y $\lim_{x\to - \infty}F_X(x) = 0$, $\lim_{x\to +\infty}F_X(x) = 1$.\\
\end{proposition}

Cuando la imagen de la variable aleatoria $X$ sea numerable, diremos que la variable aleatoria es \textit{discreta}, será descrita por la función masa de probabilidad que asigna una probabilidad a cada valor de la imagen de $X$. Por otro lado, cuando la imagen de la variable aleatoria $X$ sea infinita no numerable, diremos que la variable aleatoria es \textit{continua}. Si además puede ser descrita por una función de densidad, será \textit{absolutamente continua}.\\
  
  Si la variable aleatoria $X$ es discreta, la función de distribución tendrá puntos de discontinuidad en los valores de la imagen de $X$. Mientras que si la variable aleatoria es continua, su función de distribución será continua.\\

  \begin{definition}[Función de densidad y función masa de probabilidad]
 $X$ es una variable aleatoria con distribución absolutamente continua si existe una función integrable Lebesgue $f_X:\mathbb{R}^n\to \mathbb{R}$ tal que para cualquier conjunto Borel $B\subset \mathbb{R}$\[
P[X\in B] = \int_B f_X(x) dx,
\]
$f_X$ se llama función de densidad de $X$. Si hay una secuencia (finita o numerable) de vectores $x_1,x_2,\dots$ tal que para cualquier conjunto Borel $B\subset \mathbb{R}^n$\[
P[X\in B] = \sum_{x_i\in B}P[X = x_i],
\]
entonces decimos que $X$ tiene una distribución discreta que toma los valores $x_1,x_2,\dots$ y cuya función masa de probabilidad es $P[X=x_i]=p(x_i)$ en $x_i$.\\
\end{definition}

\begin{definition}[Soporte]
Llamamos soporte de una variable aleatoria continua $X$ al conjunto de puntos donde $f(x)>0$, lo notaremos $sop(X)$.\\
\end{definition}


\begin{definition}[Probabilidad condicionada]
  Para cualesquiera dos sucesos $A,B \in \mathcal{A}$ tal que $P[B]\neq 0$, la probabilidad condicionada de A por B se define como\[
P[A|B] = \frac{P[A\cap B]}{P[B]}.
  \]\\[-10pt]
\end{definition}

\begin{definition}[Función de densidad condicionada]
  Para cualesquiera dos variables aleatorias $X$, $Y$ con funciones de densidad $f_X$ y $f_Y$ respectivamente y función de densidad conjunta $f$, la densidad de $X$ condicionada a que $Y=y$ viene dada por\[
f_X(x|Y=y) = \frac{f(x,y)}{f_Y(y)}.
  \]\\[-10pt]
\end{definition}

\begin{definition}[Sucesos independientes]
  Los sucesos $A_1,\dots,A_n\in \mathcal{A}$ se dicen independientes si\[
P\left [\bigcap_{j=1}^kA_{i_j} \right ] = \prod_{j=1}^kP\left [A_{i_j} \right],
\]
para cualesquiera índices $1\leq i_1\leq \dots \leq i_k\leq n$.\\
\end{definition}

\begin{definition}[Variables aleatorias independientes]
  Dos variables aleatorias $X$ e $Y$ son independientes si para cualesquiera conjuntos Borel $A, B \in \mathcal{B}(\mathbb{R}^n)$ los sucesos $[X\in A]$ e $[Y\in B]$ son independientes.\\

De forma general, $k$ variables aleatorias $X_1,\dots,X_k$ serán independientes si para cualesquiera conjuntos de Borel $B_1,\dots,B_k \in \mathcal{B}(\mathbb{R}^n)$ los sucesos $[X_1 \in B_1],\dots,[X_k\in B_k]$ son independientes.\\
\end{definition}

\begin{proposition}[Caracterizaciones de independencia]
  Dada una variable aleatoria $X =(X_1,\dots,X_n)$ sus componentes son independientes si, y solo si, se da alguna de las siguientes condiciones:
  \begin{itemize}
  \item La función de distribución verifica $F_X(x) = F_{X_1}(x_1)\cdot \ldots\cdot F_{X_n}(x_n)$ para todo $x \in \mathbb{R}^n$.
  \item $X$ tiene función de densidad y $f_X(x) = f_{X_1}(x_1)\cdot \ldots\cdot f_{X_n}(x_n)$ para todo $x \in \mathbb{R}^n$, salvo, a lo sumo, en un conjunto de medida nula.\\
  \end{itemize}
\end{proposition}

\begin{definition}[Esperanza]
  La esperanza de una variable aleatoria $X$ unidimensional se define como:\[
\mu = \mathbb{E}[X] = \int_{\Omega}X(\omega)dP(\omega) = \int_{\mathbb{R}}xdF(x).
\]

En el caso discreto, si la variable aleatoria $X$ tiene un número finito de posibles salidas $x_1,\dots,x_k$ cada una con probabilidad $p_1,\dots, p_k$ respectivamente, la esperanza es\[
\mathbb{E}[X] = \sum_{i=1}^kx_ip_i.
\]

En el caso de que $X$ sea una variable aleatoria absolutamente continua y admita función de densidad, $f(x)$, la esperanza se calcula\[
\mathbb{E}[X] = \int_{\mathbb{R}}xf(x)dx.
\]
  
  Si  $X=(X_1,\dots,X_n)^T$ es una variable aleatoria multidimensional. Se define la esperanza de $X$ como:\[
\mu_X = \mathbb{E}[X] = \begin{bmatrix}
           \mathbb{E}[X_1] \\
          \vdots \\
          \mathbb{E}[X_n]
         \end{bmatrix}
 = \begin{bmatrix}
           \mu_1 \\
          \vdots \\
          \mu_n
 \end{bmatrix},  \]
 siempre que existan todas las esperanzas unidimensionales.\\
\end{definition}

\begin{proposition}[Propiedades de la esperanza]\label{p:prop_esp}
  Sean $X, X_1,\dots, X_n$ variables aleatorias con funciones de densidad $f_X,f_{X_1},\dots,f_{X_n}$ respectivamente y $g,g_1,\dots,g_n:\R\to \R$ funciones integrables Lebesgue, entonces se cumplen las siguientes propiedades de la esperanza:
  \begin{itemize}
  \item $g(X)$ es también una variable aleatoria y su esperanza será\[
\mathbb{E}[g(X)] = \int g(x)f_X(x)dx.
\]
Esta propiedad se conoce como \textit{ley del estadístico inconsciente}.
\item $\mathbb{E}\left [\sum_{j=1}^kc_jg_j(X) \right] = \sum_{j=1}^k c_j \mathbb{E} \left [ g_j(X) \right ]$.

\item Si $X_1,\dots,X_n$ son independientes, entonces\[
\mathbb{E} \left [ \prod_{i=1}^n X_i \right ] = \prod_{i=1}^k\mathbb{E}[X_i].
  \]\\[-10pt]
  \end{itemize}
  
\end{proposition}

\begin{theorem}[Desigualdad de Jensen]\label{th:jensen}
  Si $g$ es una función integrable Lebesgue, convexa y $X$ es una variable aleatoria, entonces\[
 g\left ( \E[X] \right) \leq \E\left[g(X)\right]
  \]
dándose la igualdad si, y solo si, $g$ es una función afín.\\
\end{theorem}

\begin{definition}[Esperanza condicionada]
  Se define la esperanza condicionada de la variable aleatoria $Y$ dado $X$ como la variable aleatoria $\mathbb{E}[Y|X]$ tal que \[
\mathbb{E} [ Y | X = x ] = \int y f_Y(y| X = x) dy.
  \] \\[-10pt]
  
\end{definition}
\begin{definition}[Varianza y matriz de covarianzas]
  Sea $X$ una variable aleatoria unidimensional de esperanza $\mu_X$, definimos su varianza como\[
\sigma_X^2 = \Var(X) = \mathbb{E}\left [ (X- \mu_X)^2 \right ].
  \]
  Se define la matriz de covarianzas de una variable aleatoria multidimensional $X= (X_1,\dots,X_n)^T$ como\[
\Sigma = Cov(X) = \mathbb{E}\left [(X-\mu_X)(X-\mu_X)^T \right ] =  \begin{bmatrix} \sigma_{11} & \dots & \sigma_{1n} \\ \vdots& \ddots & \vdots \\ \sigma_{n1} &  \dots & \sigma_{nn}\end{bmatrix}\,,
\]
donde $\sigma_{ij} = Cov(X_i, X_j) = \mathbb{E}\left[(X_i-\mu_i)(X_j-\mu_j)\right]=\sigma_{ji}$ es la covarianza de las variables aleatorias $X_i$ y $X_j$. La podremos definir cuando existan todas las covarianzas.\\ 
\end{definition}

\subsection{Estimadores}

En esta sección recordaremos algunos conceptos sobre estimación puntual que utilizaremos más adelante.\\

\begin{definition}[Muestra aleatoria simple]
  Una muestra aleatoria simple de tamaño $n$ de una variable aleatoria $X$ con distribución teórica $F$, son $n$ variables aleatorias $(X_1,\dots,X_n)$, independientes e idénticamente distribuidas, todas con la misma distribución $F$.\\
\end{definition}

En un amplio número de estudios estadísticos la distribución teórica $F$ de la variable aleatoria a estudiar es desconocida. Notamos $\mathcal{F}$ a la familia de distribuciones candidatas a ser realmente la distribución (se obtiene a partir de información previa). En estadística paramétrica se conoce la forma funcional de $F$ pero se desconocen uno o algunos parámetros de la misma. Para tratar de obtener información sobre este parámetro desconocido a partir de una muestra utilizaremos los \textit{estimadores}. En estadística no paramétrica se desconoce la forma funcional de $F$, y se trata de obtener aproximaciones de la distribución o de algunos de sus momentos a partir de una muestra.\\

\begin{definition}[Estadístico]
  Se denomina estadístico a cualquier función medible de una muestra aleatoria simple $T(X_1,\dots,X_n)$, donde $T:\mathbb{R}^n\to \mathbb{R}$ es una determinada función medible.\\
\end{definition}

\begin{example}
  Algunos estadísticos habituales son:

  \begin{itemize}
  \item $T(X_1,\dots,X_n) = \frac{1}{n}\sum_{i=1}^nX_i$, media aritmética o media muestral, denotada por $\olsi{X}$.
  \item $T(X_1,\dots,X_n) = \frac{1}{n}\sum_{i=1}^n\left(X_i-\olsi{X} \right)^2$, varianza muestral, notada $s^2$.
  \item $T(X_1,\dots,X_n) = \min (X_1,\dots,X_n)$, el menor valor muestral.
  \item $T(X_1,\dots,X_n) = \max (X_1,\dots,X_n)$, el mayor valor muestral.\\
  \end{itemize}
\end{example}

\begin{definition}[Estimador]
Un estimador es un estadístico cuyos valores se utilizan para obtener información (estimación puntual) de un parámetro desconocido $\theta$, lo notaremos  $\widehat{\theta}(X_1, \dots, X_n)$.\\
\end{definition}

Como comentamos anteriormente, se usan los valores del estimador para obtener información de algún parámetro de la población. Las siguientes propiedades nos informan de la ``calidad'' de la estimación y permiten comparar diferentes estimadores de un mismo parámetro.\\

\begin{definition}[Estimador insesgado]
  Si $\widehat{\theta}$ es un estimador del parámetro $\theta$, la diferencia \[
\bias\left( \widehat{\theta}\right) = \mathbb{E}\left [\widehat{\theta} \right ] - \theta
\]
se denomina sesgo del estimador $\widehat{\theta}$ como estimador de $\theta$. Cuando el sesgo es nulo para cualquier valor del parámetro, es decir, si\[
\mathbb{E} \left [\widehat{\theta}\right] = \theta\quad \forall\theta \in\Theta,
\]
el estimador $\widehat{\theta}$ se dice insesgado para $\theta$.\\
\end{definition}

\begin{example}\label{ej:medest}
  % Media muestral estimador insesgado
  Sea $X_1,\dots,X_n$ una muestra aleatoria de una variable aleatoria $X$ y $\mu = \E\left[X\right]$. Si el parámetro de interés es la media, $\mu$, podemos utilizar la media muestral, $\olsi{X}$, para estimarlo. Calculamos su sesgo,\[
\bias\left(\olsi{X} \right) = \E\left[ \olsi{X}\right] - \mu = \frac{1}{n} \sum_{i=1}^n\E \left [ X_i\right] - \mu = \mu - \mu = 0,
\]
hemos comprobado que la media muestral es un estimador insesgado para $\mu$.\\
\end{example}

\begin{definition}[Eficiencia]
  Si $\hat{\theta}_1, \hat{\theta}_2$ son dos estimadores del parámetro $\theta$, diremos que $\hat{\theta}_1$ es más eficiente que $\hat{\theta}_2$ si \[
\Var\left(\hat{\theta}_1\right) < \Var\left(\hat{\theta}_2\right).
  \]\\[-10pt]
\end{definition}

El tamaño de la muestra será un factor que influya en la estimación. Por ello, otra propiedad deseable en un estimador es la consistencia. Para definir esta propiedad es necesario conocer la definición de convergencia en probabilidad, una de las diferentes nociones de convergencia de variables aleatorias.\\

\begin{definition}[Convergencia en probabilidad]
  Una sucesión de variables aleatorias $\{X_n\}$ converge en probabilidad a la variable $X$ si para todo $\varepsilon > 0$,\[
\lim_{n\to\infty}P\left[|X_n-X|>\varepsilon \right] = 0. 
  \] Lo notaremos $X_n\xrightarrow[]{P}X$.\\
\end{definition}


\begin{definition}[Estimador consistente]
  Sea $\hat{\theta}_n$ un estimador del parámetro $\theta$ para una muestra de tamaño $n$. Diremos que el estimador $\hat{\theta}_n$ es consistente si, cuando $n\to\infty$, se verifica\[
\hat{\theta}_n\xrightarrow[]{P}\theta.
  \]\\[-10pt]
\end{definition}

\begin{lemma}\label{l:est_cons}
  Sea $\hat{\theta}_n$ un estimador del parámetro $\theta$ para una muestra de tamaño $n$. Si $\bias\left(\hat{\theta}_n\right)\xrightarrow[]{n\to\infty} 0$ y $\Var\left(\hat{\theta}_n\right)\xrightarrow[]{n\to\infty}0$, entonces $\hat{\theta}_n\xrightarrow[]{P}\theta$, esto es, el estimador $\hat{\theta}_n$ es consistente.\\
\end{lemma}

Cuando $\bias\left(\hat{\theta}_n\right)\xrightarrow[]{n\to\infty} 0$ diremos que el estimador $\hat{\theta}_n$ es asintóticamente insesgado.\\

\begin{example}
  % Estimadores consistentes
  Sea $X$ una variable aleatoria con media $\mu$ y varianza $\sigma^2$. Hemos visto que la media muestral es un estimador insesgado de $\mu$. Veamos ahora el comportamiento asintótico de la varianza de la media muestral.\[
\Var(\olsi{X}) = \Var\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}\sum_{i=1}^n\Var(X_i) = \frac{\sigma^2}{n}\xrightarrow[]{n\to\infty}0.\\
\]

En el Ejemplo \ref{ej:medest} habíamos visto que el sesgo de $\olsi{X}$ era nulo para cualquier tamaño de muestra $n$. Concluimos, aplicando el Lema \ref{l:est_cons}, que $\olsi{X}$ es un estimador consistente de la media de la variable $X$.\\
\end{example}

La media muestral no es solo un estimador insesgado y consistente de la media teórica de una variable aleatoria $X$. La ley (débil) de los grandes números prueba que este estimador converge en probabilidad a la media teórica.\\

\begin{theorem}[Ley (débil) de los grandes números]\label{t:lgn}
  Sean $X_1,\dots,X_n$ variables aleatorias independientes e idénticamente distribuidas con media $\mu<\infty$, entonces la media muestral\[
\olsi{X}_n = \frac{1}{n}\sum_{i=1}^nX_i
\]converge en probabilidad a $\mu$, esto es,\[
\olsi{X}_n\xrightarrow[]{P} \mu.\]\\  [-10pt]
\end{theorem}


\subsection{Algunas distribuciones discretas y continuas}

Durante el desarrollo del trabajo se utilizarán algunas distribuciones que se presentan a continuación.\\
%--------------------------------- Uniforme unidimensional--------------
\begin{definition}[Distribución uniforme discreta]
Sea $X$ una variable aleatoria discreta con valores $\{x_1,\dots,x_n\}$. $X$ tiene una distribución uniforme si $P[X = x_i] = \frac{1}{n}$ para $i=1,\dots,n$. Es decir, la función masa de probabilidad $p(x_i) = \frac{1}{n}$ para $i=1,\dots,n$.\\
\end{definition}

\begin{example}
  Si tiramos un dado no cargado hay seis posibles valores: 1, 2, 3, 4, 5 y 6; cada vez que lanzamos el dado hay probabilidad $\frac{1}{6}$ de que salga cualquiera de ellos.\\
\end{example}

\begin{definition}[Distribución uniforme continua]
  Una variable aleatoria $X$ que puede obtener cualquier valor en el intervalo $[a,b]$ con la misma probabilidad tendrá una distribución uniforme. Su función de densidad vendrá dada por \[
f(x) = 
\begin{cases}
  \frac{1}{b-a},\quad \text{si } x\in [a,b]\\
  0,\quad\quad \text{en otro caso}
\end{cases}.\\
  \]\\[-10pt]
\end{definition}
% ----------------------------- FIN uniforme----------------------------

%---------------------------------Distribución normal-------------------
\begin{definition}[Distribución normal unidimensional]
  Una variable aleatoria unidimensional seguirá una distribución normal, o distribución gaussiana, si su función de densidad $f:\R\to\R$ viene dada por \[
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma}\right ) ^2}.
\]\\[-10pt]
\end{definition}

Generalizamos esta definición para el caso multivariante.\\

\begin{definition}[Distribución normal multivariante]
  Una variable aleatoria sigue una distribución normal multivariante si cualquier combinación de sus componentes tiene una distribución normal univariante. Su función de densidad $f:\R^n\to\R$ viene dada por \[
f(x) = \frac{1}{\left (\sqrt{2\pi}\right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},
  \] donde $\mu\in \R^n$ y $\Sigma$ es una matriz $n\times n$ definida positiva.\\
\end{definition}
%--------------------------------Fin - normal---------------------------

%--------------Multinomial - Principio----------------------------------
La siguiente distribución, la distribución multinomial, modela la probabilidad de un número determinado de éxitos para $n$ experimentos independientes, donde hay $k$ posibles éxitos, cada uno de ellos con probabilidad de éxito fija.\\

\begin{definition}[Distribución multinomial]
 Sean $n$ experimentos independientes donde cada uno produce uno de los sucesos $S_1,\dots,S_k$ para $k\geq 2$ (estos sucesos son mutuamente excluyentes) y cada suceso $S_i$ ocurre con probabilidad $p_i$ ($p_1+\dots+p_k = 1$). Definimos las variables aleatorias $X_i,\ i=1,\dots,k$ como el número de experimentos en el que ocurre $S_i$. Entonces $X=(X_1,\dots,X_k)$ tiene una distribución multinomial con parámetros $n$ y $p = (p_1,\dots, p_k)$. Su función masa de probabilidad viene dada por
\begin{align*}
f(x_1,\dots,x_k; n, p_1,\dots,p_k) &= P[X_1 = x_1\text{ y } \dots \text{ y } X_k = x_k]\\[3pt] &=
\begin{cases}
    \frac{n!}{x_1!\dots x_k!} p_1^{x_1} \dots  p_k^{x_k},\quad\text{si } \sum_{i=1}^kx_i = n\\
    0, \qquad \qquad \qquad\text{en otro caso}
\end{cases},
\end{align*}
donde $x_1,\dots, x_k$ son enteros no negativos.\\
\end{definition}

%Notamos que aunque los sucesos sean independientes, sus salidas $X$ son dependientes, ya que deben sumar $n$.

\begin{example}
  Si tenemos un dado de $k$ lados y queremos calcular cuántas veces sale cada uno de ellos en $n$ lanzamientos, tendremos una distribución multinomial.\\
\end{example}

El valor esperado de observar la salida $i$ en $n$ sucesos es $\mathbb{E}(X_i) = np_i$, luego $\mathbb{E}(X) = np$.\\

Veamos cómo son los elementos de la matriz de covarianzas. Los elementos de la diagonal son la varianza de una variable aleatoria con distribución binomial, luego $\Var(X_i) = np_i(1-p_i)$. Las covarianzas de $X_i, X_j$ para $i\neq j$ son $Cov(X_i, X_j) = - n p_i p_j$. Por tanto,\[\Var(X) = n [\text{diag}(p) - p p^T],\] donde diag$(p)$ representa la matriz diagonal que tiene el elemento $p_i$ en la posición $i,i$.\\
%----------------------MULTINOMIAL-FIN----------------------------------

%--------------------------Estudio de la dependencia--------------------
\subsection{Estudio de la dependencia}

Dadas dos variables aleatorias nos podemos preguntar si son o no independientes, es decir, si la realización de una de ellas afecta o no a la realización de la otra. En caso de ser dependientes nos gustaría medir qué tipo de relación existe entre ellas. Hay numerosos métodos que nos permiten realizar esta medida, por ejemplo, el \textit{coeficiente de correlación de Pearson} o el \textit{coeficiente de correlación de Spearman}.\\

Medir la dependencia entre variables puede ser útil porque nos indica relaciones entre variables que se pueden aprovechar para hacer predicciones de una a partir de la otra. Encontramos infinidad de aplicaciones en el mundo real a conocer una dependencia entre variables. Por ejemplo, para conocer qué efecto tiene un fenómeno sobre otro. También, si conocemos la dependencia entre variables, podemos utilizar aquellas de las que tenemos información más precisa o fiable para explicar aquellas de las que no.\\

\begin{definition}[Coeficiente de correlación de Pearson]
  Se define el coeficiente de correlación de Pearson para un par de variables aleatorias $(X,Y)$ como\[
r_{X,Y} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y} = \frac{\mathbb{E}\left [ (X- \mu_X)(Y-\mu_Y)\right ]}{\sigma_X\sigma_Y}.
\]
El mismo coeficiente para una realización muestral $\{(x_1,y_1),\dots,(x_n,y_n)\}$ se define como
\[
r_{xy}=\frac{\sum_{i=1}^n(x_i-\olsi{x})(y_i-\olsi{y})}{\sqrt{\sum_{i=1}^n(x_i-\olsi{x})^2(y_i-\olsi{y})^2}},
\] donde $\olsi{x} = \frac{1}{n}\sum_{i=1}^nx_i$ la media muestral, de forma análoga para $\olsi{y}$.\\
\end{definition}

Este coeficiente mide la correlación lineal entre las variables $X,Y$. Su valor se encuentra entre $+1$ y $-1$, donde $+1$ indica una correlación lineal positiva total, $0$ que no hay correlación lineal y $-1$ correlación lineal total negativa. Si el coeficiente de correlación de Pearson de dos variables aleatorias es $0$, sabremos que no existe una dependencia lineal entre ellas, pero no podremos garantizar la independencia, pues podría existir otro tipo de dependencia entre ellas.\\

Este coeficiente es paramétrico, depende de las distribuciones marginales, el que presentamos a continuación es no paramétrico.\\

El coeficiente de correlación de Spearman mide la relación entre dos variables más allá de si es lineal o no. Atiende a si ambas variables tienen la misma monotonía, esto es, si a medida que los valores de una de las variables crecen, los de la otra también lo hacen (o al revés). Para ello, tendremos que ordenar los datos de cada variable de menor a mayor.\\


\begin{definition}[Coeficiente de correlación de Spearman]
  Sea $(X,Y)$ una variable aleatoria bidimensional con distribución conjunta $F(x,y)$ y distribuciones marginales $F_X(x)$, $F_Y(y)$ respectivamente. Sean ahora $(X_1,Y_1)\sim F$ y $(X_2,Y_2)\sim F_X \cdot F_Y$ independientes entre sí, entonces el coeficiente $\rho$ de Spearman se define como\[
\rho_{XY} = 3\left ( P\left[(X_1-X_2)(Y_1-Y_2) > 0\right] -  P\left[(X_1-X_2)(Y_1-Y_2) < 0\right]\right).
  \]
  
  Para una realización $\{(x_1,y_1),\dots,(x_n,y_n)\}$ de dos variables aleatorias $X$ e $Y$, ordenamos los datos de ambas variables de forma que $R_x = (R_{x_1},\dots,R_{x_n})$ es el vector de rangos de los $x_i$ y $R_y = (R_{y_1},\dots,R_{y_n})$ el de los $y_i$. El coeficiente de correlación de Spearman entre $X$ e $Y$ viene dado por\[
\rho_{XY}=r_{R_xR_y}=\frac{Cov(R_x,R_y)}{\sigma_{R_x}\sigma_{R_y}},
\] donde $r$ denota el coeficiente de correlación de Pearson. Si todos los puntos son distintos, se puede utilizar la fórmula:\[
\rho_{XY} = 1 - \frac{6\sum_i d_i^2}{n(n^2-1)},
\]donde $d_i = R_{x_i}-R_{y_i}$ es la diferencia entre los rangos de $x_i$ e $y_i$.\\
\end{definition}

Este coeficiente también toma valores entre $-1$ y $+1$. Si los valores son cercanos a cero la correlación monótona será muy débil, mientras que si son cercanos a $+1$ o $-1$ será muy fuerte, el signo nos indicará si a medida que una crece la otra decrece o viceversa. Al igual que coeficiente de Pearson, el de Spearman no caracteriza la independencia. Aunque ambos coeficientes, si las variables aleatorias son independientes, valdrán $0$.\\

Así, hemos visto un par de ejemplos de medidas de dependencia que caracterizan adecuadamente tipos concretos de dependencia, pero no caracterizan la independencia.\\

La información mutua es otra forma de medir la dependencia entre dos variables aleatorias capaz de determinar otros tipos de dependencias, además de las lineales o monótonas. Podríamos decir que es una medida de la dependencia global. Además, esta medida también nos dará información sobre la independencia de las variables aleatorias en estudio (véanse Teorema \ref{t:im_indd} y Corolario \ref{c:im_indc}).\\

%-----------------------------------------------------------------------
%-----------------------TEORÍA DE LA INFORMACIÓN------------------------
%-----------------------------------------------------------------------
    \section{Teoría de la información}

    El concepto de información es demasiado amplio como para ser recogido en una única definición. Sin embargo, podemos definir el concepto de entropía, que mide la cantidad de información necesaria para describir una variable aleatoria, o el de información mutua, que refleja la información que una variable aleatoria contiene sobre otra.\\

    Intuitivamente, la entropía es una medida de la información o incertidumbre de una variable aleatoria. El término entropía viene del griego, donde significa \textit{transformación} y tiene relevancia también en otras áreas, como en la física donde se define como el logaritmo del cociente entre temperatura final e inicial de un sistema.\\

    La idea de relacionar el número de estados de un sistema con una medida física viene del siglo XIX, en el que Rudolph Clausius sugirió la denominación ``entropía'' para esta medida. En 1928, Ralph Vinton Lyon Hartley define el término en el contexto de la teoría de la información, básicamente como el logaritmo del tamaño del alfabeto. Aunque no es hasta 1948 cuando Claude Shannon da una definición matemática del concepto de información como lo conocemos hoy en día,  sentando las bases de la teoría de la información, que pasó a formar parte de la teoría de la probabilidad. En su concepto de entropía no todos los símbolos de un alfabeto tienen porqué ser equiprobables. Esto nos permite medir la capacidad de comunicación de un canal. Andrei N. Kolmogorov, en la década de los 60, desarrolló su teoría de la complejidad, otra forma de teoría de la información.\\

A continuación, se explicarán estos conceptos, y algunos relacionados, junto a demostraciones de sus propiedades y algunos ejemplos, tanto para el caso discreto como para el caso continuo. Las principales fuentes consultadas para ello han sido \cite{thomas} y \cite{gray}. Se han estudiado y comprendido los capítulos correspondientes a entropía y entropía diferencial para posteriormente exponer aquí una síntesis de lo aprendido.\\

\subsection{Caso discreto}
    
Sea una variable aleatoria discreta, $X$, que toma valores en el alfabeto $\mathcal{X} = \{x_1, \dots, x_N \}$, con función masa de probabilidad $P_X(x_i) = p(x_i)$.\\

\begin{definition}[Entropía]
  La entropía de una variable aleatoria discreta $X$ viene dada por \[H(X) = - \sum_{x\in \mathcal{X}} p(x) \log p(x).\]\\[-15pt]
\end{definition}

En la fórmula anterior tomaremos, por continuidad, $0 \log 0 = 0$. Así, la entropía queda definida por un funcional de la distribución de $X$, es decir, que no depende de los valores que tome la variable sino de sus probabilidades.\\

Según cuál sea la base del logaritmo utilizado en la fórmula de la entropía utilizaremos unas unidades u otras. Para $b\in \mathbb{N}$, notaremos $H_b(X) = - \sum_{x\in \mathcal{X}} p(x) \log_b p(x)$. Si el logaritmo tiene base 2, la entropía se medirá en \textit{bits}, mientras que si la base es $e$, las unidades serán \textit{nats} (unidad natural de información).\\

Dada la variable aleatoria discreta $X$ con distribución $p(x)$, podemos considerar la variable aleatoria $g(X)$, cuya esperanza, $\E$, viene dada por:\[
\E_p\left[g(X)\right] = \sum_{x\in \mathcal{X}}g(x)p(x).
\]
Tomando $g(X) = \log \frac{1}{p(X)}$ obtenemos otra definición de entropía, la usada en termodinámica:\[
H(X) = \E_p \left[\log \frac{1}{p(X)}\right].
\]\\[-10pt]

La fórmula anterior coincide, efectivamente, con la definición de entropía dada:\[
H(X) = \E_p \left[\log \frac{1}{p(X)}\right] = \sum_{x\in \mathcal{X}}\left ( \log \frac{1}{p(x)} \right ) p(x) = - \sum_{x \in \mathcal{X}}p(x)\log p(x).
\]\\[-10pt]

\begin{lemma} La entropía de una variable aleatoria discreta es no negativa, $H(X) \ge 0$.
\end{lemma}
\begin{proof}
  \[H(X) = - \sum_{x \in \mathcal{X}}\underbrace{p(x)}_{\in [0,1]}\underbrace{\log p(x)}_{\leq 0} \ge 0.\]

  %% El producto de ambos factores es negativo para cualquier $x \in \mathcal{X}$, luego el signo menos en la fórmula de la entropía nos permite concluir que $H(X) = - \sum_{x \in \mathcal{X}}p(x)\log p(x) \ge 0$.
\end{proof}

El siguiente lema, mediante propiedades del logaritmo, nos permitirá cambiar la base con la que calculemos la entropía.\\

\begin{lemma}\[H_b(X) = (\log_ba) H_a(X).\]
\end{lemma}
\begin{proof}
  Partiremos de la definición alternativa de $H_b$ y usaremos que $\log_bp = \log_ba\ \log_ap$.
  \[
  H_b(X) = \E_p \left[\log_b \frac{1}{p(X)}\right ] = \E_p \left [ \left ( \log_b a \right )\log_a \frac{1}{p(X)} \right ] = \left ( \log_b{a} \right ) H_a(X).
  \]\\[-10pt]
\end{proof}

En el siguiente ejemplo calculamos la entropía de una variable aleatoria con probabilidades fijadas.
\begin{example}
  Sea  \[X = 
     \begin{cases}
       a, \quad\text{con probabilidad } \frac{1}{2}\\
       b, \quad\text{con probabilidad } \frac{1}{4} \\
       c, \quad\text{con probabilidad } \frac{1}{8} \\
       d, \quad\text{con probabilidad } \frac{1}{8} \\
     \end{cases}.\]

     Utilizamos la fórmula de la entropía y obtenemos
     \begin{align*}
       H(X) &= - \left (\frac{1}{2} \log \frac{1}{2} + \frac{1}{4} \log \frac{1}{4} + \frac{1}{4} \log \frac{1}{8} \right ) = - \left (-\frac{1}{2} - \frac{1}{2} - \frac{3}{4} \right )\\[5pt] &= 1 + \frac{3}{4} = \frac{7}{4} \text{ bits}.\\
       \end{align*}
\end{example}

\begin{example}
  Sea \[X = 
     \begin{cases}
       1, \quad\text{con probabilidad } p\\
       0, \quad\text{con probabilidad } 1-p \\
     \end{cases}.\]
     Vamos a analizar la entropía de la variable aleatoria $X$ como función de $p$, es decir, $H:[0,1]\to \mathbb{R}$ estará dada por $H(p)$. Se calcula como sigue:\[ H(p) := H(X) = - \left ( p \log p + (1-p) \log (1-p) \right ).\]
     Cuando $p=0$ o $p=1$, $H(p) = 0$, luego no hay incertidumbre, se conoce el valor que tomará la variable $X$. Calcularemos la primera y segunda derivada de $H(p)$ para conocer su comportamiento en el intervalo $(0,1)$.
     \begin{align*}
       H'(p) &= - \left ( \log p + \frac{p}{p} - \log(1-p) - \frac{1-p}{1-p} \right ) = - \left ( \log \left (\frac{p}{1-p} \right) \right ),\\
     H''(p) &= - \left ( \frac{1}{p} + \frac{1}{1-p} \right ) \leq 0 \quad \forall p \in (0,1).
     \end{align*}
     
     Hemos comprobado que la función $H(p)$ es cóncava. La incertidumbre máxima se alcanzará en $p=\frac{1}{2}$: \[H\left (\frac{1}{2}\right ) = - \left ( \frac{1}{2}\log\frac{1}{2} + \frac{1}{2} \log\frac{1}{2} \right ) = - \log \frac{1}{2} = 1.\]\\[-10pt]
\end{example}


La definición de entropía de una variable aleatoria se puede aplicar a un par de variables aleatorias dando lugar a la entropía conjunta.\\

\begin{definition}[Entropía conjunta]
  La entropía conjunta, $H(X,Y)$, de un par de variables aleatorias discretas $(X,Y)$ con distribución conjunta $p(x,y)$ se define como \[
  H(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y), \]
  que podemos expresar también como \[
  H(X,Y) = - \E \left[\log p(X,Y)\right].\]\\[-10pt]
\end{definition}

Presentamos el siguiente lema que será útil para la demostración de propiedades en lo sucesivo.\\

\begin{lemma}[Desigualdad de Gibbs]\label{l:gibbs}
  Dadas dos funciones masa de probabilidad $\{p_i\}$ y $\{q_i\}$, entonces \[
\sum_i p_i \log \frac{p_i}{q_i} \ge 0,
\]
dándose la igualdad si, y solo si, $q_i = p_i$ para todo $i$.
\end{lemma}

\begin{proof}
  Sea $I = \{i : p_i > 0\}$, entonces\[
- \sum_{i \in I} p_i \log \frac{p_i}{q_i} = \sum_{i \in I} p_i \log \frac{q_i}{p_i},
  \]
  usando $\log x \le x - 1$, con igualdad si, y solo si, x = 1, obtenemos\[
  \sum_{i \in I} p_i \log \frac{q_i}{p_i} \leq
  \sum_{i\in I}p_i \left ( \frac{q_i}{p_i} - 1 \right )
  = \sum_{i\in I}q_i - \underbrace{\sum_{i \in I}p_i}_{= 1} \leq 0.\]

  Tendremos la igualdad cuando $\frac{q_i}{p_i} = 1$ para todo $i$.\\
\end{proof}

En el siguiente lema recogemos algunas propiedades de la entropía conjunta.\\

\begin{lemma}\label{l:prop_ent_conj} Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifican:
  \begin{enumerate}
  \item $H(X) \le H(X, Y)$.
  \item $H(X,Y) \le H(X) + H(Y)$. La igualdad se dará si, y solo si, $X$ e $Y$ son independientes.
  \end{enumerate}
\end{lemma}

\begin{proof}\hfill
  \begin{enumerate}
  \item Para cualesquiera $x \in \mathcal{X}, y \in \mathcal{Y}$ se tiene que $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y $p(x,y) \le p(x)$, luego:
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \ge - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x)\\
   &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) = H(X).
\end{align*}

\item Usando la definición, $\sum_{y \in \mathcal{Y}} p(x,y) = p(x)$ y propiedades del logaritmo:
  \begin{align*}
    H&(X,Y) - (H(X) + H(Y)) \\[5pt] =& - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} p(x)\log p(x) + \sum_{y \in \mathcal{Y}} p(y)\log p(y)\\[5pt]
    = &- \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(x)\\ &+ \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y)\log p(y)\\[3pt]
    = &\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x)p(y)}{p(x,y)} \leq 0,
  \end{align*}
  donde la última desigualdad viene de aplicar el Lema \ref{l:gibbs} sobre las funciones masa de probabilidad $p(x,y)$ y $p(x)p(y)$. Esta última sería la función masa de probabilidad conjunta de dos variables aleatorias $X,Y$ con distribuciones $p(x), p(y)$ independientes. La igualdad ocurre si, y solo si, $p(x)p(y) = p(x,y)$, esto es si $X$ e $Y$ son independientes.
  \end{enumerate}
\end{proof}

Al considerar dos distribuciones distintas, además de su entropía conjunta podemos definir la entropía condicional de una respecto de la otra.\\

\begin{definition}[Entropía condicional]
  Si $(X,Y)$ sigue una distribución $p(x,y)$, la entropía condicional de $Y$ con respecto a $X$, $H(Y|X)$, se define como:
  \begin{align*}
    H(Y|X) &= \sum_{x \in \mathcal{X}} p(x) H(Y | X = x)\\[5pt]
    &= - \sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y | x) \log p(y | x)\\[5pt]
    &=  - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y | x)\\[5pt]
    &=- \E\left[ \log p(Y|X)\right].
  \end{align*}
\end{definition}

\begin{theorem}[Regla de la cadena]\label{t:regla_cadena}
  Si $(X,Y)$ sigue una distribución conjunta $p(x,y)$, entonces se verifica:\[
H(X,Y) = H(X) + H(Y|X).
  \]
\end{theorem}

\begin{proof}
Para la prueba utilizaremos que $p(x,y) = p(x)p(y|x)$,
  \begin{align*}
    H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y)\\[5pt] &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \left( p(x)p(y|x) \right )\\[3pt]
    &= - \left(\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y|x) \right ),
  \end{align*}
usando que $\sum_{y\in \mathcal{Y}}p(x,y) = p(x)$, obtenemos
\begin{align*}
  H(X,Y) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) + H(Y|X)\\
    &= H(X) + H(Y|X).
  \end{align*}
\end{proof}

\begin{corollary} En las condiciones del Teorema \ref{t:regla_cadena},\[
H(X,Y|Z) = H(X|Z) + H(Y|X, Z).
  \]\\[-10pt]
\end{corollary}

\begin{lemma}
  Sean $X, Y$ dos variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces la entropía condicional cumple:\[
0 \leq H(Y|X) \leq H(Y).
\]
Donde la desigualdad de la derecha es una igualdad si, y solo si, $X$ e $Y$ son independientes.
\end{lemma}

\begin{proof}
  La desigualdad izquierda se obtiene de la definición de entropía condicional, \[H(Y|X) = - \E [\underbrace{\log \overbrace{p(Y|X)}^{\in [0,1]}}_{\leq 0} ]\ge 0.\]

  Aplicando el Teorema \ref{t:regla_cadena} y el Lema \ref{l:prop_ent_conj} se obtiene la desigualdad de la derecha:\[
H(X) + H(Y|X) = H(X,Y) \leq H(X) + H(Y),
\]
dándose la igualdad solo en el caso en que las variables $X$ e $Y$ sean independientes.\\
\end{proof}

\begin{example}\label{e:dist_conj}
Sea $(X,Y)$ una variable aleatoria con distribución conjunta:
  \begin{table}[H]
\centering
%\caption{}
\label{}
\begin{tabular}{r|llll|l}
 \toprule
 \backslashbox{$Y$}{$X$} & 1 & 2 & 3 & 4&\\ %\hline\\[-10pt]
 \cline{1-5} \\[-10pt]
1 & $\frac{1}{8}$ & $\frac{1}{16}$ & $\frac{1}{32}$ & $\frac{1}{32}$  & $\frac{1}{4}$ \\[5pt]
2 & $\frac{1}{16}$ & $\frac{1}{8}$ & $\frac{1}{32}$ & $\frac{1}{32}$ & $\frac{1}{4}$ \\[5pt]
3 & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{4}$ \\[5pt]
4 & $\frac{1}{4}$ & 0 & 0 & 0 & $\frac{1}{4}$ \\[5pt]
%\hline\\[-10pt]
\cline{2-5}\\[-10pt]
& $\frac{1}{2}$ &  $\frac{1}{4}$ &  $\frac{1}{8}$ &  $\frac{1}{8}$ \\[5pt]   
\bottomrule
\end{tabular}
\end{table}

  Calculamos las entropías de las variables marginales:
  \begin{align*}
    H(X) &= - \frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} - 2 \frac{1}{8} \log \frac{1}{8} = -\frac{1}{2} - \frac{1}{2} - \frac{6}{8} = \frac{7}{4} \text{ bits}.\\[5pt]
    H(Y) &= - 4 \frac{1}{4} \log \frac{1}{4} = 2 \text{ bits}.
  \end{align*}
  Calculamos la entropía conjunta:
  \begin{align*}
    H(X,Y) &= - \frac{1}{4} \log \frac{1}{4} - \frac{2}{8} \log \frac{1}{8} - \frac{6}{16} \log \frac{6}{16} - \frac{4}{32} \log \frac{1}{32}\\[5pt]
    &= \frac{1}{2} + \frac{3}{4} + \frac{6}{4} + \frac{5}{8} = \frac{27}{8} \text{ bits}.\\
  \end{align*}

  Vemos que, efectivamente, la entropía conjunta es mayor que cada una de las individuales, pero menor que su suma $\left( \frac{30}{8} \right )$. Calculamos las entropías condicionales:
  \begin{align*}
    H(Y|X) = &- \frac{1}{8} \log \frac{1}{4} - \frac{2}{16} \log\frac{1}{8} - \frac{1}{4} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\[5pt]
    &- \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2} - \frac{2}{32} \log \frac{1}{4} - \frac{1}{16} \log \frac{1}{2}\\[5pt]
    =& \frac{1}{2} + \frac{6}{8} + \frac{3}{8} = \frac{13}{8} \text {bits}.\\[10pt]
    H(X|Y) = & - \frac{1}{8} \log \frac{1}{2} - \frac{2}{16} \log \frac{1}{4}- \frac{2}{16} \log \frac{1}{4} - \frac{1}{8} \log \frac{1}{2}\\[5pt]
    &- \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4} - \frac{2}{32} \log \frac{1}{8} - \frac{1}{16} \log \frac{1}{4}\\[5pt]
    =& \frac{1}{4} + \frac{6}{8} + \frac{3}{8} = \frac{11}{8} \text{ bits}.\\
  \end{align*}
  Observamos que las entropías condicionales son positivas y menores que las entropías individuales, además $H(Y|X) \neq H(X|Y)$. Comprobamos que se verifica el Teorema \ref{t:regla_cadena}:
  \begin{align*}
    H(X,Y) &= H(X) + H(Y|X) = \frac{7}{4} + \frac{13}{8} = \frac{27}{8} \text{ bits}.\\[5pt]
    H(X,Y) &= H(Y) + H(X|Y) = 2 + \frac{11}{8} = \frac{27}{8} \text{ bits}.\\[-5pt]
  \end{align*}
\end{example}

Pasamos a definir otra medida de información, la entropía relativa, esta será una medida de la distancia entre dos distribuciones. La podemos ver como una medida de ineficiencia al asumir que la distribución es $q$ cuando en realidad es $p$. Será una medida de ineficiencia en el sentido de cuánta información perderíamos si utilizáramos la distribución $q$ para aproximar a la distribución $p$.\\

\begin{definition}[Entropía relativa]
  La entropía relativa (o ``distancia'' Kullback Leibler) entre dos funciones masa de probabilidad $p(x)$ y $q(x)$ se define como\[
D(p||q) = \sum_{x \in \mathcal{X}}p(x) \log \frac{p(x)}{q(x)} = \E_p \left[\log \frac{p(X)}{q(X)}\right].
  \]\\[-10pt]
\end{definition}
Las posibles indeterminaciones en la fórmula anterior se definen como: $0 \log \frac{0}{0} = 0$ y $p \log \frac{p}{0} = \infty$.\\

\begin{lemma}\label{l:ent_rel_pos}
  Para dos funciones masa de probabilidad $p(x)$ y $q(x)$ se tiene \[
  D(p||q) \ge 0,\]
  con igualdad si, y solo si, $p(x) = q(x) \quad \forall x \in \mathcal{X}$.
\end{lemma}
\begin{proof}
  Usando la definición de entropía relativa, se obtiene el resultado aplicando el Lema \ref{l:gibbs}.\\
\end{proof}

La entropía relativa no es realmente una distancia, ya que no es simétrica ni verifica la desigualdad triangular. Sin embargo, a veces es útil pensar en ella como una medida de distancia entre distribuciones.\\

\begin{example}
  Sea $\mathcal{X} = \{0,1\}$ y consideramos dos distribuciones $p$ y $q$ en $\mathcal{X}$. Tomamos $p(0) = 1-r$, $p(1) = r$ y $q(0) = 1-s,\ q(1) = s$, con $r,s \in [0,1]$.
  \begin{align*}
    D(p \Vert q) &= \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = (1 - r) \log \frac{1-r}{1-s} + r \log \frac{r}{s}.\\[5pt]
    D(q \Vert p) &= \sum_{x \in \mathcal{X}}q(x) \log \frac{q(x)}{p(x)} = (1-s) \log \frac{1-s}{1-r} + s \log \frac{s}{r}.
  \end{align*}

  Si tuviéramos $r=s$, entonces $D(p \Vert q) = 0 = D(q \Vert p)$.
  Tomando $r = \frac{1}{2}$ y $s = \frac{1}{4}$ comprobaremos que no se da la simetría de la entropía relativa.
  \begin{align*}
    D(p \Vert q) &= \frac{1}{2} \log \frac{1/2}{3/4} + \frac{1}{2} \log \frac{1/2}{1/4} = 1 - \frac{1}{2} \log 6 + \frac{1}{2} = 1 - \frac{1}{2} \log 3\\ &= 0.2075 \text{ bits}.\\[5pt]
    D(q \Vert p) &= \frac{3}{4} \log \frac{3/4}{1/2} + \frac{1}{4} \log \frac{1/4}{1/2} = \frac{3}{4} \log 6 - \frac{3}{2} - \frac{1}{4} = -1 + \frac{3}{4} \log 3\\ &= 0.1887 \text{ bits}. % \frac{2}{4} - \frac{3}{2} + \frac{3}{4} \log 3 =
  \end{align*}
Luego, $D(p\Vert q) \ne D(q \Vert p)$.\\
\end{example}

\begin{lemma}
  Si $X$ es una variable aleatoria con alfabeto $\mathcal{X}$, entonces \[
  H(X) \leq \log{\left\Vert\mathcal{X}\right\Vert}, 
  \]
donde $\left\Vert\mathcal{X}\right\Vert$ representa al cardinal de $\mathcal{X}$.
\end{lemma}
\begin{proof}
  Sea $u(x) = \frac{1}{\left\Vert\mathcal{X}\right\Vert}$ la función masa de probabilidad uniforme sobre $\mathcal{X}$, y sea $p(x)$ la función masa de probabilidad de $X$. Entonces,
  
 \begin{align*}
   D \left ( p \Vert u \right ) &= \sum_{x \in \mathcal{X}}
   p(x) \log \frac{p(x)}{u(x)} = \sum_{x \in \mathcal{X}} p(x) \log p(x) - \sum_{x \in \mathcal{X}} p(x) \log \frac{1}{\left\Vert\mathcal{X}\right\Vert}\\
  &= -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
 \end{align*}
 
    Usando el Lema \ref{l:ent_rel_pos}, obtenemos el resultado: \[
0 \leq D\left ( p \Vert u \right ) = -H(X) + \log \left\Vert\mathcal{X}\right\Vert.
    \]
\end{proof}

La información mutua es una medida de la dependencia entre dos variables aleatorias. Mide la cantidad de información que tenemos sobre una de ellas si hemos observado la otra. Se define como sigue.

\begin{definition}[Información mutua]\label{def:im_disc}
  Dadas dos variables aleatorias $X$ e $Y$ discretas con función masa de probabilidad conjunta $p(x,y)$ y funciones masa de probabilidad marginales $p(x)$ y $p(y)$. La información mutua, representada por $I(X;Y)$, es la entropía relativa entre la distribución conjunta y la distribución producto $p(x)p(y)$.
  \begin{align*}
  I(X;Y) &= D \left ( p(x,y) \Vert p(x)p(y) \right ) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\ &= \E_{p(x,y)} \left [ \log \frac{p(X,Y)}{p(X)p(Y)}\right].\\
  \end{align*}
\end{definition}

Notamos que la información mutua, al estar definida como una entropía relativa, será no negativa (Lema \ref{l:ent_rel_pos}).\\

\begin{theorem}[Relación entre entropía e información mutua]\label{t:ent_im}
  La siguiente lista recoge algunas relaciones entre la entropía y la información mutua.
\begin{enumerate}[label={\alph*)}]
  \item $I(X;Y) = H(X) - H(X|Y)$.
  \item $I(X;Y) = H(Y) - H(Y|X)$.
  \item $I(X;Y) = H(X) + H(Y) - H(X,Y)$.
  \item $I(X;Y) = I(Y;X)$.
  \item $I(X;X) = H(X)$, la entropía es un caso particular de la información mutua.
  \end{enumerate}
\end{theorem}
\begin{proof}
   
  \begin{align*}
    \text{a) }I(X;Y) &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\[3pt] &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(y) p(x|y) \log \frac{p(y)p(x|y)}{p(x)p(y)}\\
    &= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(x|y) - \sum_{x \in \mathcal{X}}\underbrace{\left ( \sum_{y \in \mathcal{Y}} p(y) p(x|y) \right)}_{p(x)} \log p(x)\\
    &= - H(X|Y) + H(X).
  \end{align*}
  
  b) Se obtiene de forma análoga.\\
  
  c) Se prueba aplicando la regla de la cadena sobre $H(X|Y)$, \[I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y).\]\\[-20pt]
  
  d) Se prueba usando el apartado c).\\
  
  e) $I(X;X) = H(X) - H(X|X) = H(X)$.\\
  
\end{proof}

\begin{example}
  Para las distribuciones consideradas en el Ejemplo \ref{e:dist_conj}, calculamos la información mutua:
  \begin{align*}
    I(X;Y) =\ & \frac{1}{8} \log \frac{1/8}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4}\\[3pt] +& \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4} + \frac{1}{8} \log \frac{1/8}{1/4 \cdot 1/4} + \frac{2}{32} \log \frac{1/32}{1/8 \cdot 1/4}\\[3pt] +& \frac{1}{16} \log \frac{1/16}{1/2 \cdot 1/4} + \frac{1}{16} \log \frac{1/16}{1/4 \cdot 1/4} + \frac{2}{16} \log \frac{1/16}{1/8 \cdot 1/4}\\[3pt] +& \frac{1}{4} \log \frac{1/4}{1/2 \cdot 1/4} = \frac{2}{16} \log \frac{1}{2} + \frac{1}{2} \log 2 = \frac{3}{8} \text{ bits}.
  \end{align*}

  Vemos que se cumplen las propiedades del Teorema \ref{t:ent_im}:
  \begin{align*}
    H(X) -H(X|Y) &= \frac{7}{4} - \frac{11}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(Y) -H(Y|X) &= 2 - \frac{13}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
    H(X) + H(Y) - H(X,Y) &= \frac{7}{4} + 2 - \frac{27}{8} = \frac{3}{8} \text{ bits} = I(X;Y).\\
  \end{align*}
\end{example}

\begin{theorem}\label{t:im_indd}
  Sean $X$, $Y$ variables aleatorias discretas con alfabeto $\mathcal{X}, \mathcal{Y}$ respectivamente. Entonces se verifica que $I(X;Y)\ge 0$, dándose la igualdad si, y solo si, $X,Y$ son independientes.
\end{theorem}
\begin{proof}
El apartado c) del Teorema \ref{t:ent_im} nos dice que $I(X;Y) = H(X)+H(Y)-H(X,Y)$, usando el Lema \ref{l:prop_ent_conj}.2 tendremos que $I(X;Y)\ge 0$, dándose la igualdad si, y solo si, $X,Y$ son independientes.\\
\end{proof}
%-----------------------------------------------------------------------
%----------------------- CASO CONTINUO ---------------------------------
\subsection{Caso continuo}
Hasta ahora las definiciones dadas eran para variables aleatorias discretas, que solo tomaban un conjunto finito de valores. Podemos extender las definiciones estudiadas al caso continuo, para aplicarlas en variables aleatorias continuas. Trabajaremos en este apartado con una variable aleatoria $X$ absolutamente continua con función de distribución $F$, función de densidad $f$ y conjunto soporte $sop(X)$.\\

Las definiciones a continuación incluyen integrales, por lo que entenderemos que todas ellas existen para no alargar la redacción.\\

La entropía en el caso continuo pasa a llamarse entropía diferencial y podemos definirla como sigue.\\

\begin{definition}[Entropía diferencial]
  La entropía diferencial $h(X)$ de una variable aleatoria continua $X$ viene dada por \[
h(X) = - \int_{sop(X)}\log \left(f(x)\right) f(x) dx.
\]\\[-10pt]
\end{definition}

Como en el caso discreto, la entropía diferencial solo depende de la probabilidad de la variable aleatoria, no de los valores que toma. Notamos que en la definición anterior la variable aleatoria $X$ podría ser, o bien, unidimensional, o bien, multidimensional.\\

\begin{example}\label{ej:uni} Consideramos una variable aleatoria $X$ absolutamente continua con una distribución uniforme sobre $[0,a]$. En este intervalo su densidad es $f(x) = \frac{1}{a}$ para $x\in[0,a]$ y para el resto de puntos es nula. La entropía es \[
h(X) = - \int_0^a\log\left(\frac{1}{a}\right)\frac{1}{a}dx = \log a.
  \]\\[-5pt]
Notamos que si $a<1, \log a < 0$, esto es, la entropía en el caso continuo puede ser negativa, no como en el caso discreto.\\
\end{example}

\begin{example}\label{ej:norm_uni}
  Consideramos una variable aleatoria $X$ con distribución normal univariante de media $\mu$ y varianza $\sigma^2$, su función de densidad es\[ f(x) = \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}},\] queremos calcular su entropía.
  \begin{align*}
    h(X) &= - \int_{\mathbb{R}} \log \left(f(x)\right) f(x)  dx\\ &= - \int_{\mathbb{R}}  \log \left ( \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \right ) \frac{1}{\sigma \sqrt{2\pi}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}dx\\[5pt]
    &= -  \frac{1}{\sigma \sqrt{2\pi}} \int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \left [ \log \left ( e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \right ) - \log \left(\sigma \sqrt{2\pi}\right ) \right ] dx\\[5pt]
    &= -  \frac{1}{\sigma \sqrt{2\pi}}\left [ \int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}\! \left ( -\frac{(x-\mu)^2}{2\sigma ^2} \right )\!dx -\!\!\int_{\mathbb{R}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \log \left ( \sigma \sqrt{2\pi}\right ) dx \right]\!.\\[-5pt]
  \end{align*}

  Ahora, utilizando la ley del estadístico inconsciente (que recordamos en la Proposición \ref{p:prop_esp}), tenemos,
  \begingroup
\allowdisplaybreaks
  \begin{align*}
    h(X) &= \frac{\mathbb{E}\left [ (X-\mu) ^2\right ] }{2\sigma^2} + \log\left( \sigma\sqrt{2\pi} \right )\underbrace{\int_{\mathbb{R}} \frac{1}{\sigma\sqrt{2\pi}}e ^{-\frac{(x-\mu)^2}{2\sigma ^2}}dx}_{1}\\[3pt]
    &= \frac{\sigma^2}{2\sigma^2} + \log \sqrt{\sigma^2 2 \pi} = \frac{1}{2} + \frac{1}{2}\log\left(  \sigma^2 2 \pi\right)\\[3pt]
    &= \frac{1}{2} \log e + \frac{1}{2} \log\left( \sigma^2 2 \pi\right)\\ &= \frac{1}{2}\log\left({2\pi e\sigma^2} \right) \text{ nats.}\\
  \end{align*}
  \endgroup
  
\end{example}


\begin{example}\label{ej:norm_multi}
  Tomamos una variable aleatoria multidimensional $X = (X_1,\dots,X_n)$ con distribución normal multivariante de vector de medias $\mu$ y matriz de covarianzas $\Sigma$. Su función de densidad es \[f(x) = \frac{1}{\left (\sqrt{2\pi}\right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}.\]
    
    Comenzamos los cálculos de la entropía,
  \begin{align*}
    h(X) &= - \int_{\mathbb{R}^n} \log\left( f(x)\right)f(x) dx \\
    &= - \int_{\mathbb{R}^n} \log \left ( \frac{1}{\left ( \sqrt{2\pi} \right )^n |\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \right )f(x) dx\\[3pt]
    &= - \int_{\mathbb{R}^n}\left (- \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) - \log\left [ \left (\sqrt{2\pi} \right )^n |\Sigma|^{\frac{1}{2}} \right ]  \right ) f(x) dx\\[3pt]
    &= \frac{1}{2} \mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(X-\mu) \right ] + \frac{1}{2}\log\left [(2\pi)^n|\Sigma|\right ].\\[-5pt]
  \end{align*}

  Continuamos desarrollando la expresión $\mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(X-\mu) \right ]$,
  \begin{align*}
    \mathbb{E}\left [ (X-\mu)^T\Sigma^{-1}(X-\mu) \right ] &= \mathbb{E}\left [ \sum_{i,j}(X_i - \mu)(\Sigma^{-1})_{ij}(X_j-\mu)\right ]\\
    &= \mathbb{E}\left[ \sum_{i,j}(X_j-\mu)(X_i-\mu)(\Sigma^{-1})_{ij}\right]\\
    &= \sum_{i,j}\underbrace{\mathbb{E}\left[ \sum_{i,j}(X_j-\mu)(X_i-\mu)\right]}_{Cov(X_j,X_i)}(\Sigma^{-1})_{ij}\\
    &= \sum_j\sum_i(\Sigma)_{ji}(\Sigma^{-1})_{ij} = \sum_j (\Sigma\Sigma^{-1})_{jj}\\ &= \sum_j (I_n)_{jj} = n.\\[-7pt]
  \end{align*}

  Sustituyendo este valor en la expresión de la entropía, obtenemos,
  \begin{align*}
    h(X) &= \frac{n}{2} + \frac{1}{2}\log\left [ (2\pi)^n|\Sigma|\right ] = \frac{1}{2} \left (\log e^n + \log \left[(2\pi)^n|\Sigma|\right] \right)\\[3pt]
    &= \frac{1}{2}\log\left[(2\pi e)^n|\Sigma|\right] \text{ nats.}\\
  \end{align*}

  
\end{example}

\begin{theorem}\label{t:mmlog}
  Sean $X_1,\dots, X_n$ una serie de variables aleatorias independientes e idénticamente distribuidas con función de densidad $f_X(x)$. Si notamos por $f(x_1,\dots,x_n)$ a la función de densidad conjunta, entonces\[
-\frac{1}{n}\log f(X_1,\dots, X_n) \xrightarrow[]{P} \mathbb{E}\left [ - \log f_X(X) \right ] = h(X).
  \]
\end{theorem}

\begin{proof}
  Como las variables son independientes, se verifica\[
-\frac{1}{n}\log f(X_1,\dots, X_n) = - \frac{1}{n} \sum_{i=1}^n \log f_X(X_i),
\]
por el Teorema \ref{t:lgn} tenemos que la expresión anterior converge en probabilidad a $\mathbb{E}\left [ - \log f_X(X) \right ] = h(X)$.\\
\end{proof}


\begin{definition}[Entropía diferencial condicionada]
  Si $X,Y$ son variables aleatorias con función de densidad conjunta $f(x,y)$, definimos la entropía diferencial condicionada $h(X|Y)$ como\[
h(X|Y) = -\int\int  \log \left(f(x|y)\right) f(x,y) dxdy.
\]
Como $f(x|y) = \frac{f(x,y)}{f(y)}$, también podemos definirla como\[
h(X|Y) = h(X,Y) - h(Y),
\]
siempre que las entropías diferenciales sean finitas.\\
\end{definition}

\begin{definition}[Entropía relativa]
  La entropía relativa (o ``distancia'' de Kullback-Leibler) $D(f||g)$ entre dos densidades $f$ y $g$ se define como \[
D(f||g) = \int_{\mathbb{R}^n} \log\left( \frac{f(x)}{g(x)}\right)f(x) dx.
  \]\\[-15pt]
\end{definition}

Notamos que $D(f||g)$ es finita si, y solo si, el conjunto soporte de $f$ está contenido en el conjunto soporte de $g$. 
Al igual que en el caso discreto y motivados por la continuidad consideraremos $0\log\frac{0}{0} = 0$.\\

\begin{definition}[Información mutua]\label{def:im_cont}
  La información mutua, $I(X;Y)$, entre dos variables aleatorias con función de densidad conjunta $f(x,y)$ y marginales $f(x)$ y $f(y)$ respectivamente, viene dada por
  \[
I(X;Y) = \int \int \log \left(\frac{f(x,y)}{f(x)f(y)}\right)f(x,y) dxdy = D(f(x,y)||f(x)f(y)).
  \]\\[-15pt]
\end{definition}

\begin{lemma}\label{l:ent_im}
  Siguiendo la notación anterior, la información mutua de dos variables aleatorias $X$, $Y$ verifica:\[
  I(X;Y) = h(X) + h(Y) - h(X,Y).
  \]
\end{lemma}

\begin{proof}
  Supongamos que las funciones de densidad marginales de $X$ e $Y$ son $f(x)$, $f(y)$, respectivamente, y la conjunta es $f(x,y)$.

  \begin{align*}
    I(X;Y) =& \int \int  \log \left(\frac{f(x,y)}{f(x)f(y)}\right)f(x,y)dxdy \\[3pt]
    =& \int \int  \log \left(f(x,y)\right)f(x,y)dxdy\\
    &- \int \int  \log \left(f(x)f(y)\right) f(x,y)dxdy\\[3pt]
    =& -h(X,Y)-\int\int \log \left(f(x)\right)f(x,y)  dy dx\\
    &- \int \int \log \left(f(y)\right)f(x,y)dxdy\\[3pt]
    =& -\int  \log f(x)f(x)dx - \int \log f(y) f(y)dy -h(X,Y)\\[3pt]
    =& h(X) + h(Y) - h(X,Y).\\[-10pt]
  \end{align*}

En la cadena de igualdades anterior hemos utilizado las propiedades de los logaritmos y el Teorema de Fubini.\\
\end{proof}

\begin{lemma}
  La información mutua de dos variables aleatorias $X$, $Y$ se relaciona con la entropía condicional de la siguiente forma:\[
I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X).
  \]
\end{lemma}

\begin{proof}
  Sean $f(x)$, $f(y)$ las funciones de densidad de $X$ e $Y$ respectivamente y $f(x,y)$ la función de densidad conjunta.
  Como en el lema anterior, partiremos de la definición de información mutua y utilizaremos propiedades del logaritmo, el Teorema de Fubini y la definición de función de densidad condicionada.
  
  \begin{align*}
    I(X;Y)=& \int \int  \log\left( \frac{f(x,y)}{f(x)f(y)}\right)f(x,y)dxdy \\[3pt]
    =& - \int\int \log \left(f(x)\right)f(x,y)  dx dy\\ &+ \int\int\log \left(\frac{f(x,y)}{f(y)}\right)  f(x,y)dx dy\\[3pt]
    =& - \int  \log \left(f(x)\right)f(x) dx + \int\int \log \left(f(x|y)\right)f(x,y) dx dy\\  
    =& h(X) - h(X|Y).
  \end{align*}
De manera análoga se obtiene la segunda igualdad.\\
\end{proof}

\begin{definition}[Variable aleatoria cuantificada]
  Sea $X$ una variable aleatoria con función de densidad $f$. Si dividimos la imagen de $X$ en intervalos de longitud $\epsilon$ y asumimos que la densidad es continua en cada intervalo, el Teorema del valor medio nos dice que, para cada intervalo $\left[i\epsilon, (i+1)\epsilon\right]$ existe un valor $x_i\in \left[i\epsilon, (i+1)\epsilon\right]$ tal que \[f(x_i)\epsilon = \int_{i\epsilon}^{(i+1)\epsilon}f(x)dx.\]
  
  La variable aleatoria cuantificada, $X^{\epsilon}$, se define de la siguiente forma\[
X^{\epsilon} = x_i, \quad \text{ si } i\epsilon \leq X  < (i+1)\epsilon.
\]Su función masa de probabilidad viene dada por \[
P[X^\epsilon = x_i] = p_i =  \int_{i\epsilon}^{(i+1)\epsilon}f(x)dx = f(x_i)\epsilon.
\]\\[-10pt]
\end{definition}

\begin{theorem}
  Si la densidad $f$ de una variable aleatoria $X$ es integrable Riemann, entonces\[
H(X^{\varepsilon}) + \log \epsilon \xrightarrow[]{\epsilon \to 0}  h(X).\\
  \] 
\end{theorem}
\begin{proof}
  Partimos de la entropía de $X^{\epsilon}$,
  \begin{align*}
    H(X^{\epsilon}) &= - \sum_{-\infty}^{+\infty} p_i \log p_i = - \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i)\epsilon \right )\\
    &= -  \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right ) - \underbrace{\sum_{-\infty}^{+\infty}f(x_i)\epsilon}_{\int_{-\infty}^{+\infty}f(x)dx=1} \log\left ( \epsilon \right ),
  \end{align*}
  luego, tenemos que $H(X^{\epsilon}) + \log \epsilon  = \sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right )$.\\

  Como $f$ es integrable Riemann, tomando $\epsilon \to 0$ obtenemos \[\lim_{\epsilon \to 0}\sum_{-\infty}^{+\infty}f(x_i)\epsilon \log\left ( f(x_i) \right )  = - \int_{-\infty}^{+\infty}\log\left( f(x)\right) f(x)dx = h(X).\]

  Por tanto,\[
H(X^{\varepsilon}) + \log \epsilon \xrightarrow[]{\epsilon \to 0}  h(X).
  \] \\[-10pt]
\end{proof}

Teniendo en cuenta el resultado anterior podemos aproximar la información mutua de dos variables aleatorias por la de sus variables cuantificadas.
\begin{align}\label{eq:imq}
  I(X^{\epsilon};Y^{\epsilon}) &= H(X^{\epsilon}) - H(X^{\epsilon}|Y^{\epsilon})\nonumber\\&\approx h(X) - \log\epsilon - (h(X|Y) - \log\epsilon) = I(X;Y).
\end{align}

De forma más general, podemos definir la información mutua en términos de particiones finitas de la imagen de la variable aleatoria. Sea $\mathcal{X}$ la imagen de la variable aleatoria $X$. Una partición $\mathcal{P}$ de $\mathcal{X}$ es una colección finita de conjuntos disjuntos $P_i$ de forma que $\cup_i P_i = \mathcal{X}$. La cuantificación de $X$ dada la partición $\mathcal{P}$, que notaremos $[X]_{\mathcal{P}}$, es la variable aleatoria discreta con función masa de probabilidad\[
P\left[[X]_{\mathcal{P}} = i\right ] = P [X\in P_i] = \int_{P_i}dF(x).
\]
Utilizando esto para dos variables aleatorias con sendas particiones, podremos calcular la información mutua de las variables cuantificadas utilizando (\ref{eq:imq}). De esta forma, obtenemos una nueva definición de la información mutua.\\

\begin{definition}[Información mutua]
  La información mutua de dos variables aleatorias $X$ e $Y$ viene dada por \[
I(X;Y) = \sup_{\mathcal{P},\mathcal{Q}}I\left ( [X]_{\mathcal{P}}, [Y]_{\mathcal{Q}} \right),
  \]donde el supremo es sobre todas las particiones finitas $\mathcal{P}$ y $\mathcal{Q}$.\\
\end{definition}
Notamos que esta definición también será válida cuando las variables aleatorias no tengan definida su función de densidad. Esta definición es equivalente a la Definición \ref{def:im_disc} para variables aleatorias discretas y a la Definición \ref{def:im_cont} para variables aleatorias continuas.\\

\begin{theorem}Si $f$ y $g$ son dos funciones de densidad, entonces\[
D\left ( f || g \right ) \ge 0
\]
con igualdad si, y solo si, $f = g$ en casi todo punto.
\end{theorem}
\begin{proof}
Sea $sop(f)$ el soporte de $f$, entonces 
\begin{align*}
  - D\left ( f || g \right ) &= \int_{sop(f)} \log \left(\frac{g(x)}{f(x)}\right ) f(x) dx  \leq \log \left( \int_{sop(f)} f(x)\cdot \frac{g(x)}{f(x)}dx \right)\\
  &= \log \left ( \int_{sop(f)}g(x)dx\right) \leq 1 = 0.
\end{align*}
La primera desigualdad es la desigualdad de Jensen, Teorema \ref{th:jensen}, aplicada a $\log$, una función cóncava que no es afín. Por ello, será igualdad si, y solo si, $\log \frac{g}{f} = 0$ para casi todo punto, esto es, $f=g$ en casi todo punto, la misma condición para que la segunda desigualdad sea igualdad.\\
\end{proof}

\begin{corollary}\label{c:im_indc}
$I(X;Y)\ge 0$ con igualdad si, y solo si, $X$ e $Y$ son independientes.\\
\end{corollary}

\begin{corollary}
$h(X|Y) \le 0$ con igualdad si, y solo si, $X$ e $Y$ son independientes.\\
\end{corollary}

\begin{theorem}[Regla de la cadena para la entropía diferencial] Sean $X_1,\dots, X_n$ variables aleatorias, entonces
  \[
h(X_1,\dots,X_n) = \sum_{i=1}^n h(X_i|X_1,\dots, X_{i-1}).
  \]
\end{theorem}

\begin{proof}
  Realizaremos la prueba por inducción sobre el número de variables.
  \begin{itemize}
  \item Si $n=2$, entonces $h(X_1,X_2) = h(X_1) + h(X_2|X_1)$ por definición de la entropía diferencial condicionada.
  \item Suponiendo cierto el caso $n-1$, veamos que también se cumple para $n$, utilizando otra vez la definición de entropía diferencial condicionada:
    \begin{align*}
h(X_1,\dots,X_n) &= h(X_n|X_1,\dots, X_{n-1}) + h(X_1,\dots, X_{n-1})\\ &= h(X_n|X_1,\dots,X_{n-1}) + \sum_{i=1}^{n-1}h(X_i|X_1,\dots,X_{i-1}).
    \end{align*}
  \end{itemize}
\end{proof}

\begin{corollary} Se tiene que
  \[h(X_1,\dots, X_n) \leq \sum_i h(X_i),\]
  dándose la igualdad si, y solo si, $X_1,\dots, X_n$ son independientes.\\
\end{corollary}
% EN la prueba se podría usar el leman 2.21 para ver que h(X) >= h(X|Y)

\begin{theorem}
  Sea $X$ una variable aleatoria absolutamente continua y $c$ un vector de escalares, entonces
  \[
  h(X+c) = h(X).
  \]
  La entropía diferencial no cambia por traslaciones.
\end{theorem}

\begin{proof}
  Sea $f_X$ la función de densidad de $X$. Llamemos $Y = X+ c$, esta será una variable aleatoria cuya función de densidad se relaciona con la de $X$ como sigue: $f_Y(y)=f_X(y-c)$. Calculamos su entropía diferencial usando su definición y aplicando el cambio de variable $y = x + c$.
  \begin{align*}
    h(X+c) &= h(Y) = - \int \log \left(f_Y(y)\right)f_Y(y) dy\\
    &= - \int  \log \left(f_X(y-c)\right) f_X(y-c) dy\\
    &= - \int  \log \left(f_X(x)\right) f_X(x) dx = h(X).
  \end{align*}
\end{proof}

\begin{theorem}
  Sea $X$ una variable aleatoria unidimensional y $a$ un escalar no nulo, entonces\[
h(aX) = h(X) + \log |a|.
  \]
\end{theorem}

\begin{proof}
  Llamemos $Y = aX$, entonces $f_Y(y) = \frac{1}{|a|}f_X\left (\frac{y}{a} \right )$. Usaremos la definición de entropía, la función de densidad recién calculada y un cambio de variables ($x = \frac{y}{a}$):
  \begin{align*}
    h(aX) &= h(Y) = -\int \log \left(f_Y(y)\right) f_Y(y)dy\\[3pt]
    &=  -\int \log \left ( \frac{1}{|a|}f_X\left (\frac{y}{a} \right ) \right ) \frac{1}{|a|}f_X\left (\frac{y}{a} \right ) dy\\[3pt]
    &= -\int  \log \left ( \frac{1}{|a|}f_X\left (x \right ) \right )\frac{|a|}{|a|}f_X\left (x \right ) dx\\[3pt]
    &= -\int \log \left (f_X\left (x \right ) \right ) f_X\left (x \right ) dx + \int\log\left({|a|}\right) f_X(x)dx\\[3pt]
    &= h(X) + \log{|a|}.
  \end{align*}
\end{proof}

\begin{corollary}
  Si $X$ es una variable aleatoria multidimensional y $A$ una matriz entonces,\[
h(AX) = h(X) + \log|det(A)|.
  \]\\[-10pt]
\end{corollary}


\section{Estimación de la entropía y de la información mutua}

Generalmente no conocemos las funciones de densidad de las variables aleatorias cuya entropía o información mutua queremos calcular, por ello, tratamos de aproximar este valor a partir de información que sí conocemos, como realizaciones muestrales de las variables. El objetivo es, para dos variables aleatorias $X,Y$ ($d$-dimensionales) y la variable aleatoria conjunta $Z = (X,Y)$, estimar $h(X)$ o $I(X;Y)$ a partir de un conjunto de realizaciones muestrales, $\{z_i=(x_i,y_i) : i = 1,\dots, N\}$, sin conocer las funciones de densidad $f_z, f_x$ y $f_y$.  Asumiremos en el desarrollo a continuación que todas las funciones de densidad son propias, es decir, que no son idénticamente $-\infty$.\\

Presentaremos brevemente un estimador basado en particiones antes de estudiar estimadores basados en los $k$ vecinos más cercanos y del tipo de Kozachenko - Leonenko.\\

\subsection{Estimador basado en particiones}

En este enfoque, dividiremos el soporte de $X$ e $Y$ en particiones de tamaño finito (no necesariamente el mismo) y aproximamos la información mutua por \[
I(X,Y) \approx I_{\textit{binned}}(X,Y) = \sum_{i,j} p(i,j) \log \frac{p(i,j)}{p_x(i)p_y(j)},
\]
donde $p_x(i) = \int_i f_x(x)dx$, $p_y(j) = \int_j f_y(y)dy$, $\int_i$ representa la integral sobre la $i$-ésima partición, y $p(i,j) = \int_i\int_jf(x,y)dxdy$.\\

Llamamos $n_x(i)$ al número de puntos en la i-ésima partición de $X$, análogamente $n_y(j)$. $n(i,j)$ es el número de puntos en la intersección de $n_x(i)$ y $n_y(j)$.\\

Aproximamos \begin{align*}
  p_x(i) &\approx \frac{n_x(i)}{N},\\
  p_y(j) &\approx \frac{n_y(j)}{N},\\
  p(i,j) &\approx \frac{n(i,j)}{N}.\\[-10pt]
  \end{align*}

Cuando $N$ tienda a infinito y el tamaño de cada partición converja a cero, la estimación $I_{\textit{binned}}(X,Y) \to I(X;Y)$, véase el Teorema 1 de \cite{paninski}.\\


\subsection{Estimadores de la entropía basados en los $k$ vecinos más cercanos}

Introducimos a continuación estimadores de la entropía, $h(X)$, basados en los $k$ vecinos más cercanos. Para ello, consideramos $X$ una variable aleatoria $d$-dimensional, con función de densidad $f(x)$. Queremos estimar la entropía\[
h(X) = - \int \log\left(f(x)\right)f(x)dx
\]
a partir de una muestra aleatoria simple $X_1,\dots,X_N \text{ de } X$.\\

Teniendo en cuenta el Teorema \ref{t:mmlog}, si tenemos un estimador consistente de $\log f(X_i)$, que notaremos por $\widehat{\log f}(X_i)$, podemos estimar $h(X)$ mediante\[
-\frac{1}{N}\sum_{i=1}^N\widehat{\log f}(X_i).
\]\\[-10pt]

Para obtener el estimador $\widehat{\log f}(X)$, partimos de los estimadores de la función de densidad $\hat{f}(X)$ basados en los $k$ vecinos más cercanos.\\

Sea $X_1,\dots,X_N$ una muestra aleatoria simple y $x_1,\dots, x_N$ una observación de dicha muestra. Consideramos en $\mathbb{R}^d$, la métrica inducida por una norma $\Vert \cdot \Vert$ (euclídea, del máximo,...). Fijado $x_i\in \mathbb{R}^d$, ordenamos el resto de observaciones de la muestra en orden creciente de la distancia a este punto y llamamos $\epsilon_k(x_i)$ a la distancia de $x_i$ a su $k$ vecino más próximo, que notamos por $x_{i_k}$.\\

Consideramos la bola de centro $x_i$ y radio $\epsilon_k(x_i)$,\[
B(x_i, \epsilon_k(x_i)) = \{y \in \R^d: \Vert x_i - y\Vert \leq \epsilon_k(x_i)\}.\]\\[-15pt]

Comenzaremos aproximando, para cualquier $j=1,\dots, N$,\[
P[X_j\in B(x_i,\epsilon_k(x_i))] \approx \int_{B(x_i,\epsilon_k(x_i))}f(y)dy.
\]

Podemos ver \[
\frac{1}{N} \sum_{j=1}^NI\left(X_j\in B(x_i,\epsilon_k(x_i))\right),\]donde $I$ es la función indicadora, como un estimador de la distribución $P[X_j\in B(x_i,\epsilon_k(x_i))]$. Por la definición de $\epsilon_k(x_i)$ tenemos que \begin{align}\label{eq:knn-ind}
 \frac{1}{N} \sum_{j=1}^NI\left(X_j\in B\left(x_i,\epsilon_k(x_i)\right)\right) = \frac{k}{N}.
\end{align}

Cuando $N$ es grande y $k$ es relativamente pequeño en comparación, $\epsilon_k(x_i)$ será pequeño porque $\frac{k}{N}$ lo es. Luego, la densidad $f(y)$ en la bola $B(x_i,\epsilon_k(x_i))$ no cambiará demasiado. Para cualquier punto de la bola consideraremos que su densidad coincide con la del centro de la misma, $f(y)\approx f(x_i)$. Tenemos entonces\begin{align*}
P[X_j\in B(x_i,\epsilon_k(x_i))] &\approx \int_{B(x_i,\epsilon_k(x_i))}f(y)dy\approx f(x_i) \int_{B(x_i,\epsilon_k(x_i))}dy\\[3pt] &= f(x_i)\cdot V_d \cdot \epsilon^d_k(x_i),
\end{align*}
donde $V_d\cdot \epsilon^d_k(x_i)$ es el volumen de la bola $d$ dimensional de radio $\epsilon_k(x_i)$ (este volumen depende la norma utilizada).\\

Utilizando (\ref{eq:knn-ind}) para estimar $P[X_j\in B(x_i,\epsilon_k(x_i))]$, tendremos,\begin{align*}
f(x_i)\cdot V_d \cdot \epsilon_k(x_i) &\approx P[X_j\in B(x_i,\epsilon_k(x_i))]\approx  \frac{1}{N} \sum_{j=1}^NI\left(X_j\in B\left(x_i,\epsilon_k(x_i)\right)\right)\\ &= \frac{k}{N}.
\end{align*}

Tomando\[
f(x_i)\cdot V_d \cdot \epsilon^d_k(x_i) \approx \frac{k}{N}
\]
se obtiene el estimador de la función de densidad,
\[
\hat{f}(x) = \frac{k}{N}\cdot\frac{1}{V_d \cdot \epsilon^d_k(x)},\quad \forall x\in \R^d.
\]\\[-10pt]

El comportamiento asintótico de este estimador no depende solo del tamaño de la muestra sino también de la dimensión $d$ y del valor $k$ elegido. De hecho, la varianza depende exclusivamente del valor de $k$, la variabilidad del estimador viene dada por el número de vecinos más cercanos a considerar. Este estimador es sesgado, aunque es asintóticamente insesgado y $\Var\left (\hat{f}\right)\to 0$ solo si $k\to \infty, N \to \infty$ y $\frac{k}{N} \to 0$, véase el corolario 2 de \cite{inproceedings} para la demostración general o \cite{stat} para el desarrollo del caso $d=1$. Aumentar el valor de $k$ incrementa el tiempo de cómputo notablemente, dificultando el cálculo efectivo.\\

Como habíamos expuesto, utilizaremos este estimador de la función de densidad de $X$ para obtener un estimador de $\log f(X)$, este es,\[
\widehat{\log f}(x_i) = \log\left ( \frac{k}{N}\cdot\frac{1}{V_d \cdot \epsilon^d_k(x_i)} \right ) = \log\frac{k}{N} - \log V_d - d\log  \epsilon_k(x_i).
\]\\[-10pt]

Obteniendo el estimador de la entropía
\begin{align*}
\hat{h}(x) &= -\frac{1}{N} \sum_{i=1}^N \widehat{\log f}(x_i)\\
  &= - \log\frac{k}{N} + \log V_d +  \frac{d}{N} \sum_{i=1}^N\log  \epsilon_k(x_i).
\end{align*}

Como proviene del estimador $\hat{f}$, el estimador $\hat{h}$ mantiene sus propiedades asintóticas. Para corregir en parte el sesgo del estimador y para solucionar los problemas de cómputo de este estimador, se propone la modificación siguiente, basada en el estimador de Kozachenko - Leonenko. \\

\subsection{Estimadores de la entropía del tipo de Kozachenko - Leonenko}

Consideramos $X_1,\dots, X_n$ una muestra aleatoria simple de la variable aleatoria $X$ con función de densidad $f$. Partiendo del mismo razonamiento, trataremos de estimar la entropía, $h(X)$, a partir de \[
\widehat{h}(X) = -\frac{1}{N}\sum_{i=1}^N\widehat{\log f}(X_i).\]

La diferencia vendrá marcada por la forma de estimar $\widehat{\log f}(X_i)$, para corregir el sesgo del estimador anterior.\\

Siguiendo con la notación anterior, llamamos $\mathcal{E}_{i_k}$ a la variable aleatoria $\Vert X_i - X_{i_k} \Vert$ y $P_k(\epsilon)$ a su función de densidad.\\

Para obtener el estimador $\widehat{\log f}(X_i)$, para un $k$ fijo, consideramos la función de densidad, $P_k(\varepsilon)$, de la distancia entre $X_i$ y su $k$-ésimo vecino más cercano, donde $\epsilon$ será una realización de $\mathcal{E}_{i_k}$.\\

Partimos de una observación $x_1,\dots, x_n$, de la que seleccionamos un punto $x_i$ cualquiera. Vamos a calcular $P\left[\epsilon \leq \mathcal{E}_{i_k} \leq \epsilon + h \right]$. Esta probabilidad es igual a la probabilidad de que haya un punto a distancia $r\in \left[ \varepsilon,\varepsilon + h \right ]$ de $x_i$, de que haya otras $k-1$ observaciones a distancia menor que $\epsilon$; y de que el resto de puntos se encuentren a mayor distancia de $x_i$.\\

Llamamos \[
p_i(\varepsilon) = \int_{B(x_i,\epsilon)}f(y)dy.
\]\\[-10pt]

Para cada $j = 1,\dots,n$ aproximaremos\[
P\left[X_j \in B(x_i,\epsilon)\right]\approx p_i(\epsilon),
\]donde $B(x_i,\epsilon)$ es la bola centrada en $x_i$ y de radio $\varepsilon$.\\

Usaremos la fórmula de la distribución multinomial para expresar $P_k(\varepsilon)$,
\begin{align*}
  P_k(\epsilon) = \lim_{h\to 0}&\frac{P\left[\epsilon \leq \mathcal{E}_{i_k} \leq \epsilon + h\right]}{h}\\[5pt] = \lim_{h\to 0}& \left [\frac{(N-1)!}{1!(k-1)!(N-k-1)!}  \cdot p_i(\epsilon)^{k-1} \cdot \left(1-p_i(\epsilon)\right)^{N-k-1} \right.\\[3pt] &\left.\left(\frac{p_i(\epsilon + h ) - p_i(\epsilon)}{h}\right)\right ]\\[5pt] = \ \  k\ \ &\binom{N-1}{k} \cdot \frac{d p_i(\varepsilon)}{d\varepsilon} \cdot p_i(\epsilon)^{k-1} \cdot \left(1-p_i(\epsilon)\right)^{N-k-1}.\\[-5pt]
\end{align*}

Observamos que \[
\int_0^{\infty} P_k(\varepsilon)d\varepsilon = k\binom{N-1}{k} \int_0^1 p^{k-1}(1-p)^{N-k-1}dp = 1,
\]que debe cumplir para ser función de densidad.\\

Podemos calcular la esperanza de $\log p_i (\varepsilon)$, a partir de la posición de los $N-1$ puntos restantes, con $x_i$ fijo:
\begin{align*}
\mathbb{E} \left[\log p_i(\epsilon)\right ] &= \int_0^\infty P_k(\varepsilon) \log p_i(\varepsilon) d\varepsilon\\ &=  k\binom{N-1}{k} \int_0^1 p^{k-1}(1-p)^{N-k-1}\log p\ dp\\[5pt] &= \psi (k) - \psi(N),
\end{align*}

donde $\psi$ es la función digamma, esta expresión se ha obtenido de \cite{kraskov}.\\ 

Si asumimos que $f(x)$ es constante en la bola de radio $\epsilon$, tendremos \begin{align*}
p_i(\epsilon) &\approx \int_{B(x_i,\epsilon)}f(y)dy\approx f(x_i) \int_{B(x_i,\epsilon)}dy\\[5pt] &= f(x_i)\cdot V_d \cdot \epsilon^d.\\[-5pt]
\end{align*}

Tomando logaritmos sobre la aproximación de $p_i(\varepsilon)$, obtenemos:\[
\log p_i(\varepsilon) \approx \log f(x_i) + \log V_d + d \log \varepsilon, 
\]
despejando $\log f(x_i)$,\[
\log f(x_i) \approx  \log p_i(\varepsilon) - \log V_d - d \log \varepsilon.
\]

Continuamos tomando esperanzas respecto de $\varepsilon$ y sustituyendo la esperanza del logaritmo ya calculada, notamos que $\log f(X_i)$ no depende de $\epsilon$:\begin{align*}
\log f(x_i) &\approx \mathbb{E}\left[\log p_i(\epsilon)\right] - \log V_d  - d \mathbb{E} \left[\log \varepsilon\right]\\[2pt] &= \psi (k) - \psi (N) - \log V_d  - d \mathbb{E} \left[\log \varepsilon\right].\\[-5pt]
\end{align*}

Hemos calculado un estimador de $\widehat{\log f} (X_i)$ que nos permite obtener un estimador de la entropía:
\begin{align}
  \widehat{h}(X) &= - \frac{1}{N} \sum_{i=1}^N\widehat{\log f} (X_i) = - \frac{1}{N} \sum_{i=1}^N\left (\psi (k) - \psi (N) - \log V_d  - d\log \varepsilon \right ) \nonumber \\
  &= - \psi (k) + \psi (N) + \log V_d + \frac{d}{N} \sum_{i=1}^N \log \varepsilon (x_i). \label{eq:est_ent}
\end{align}\\[-5pt]

El estimador anterior es el de Kozachenko - Leonenko en el caso $k=1$ \cite{kozachenko-leonenko}.\\

La forma de estimar $\widehat{\log f} (X_i)$ elimina una parte del sesgo, aunque se mantiene el sesgo proveniente de aproximar\[\int_{B(x_i,\epsilon)}f(y)dy\approx f(x_i) \int_{B(x_i,\epsilon)}dy.\] Si la densidad $f(x)$ es constante, el sesgo proveniente de esta aproximación también desaparece y obtenemos un estimador de la entropía, $\widehat{h}(X)$, que es insesgado.

\subsection{Estimadores de la información mutua del tipo de Kozachenko - Leonenko}

Extendemos las ideas anteriores para estimar la información mutua.\\

Consideremos ahora la variable aleatoria conjunta $Z=(X,Y)$ con la norma del máximo\[ \Vert Z - Z' \Vert = \max \{\Vert X-X' \Vert, \Vert Y-Y'\Vert\},\] donde las normas en $X$ e $Y$ pueden ser cualesquiera, no necesariamente la misma. \\

Para un $k$ fijo, tomamos uno de los $N$ puntos de la muestra, $z_i$, y notamos $\varepsilon_k(z_i)$ la distancia de este punto a su $k$-ésimo vecino más cercano, esta será una realización de $\mathcal{E}_{i_k}$. $\epsilon_k(x_i)$ y $\epsilon_k(y_i)$ reflejan la distancia de ese punto proyectada en los subespacios $X$ e $Y$ respectivamente. Por la norma elegida, se verifica \[\varepsilon_k(z_i) = \max \{\varepsilon_k(x_i), \varepsilon_k(y_i)\}.\]

Distinguiremos dos algoritmos diferentes para estimar la información mutua según si consideramos los vecinos que se encuentren en el cuadrado de lado $2\varepsilon(z_i)$, o en el rectángulo de lados $2\varepsilon(x_i)$, $2\varepsilon(y_i)$. 

\subsubsection{Primer algoritmo}

Este algoritmo se caracteriza por definir $n_x(i)$ como el número de puntos $x_j$ cuya distancia a $x_i$ es menor que $\varepsilon_k(z_i)$, análogamente se define $n_y(i)$. $\varepsilon_k(z_i)$ es una observación de una variable aleatoria con densidad dada por $P_k(\varepsilon_k(z_i))$, luego $\mathbb{E}\left[\log p_i\left(\varepsilon_k(z_i)\right)\right] = \psi(k) - \psi(N)$ se cumple también en este caso.\\

Para utilizar el estimador de la entropía calculado anteriormente debemos cambiar\!\! : $x_i$ por $z_i=(x_i,y_i)$, $d$ por $d_X + d_Y$ y $V_d$ por $V_{d_X}V_{d_Y}$. Con estas modificaciones obtenemos\[
\widehat{h}(X,Y) = - \psi(k) + \psi(N) + \log (V_{d_X}V_{d_Y}) + \frac{d_X+d_Y}{N} \sum_{i=1}^N\log\left(\varepsilon_k(z_i)\right).
\]

Para obtener una estimación de $I(X;Y)$ utilizaremos el Lema \ref{l:ent_im},\[
\hat{I}(X;Y) = \hat{h}(X) + \hat{h}(Y) - \hat{h}(X,Y).
\]\\[-15pt]

Podríamos usar el mismo valor de $k$ para realizar estas estimaciones, pero entonces estaríamos utilizando diferentes escalas en el espacio conjunto y en los marginales. Para un $k$ fijo, la distancia al $k$-ésimo vecino en el espacio conjunto sería mayor que las distancias a los vecinos en los espacios marginales. El sesgo en el estimador de la entropía proveniente de la no uniformidad de la densidad depende de estas distancias, los sesgos en $\widehat{h}(X)$, $\widehat{h}(Y)$, $\widehat{h}(X,Y)$ no se cancelarían. Para evitar eso, notamos que la estimación se cumple para cualquier valor de $k$ y que no tenemos por qué mantenerlo fijo.\\

Suponemos que el $k$-ésimo vecino de $x_i$ se encuentra en uno de los lados verticales del cuadrado de lado $2\varepsilon_k(z_i)$. Entre las rectas verticales $x_i - \varepsilon_k(z_i)$ y $x_i + \varepsilon_k(z_i)$ hay $n_x(i)$ puntos, por definición de $n_x(i)$, luego, $\varepsilon_k(z_i)$ coincide con la distancia entre $x_i$ y su $(n_x(i) + 1)$-ésimo vecino más cercano. Tomando $k = n_x(i)+1$, estimamos la entropía de la variable $X$ mediante\[
\widehat{h}(X) = - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log V_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon_k(z_i).
\]

Para la variable $Y$, $\varepsilon_k(z_i)$ no tendría por qué coincidir con la distancia entre $y_i$ y su $(n_y(i)+1)$-ésimo vecino. Sin embargo, consideramos la ecuación anterior una buena aproximación para $h(Y)$ (cambiando $X$ por $Y$). Esta aproximación será exacta cuando $n_y(i) \to \infty$ y cuando $N\to \infty$ \cite{singh_finite-sample_2016}.\\ 

Usando estas estimaciones en la ecuación correspondiente y notando $\epsilon(i) = \varepsilon_k(z_i)$, obtenemos:
\begin{align*}
  I^{(1)}(X,Y) \approx& \ \widehat{h}(X) + \widehat{h}(Y) - \widehat{h}(X, Y)\\ =& - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)+1) + \psi(N) + \log V_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log (\varepsilon(i))\\
  & - \frac{1}{N}\sum_{i=1}^N\psi(n_y(i)+1) + \psi(N) + \log V_{d_Y} + \frac{d_Y}{N} \sum_{i=1}^N\log (\varepsilon(i))\\
  & + \psi(k) - \psi(N) - \log (V_{d_X}V_{d_Y}) - \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i))\\
  =& \ \psi(k) - \frac{1}{N}\sum_{i=1}^N\left( \psi(n_x(i)+1) + \psi(n_y(i)+1) \right ) + \psi(N).\\[-5pt]
\end{align*}

Notamos $\langle \dots \rangle$ a la media sobre todos los $i\in\{1,\dots,N\}$ y sobre todas las realizaciones de las muestras aleatorias,\[
\langle \dots \rangle = \frac{1}{N} \sum_{i=1}^N\mathbb{E}[\dots(i)].
\]

Utilizando esta notación la estimación de la información mutua quedaría:
\begin{align}
I^{(1)}(X,Y) \approx \psi(k) - \langle \psi(n_x+1) + \psi(n_y+1) \rangle + \psi(N).\label{eq:est1}
\end{align}\\[-10pt]

El principal inconveniente de esta estimación es que solo usamos el estimador de la entropía correctamente en una dirección marginal. Esto parece inevitable si queremos usar (hiper)cubos en el espacio conjunto, como opción alternativa podríamos usar (hiper)rectángulos.\\

\subsubsection{Segundo algoritmo}

En este caso, $n_x(i)$ y $n_y(i)$ serán el número de puntos con $\Vert x_i - x_j \Vert \leq \varepsilon_k(x_i)$ y $\Vert y_i - y_j \Vert \leq \varepsilon_k(y_i)$ respectivamente. Seguimos notando $\varepsilon(i) = \max \{\varepsilon_k(X_i), \varepsilon_k(Y_i)\}$. Tenemos que distinguir dos situaciones:

\begin{enumerate}[label={\alph*)}]
\item Los lados $\varepsilon_k(x_i)$ y $\varepsilon_k(y_i)$ quedan determinados por el mismo punto, lo denotamos suceso $A$.
\item Los lados $\varepsilon_k(x_i)$ y $\varepsilon_k(y_i)$ quedan determinados por puntos diferentes, lo llamamos suceso $B$.
\end{enumerate}

En ambos casos tenemos que cambiar $P_k(\varepsilon)$ por una probabilidad 2-dimensional:
\begin{align*}
  P_k(\varepsilon_x, \varepsilon_y) &= P_k^{a)}(\varepsilon_x, \varepsilon_y) + P_k^{b)}(\varepsilon_x, \varepsilon_y)\\[3pt] &= P_k\left (\epsilon_x, \epsilon_y | A \right) \cdot P (A) + P_k\left (\epsilon_x, \epsilon_y | B \right) \cdot P(B)\\
  &= P_k\left (\epsilon_x, \epsilon_y | A \right) \cdot \frac{1}{k} + P_k\left (\epsilon_x, \epsilon_y | B \right) \cdot \frac{k-1}{k}.\\[-5pt]
\end{align*}

En el artículo \cite{kraskov} se especifican los valores de estas probabilidades, necesarias para llegar a la siguiente expresión sobre $q_i \equiv q_i(\varepsilon_k(X_i), \varepsilon_k(Y_i))$, la función de densidad del rectángulo de lados $2\varepsilon_k(X_i) \times 2\varepsilon_k(Y_i)$ centrado en el punto $(X_i, Y_i)$,
\[
\mathbb{E}(\log q_i) = \int \int_0^{\infty} P_k(\varepsilon_x, \varepsilon_y) \log q_i(\varepsilon_x, \varepsilon_y) d \varepsilon_x d\varepsilon_y
= \psi(k) - \frac{1}{k} - \psi(N).
\]\\[-10pt]

Aproximamos $\log f(x_i, y_i)$ por \[
\log f(x_i, y_i) \approx  \log q_i(\varepsilon(i)) - \log V_d - d \log \varepsilon(i),
\]

tomando esperanzas, obtenemos \[
\log f(x_i, y_i) \approx  \psi(k) - \frac{1}{k} - \psi(N) - \log V_d - d \E\left [ \log \varepsilon(i) \right].
\]\\[-10pt]

Esto nos da el siguiente estimador de la entropía
 \begin{align*}
  \widehat{h}(X, Y) &= - \frac{1}{N} \sum_{i=1}^N\widehat{\log f} (X_i, Y_i)\\ &= - \frac{1}{N} \sum_{i=1}^N\left (\psi (k) - \frac{1}{k} - \psi (N) - \log V_d  - d\log \varepsilon(i) \right ) \\
  &= - \psi (k) + \frac{1}{k}+ \psi (N) + \log V_d + \frac{d}{N} \sum_{i=1}^N \log \varepsilon (i). 
 \end{align*}

 Como ya hicimos en el primer algoritmo, utilizaremos el Lema \ref{l:ent_im} para estimar la información mutua, sustituyendo la entropía conjunta por el estimador recién conseguido y las individuales por el estimador obtenido en el caso anterior, notando que cambia el significado de $n_x$ y $n_y$.
\begin{align*}
  I^{(2)}(X,Y) \approx& \ \widehat{h}(X) + \widehat{h}(Y) - \widehat{h}(X, Y)\\ =& - \frac{1}{N}\sum_{i=1}^N\psi(n_x(i)) + \psi(N) + \log V_{d_X} + \frac{d_X}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & - \frac{1}{N}\sum_{i=1}^N\psi(n_y(i))  + \psi(N) + \log V_{d_Y} + \frac{d_Y}{N} \sum_{i=1}^N\log \varepsilon(i)\\
  & + \psi(k) - \frac{1}{k} - \psi(N) - \log (V_{d_X}V_{d_Y}) - \frac{d_X+d_Y}{N} \sum_{i=1}^N\log(\varepsilon(i))\\
  =& \ \psi(k)  - \frac{1}{k}- \frac{1}{N}\sum_{i=1}^N\left( \psi(n_x(i)) + \psi(n_y(i)) \right ) + \psi(N).\\
\end{align*}

La información mutua se aproximaría por
\begin{align}
I^{(2)}(X,Y) \approx \psi(k) - \frac{1}{k} - \langle \psi(n_x) + \psi(n_y) \rangle + \psi(N). \label{eq:est2}
\end{align}\\[-10pt]


En general ambas fórmulas dan resultados similares. Para un mismo $k$, $I^{(1)}$ da menores errores estadísticos (porque $n_x(i)$ y $n_y(i)$ tienden a ser más grandes y tener menores variaciones relativas), pero tiene mayores errores sistemáticos. Esto último es grave solo si estamos interesados en variables con dimensiones muy altas, donde $\varepsilon(i)$ es habitualmente más grande que la marginal $\varepsilon_x(i)$. En ese caso, parece preferible el segundo algoritmo, en caso contrario es indiferente usar uno u otro.\\

\section{Implementaciones de la entropía y de la información mutua}

En esta sección estudiaremos dos implementaciones en Python de conceptos anteriores como la entropía o la estimación mutua. En general, dado un número finito de muestras de una variable aleatoria no podemos calcular de forma exacta la entropía o información mutua, ya que no conocemos su distribución. Para implementarlas es necesario usar algún estimador, como los estudiados en el apartado anterior.

\subsection{Implementación 1 - GaelVaroquaux}

Comenzaremos revisando la implementación encontrada en \cite{estimating}. Esta implementación fue desarrollada por Gaël Varoquaux \cite{gael-varoquaux}, un investigador de la facultad de Inria que trabaja en el ámbito de la ciencia de datos, contribuye, entre otros proyectos, en las bibliotecas \texttt{scikit-learn} y joblib. En esta implementación hace uso de la biblioteca \texttt{scikit-learn} para calcular estimadores de la entropía y la información mutua. Para revisar esta implementación analizaremos el código y nos haremos una idea de qué hace y cómo cada una de las funciones.

\begin{itemize}
\item \texttt{nearest\_distances(X, k)}: para cada punto de un vector \texttt{X}, devuelve la distancia a su $k$-ésimo vecino más cercano. Se implementa haciendo uso de las funciones de la clase \texttt{NearestNeighbors} de \texttt{scikit-learn} \cite{nearestneighbors}.

\item \texttt{entropy\_gaussian(C)}: entropía de una variable gaussiana, se utilizará para hacer pruebas. Si la matriz de covarianzas \texttt{C} es escalar, se utiliza la siguiente fórmula para la entropía:\[
  h(X) = \frac{1}{2}\left ( 1 + \log(2\pi) \right )+ \frac{1}{2} \log C.
  \]
Vimos en el ejemplo \ref{ej:norm_uni} que la entropía de la gaussiana con covarianza $\sigma^2$ es:\[
h(X) = \frac{1}{2}\log (2\pi e \sigma^2 ),
\]
coincidiendo con la implementación.

Para el caso en que la matriz \texttt{C} sea no degenerada, utiliza la expresión:\[
h(X_1,\dots, X_n) = \frac{n}{2} \left (1 + \log(2\pi) \right) + \frac{1}{2}\log |C|,
\]

Coincidiendo con la fórmula vista en el ejemplo \ref{ej:norm_multi}.

\item \texttt{entropy(X, k)}: calcula la entropía de la variable \texttt{X} usando el estimador de tipo de Kozachenko - Leonenko visto en  (\ref{eq:est_ent}). Comienza inicializando los elementos que se necesitan para aplicar la fórmula: vecinos más cercanos para todos los elementos del vector, dimensiones de \texttt{X}, volumen de la bola unidad ($V_d$). Notamos que en el cálculo de la constante $V_d$ (\texttt{volume\_unit\_ball}) se utiliza el volumen dado por la distancia euclídea. Donde en el estimador estudiado ponía $\varepsilon$ en esta implementación pone \texttt{r + eps}, donde \texttt{eps} es el menor valor que al ser sumado varía el resultado (de forma que  \texttt{1+eps $\neq$ 1}). En ambas implementaciones se añade un ínfimo ruido a los datos, para evitar que varios puntos tengan exactamente las mismas coordenadas. Devuelve el resultado de sustituir estos valores en la expresión (\ref{eq:est_ent}).

\item \texttt{mutual\_information(variables, k)}: devuelve la información mutua entre cualquier número de variables. Para ello suma las entropías marginales de cada una de ellas y les resta la entropía conjunta (usando el mismo valor $k$ para todas las estimaciones), se aplica el Lema \ref{l:ent_im} realizando el cálculo de las entropías con la función \texttt{entropy}.

\item \texttt{mutual\_information\_2d(x, y, sigma, normalized)}: calcula la información mutua entre dos variables unidimensionales a partir de su histograma conjunto. 

\item \texttt{test\_entropy()}: permite validar la entropía comparando la entropía de una variable aleatoria con distribución normal, que conocemos teóricamente y se calcula en \texttt{entropy\_gaussian}, con la entropía estimada por \texttt{entropy}. Si ambos valores difieren demasiado, de disparará un \textit{assert}\footnote{Un \textit{assert} en Python es una expresión que se debe cumplir, en caso contrario se dispara un error, si este error no es gestionado (como en este caso) provoca la finalización del programa.}.

\item \texttt{test\_mutual\_information()}: permite comprobar la información mutua de una variable aleatoria 2 dimensional estimando los resultados para una variable aleatoria con distribución normal y comparándolos con los resultados teóricos.

\item \texttt{test\_degenerate()}: comprueba que los estimadores no dan valores infinitos.

\item \texttt{test\_mutual\_information\_2d()}: es una función similar a \texttt{test\_mutual\_information}, pero para probar \texttt{mutual\_information\_2d}.
\end{itemize} 

Si ejecutamos el código original, se producen como salida dos parejas. La primera es la estimación de la información mutua entre 2 variables normales correladas usando la función \texttt{mutual\_information}. La segunda muestra la misma información para la función \texttt{mutual\_information\_2d}. En ambos casos a la izquierda está el valor obtenido y a la derecha el valor teórico. Se obtienen los siguientes resultados:
\begin{lstlisting}
(0.1092329232318674, 0.11157177565710485)
(2.1173231240729757, 2.2033596236321267)
\end{lstlisting}

Añadimos a esta tupla la diferencia entre ambos valores:
\begin{lstlisting}
(0.1092329232318674, 0.11157177565710485, 0.0023388524252374)
(2.1173231240729757, 2.2033596236321267, 0.08603649955915094)
\end{lstlisting}

Observamos una pequeña diferencia entre los valores estimados y los teóricos. Esta pequeña diferencia podría indicar que la estimación es buena. Sin embargo, para afirmar con certeza si la estimación es buena o no habría que fijar un umbral de error y probarla en muchos más casos.\\ 

\subsection{Implementación 2 -  gregversteeg}

Pasamos a revisar la segunda implementación, que encontramos en \cite{npeet}. Esta implementación, cuyo desarrollador principal es Greg Ver Steeg \cite{gregv}, investigador asociado a la universidad de California del sur, se basa en los estimadores estudiados, aquellos propuestos en \cite{kraskov}. Como en el caso anterior, comenzamos analizando el código función por función. 
\begin{itemize}
\item \texttt{entropy(x, k, base)}: utiliza el estimador de tipo Kozachenko - Leonenko visto en (\ref{eq:est_ent}) basado en los $k$ vecinos más cercanos. \texttt{x} debe ser una lista de vectores. En primer lugar, se comprueba mediante un \textit{assert} que el valor de $k$ sea mayor que el número de muestras. Se convierte la lista \texttt{x} en un \textit{array}, se almacena el número de elementos y de características, se añade ruido a los elementos de \texttt{x} (en la función \texttt{add\_noise}), se obtiene el árbol de vecinos más cercanos usando árboles \texttt{KDTree} o \texttt{BallTree} de la biblioteca \texttt{scikit-learn} según la dimensión de los datos (en la función \texttt{build\_tree}). Se obtienen los $k$ vecinos más cercanos de \texttt{x} usando este árbol y la función \texttt{query\_neighbors}, que llama a la función \texttt{query} correspondiente de \texttt{scikit-learn}, esta función devolverá la distancia de Chebyshev(dada por la norma del máximo) a los vecinos más cercanos y sus índices, pero nos quedamos solo con las distancias. La función devuelve el siguiente valor:\[
  \frac{\psi(N) - \psi(k) + d \cdot \log 2 + \frac{d}{N}\sum \varepsilon(i)}{\log(base)}.
  \]

  Dividir entre $\log (base)$ se utiliza para que la fórmula pase a estar en base 2. La fórmula coincide con la estudiada, notando que la constante $\log V_d$ es sustituida por $d \cdot \log 2$, debido al uso de la norma del máximo.

\item \texttt{centropy(x, y, k, base)}: estima la entropía de $X|Y$, se usa la función \texttt{entropy} para calcular las entropías de $(X,Y)$ e $Y$. Estima la entropía condicionada usando la definición $h(X|Y) = h(X,Y)-h(Y)$.
\item \texttt{tc(xs, k, base)}: calcula la entropía de cada característica, devuelve la suma de las entropías de las características menos la entropía de la variable.
\item \texttt{ctc(xs, y, k, base)}: similar a la función \texttt{tc}, pero calculando las entropías condicionadas.
\item \texttt{corex(xs, y, k, base)}: análoga a la función \texttt{tc} pero en vez de la entropía calcula la información mutua.
\item \texttt{mi(x, y, z, k, base, alpha)}: calcula la información mutua entre \texttt{x} e \texttt{y}, utilizando el estimador (\ref{eq:est1}). Comienza comprobando que ambos vectores tienen la misma longitud y que $k$ es menor que la misma. Hace las transformaciones con los datos correspondientes y les añade ruido. Si se ha pasado como parámetro también la  \texttt{z}, la información mutua a calcular estará condicionada a este valor así que se añade el valor \texttt{z} al conjunto de puntos. De forma similar a como se hizo en la función \texttt{entropy}, se construye el árbol de los vecinos más cercanos y se almacena la distancia de cada elemento al $k$-ésimo vecino más cercano. A continuación, se utiliza la función \texttt{avgdigamma} para encontrar el número de puntos a distancia menor que el vecino $k$-ésimo (\texttt{num\_points}, en el espacio marginal) haciendo uso de \texttt{count\_neighbors} y devuelve la media de $\psi(num\_points)$. Si el parámetro \texttt{alpha $>$ 0} se suma a $\psi(N)$ el valor de \texttt{lnc\_correction}. La función devuelve:\[
\frac{- \langle \psi(n_x) \rangle - \langle \psi(n_y) \rangle + \psi(k) + \psi(N)}{\log (base)}.
\]
\item \texttt{cmi(x, y, z, k, base)}: calcula la información mutua de \texttt{x} e \texttt{y} condicionado a \texttt{z}, hace uso de la anterior función, que en este caso devolvería \[
  \frac{- \langle\psi(n_{xz})\rangle - \langle\psi(n_{yz})\rangle + \langle\psi(n_z)\rangle + \psi(k)}{\log(base)}.
\]
\item \texttt{kldiv(x, xp, k, base)}: devuelve la distancia de Kullback Leibler o entropía relativa entre \texttt{x} y \texttt{xp}. Comienza comprobando que las distribuciones tienen la misma dimensión y que el valor $k$ escogido no supera al número de muestras. La implementación es similar a las ya vistas. En este caso se devuelve\[
\frac{\log len(xp) - \log(len(x) -1) + d \cdot (\mathbb{E}( nnp) - \mathbb{E} (nn))}{\log(base)},
\]

donde \texttt{nn} y \texttt{nnp} son las distancias a los vecinos más cercanos de los puntos de \texttt{x} y \texttt{xp} respectivamente.

\item \texttt{lnc\_correction(tree, points, k, alpha)}: a partir del árbol de vecinos más cercanos, se queda con los $k$ vecinos más cercanos y calcular el valor de la corrección que sumar al valor $\psi(N)$.

\item \texttt{entropyd(sx, base)}: con esta comienzan una serie de funciones para estimaciones en el caso de variables aleatorias discretas. Se estima la probabilidad contando el número de apariciones de un elemento y dividiendo entre el total de elementos. Para calcular la entropía, se devuelve la suma de las probabilidades multiplicadas por el logaritmo de 1 partido por la probabilidad (por eso no aparece el cambio de signo), todo ello dividido entre $\log (base)$:\[
\frac{\sum p \log \frac{1}{p}}{\log(base)}.
\]

Si las probabilidades son correctas, esta fórmula coincide con la entropía teórica.

\item \texttt{midd(x, y, base)}: se calcula la información mutua en el caso discreto utilizando la fórmula vista en el Teorema \ref{t:ent_im}.a), restando a la entropía de \texttt{x}, la entropía de \texttt{x|y}.

\item \texttt{cmidd(x, y, z, base)}: calcula la entropía de \texttt{(x,y)} condicionada a \texttt{z}. La implementación es similar a la del caso continuo, \texttt{cmi}, pero llamando a las funciones correspondientes para variables discretas.

\item \texttt{centropyd(x, y, base)}: entropía de \texttt{x|y} para variables discretas. Se calcula como entropía conjunta menos entropía de y.

\item \texttt{tcd(xs, base)}: a la entropía por características le resta la entropía total.

\item \texttt{ctcd(xs, y, base)}:  a la entropía condicionada por características le resta la entropía condicionada.

\item \texttt{corexd(xs, ys, base)}: devuelve la suma de la información mutua por columnas menos la información mutua.

\item \texttt{micd(x, y, k, base, warning)}: con este método empiezan los estimadores mixtos, en los que una variable es discreta y la otra continua. En esta función se calcula la información mutua cuando \texttt{x} sea continua e \texttt{y} sea discreta. Primero se asegura de que tengamos el mismo número de elementos de ambas variables. Se calcula la entropía de \texttt{x} y la probabilidad de cada valor de \texttt{y} (como en el caso discreto, contando el número de veces que aparece cada elemento y dividiendo entre el total). A continuación, para cada valor que toma \texttt{y} se calcula la entropía de \texttt{x} dado \texttt{y} (este valor comienza siendo $0$ y se le va sumando la entropía calculada). Para hacer este cálculo, en primer lugar, se toma el vector de elementos de \texttt{x} dado un \texttt{y}, si tiene menos elementos que el $k$ considerado se suma la probabilidad de \texttt{y} por la entropía de \texttt{x}, añadiendo un \textit{warning} de que en este caso se está asumiendo la entropía máxima. Si el número de elementos es suficiente, se calcula la entropía de este vector y se multiplica por la probabilidad de \texttt{y}. La función devolverá el valor absoluto de la diferencia entre la entropía de \texttt{x} y la entropía de \texttt{x} dado \texttt{y}.

\item \texttt{midc(x, y, k, base, warning)}: este estimador se utilizará cuando \texttt{x} sea discreta e \texttt{y} continua, consiste en llamar a la función \texttt{micd} con los argumentos intercambiados.

\item \texttt{centropycd(x, y, k, base, warning)}: calcula la entropía de \texttt{x} (continua) condicionada a \texttt{y} (discreta). Devuelve la entropía de \texttt{x} menos la información mutua (utilizando el estimador mixto correspondiente) de \texttt{x, y}.

\item \texttt{centropydc(x, y, k, base, warning)}: utiliza la función anterior con los parámetros \texttt{x, y} intercambiados.

\item \texttt{ctcdc(xs, y, k, base, warning)}: devuelve la suma de la entropía condicionada por columnas menos la entropía condicionada cuando \texttt{x} es discreta e \texttt{y} continua.

\item \texttt{ctccd(xs, y, k, base, warning)}: llama a la función \texttt{ctcdc} para \texttt{x} continua e \texttt{y} discreta.

\item \texttt{corexcd(xs, ys, k, base, warning)}: devuelve el valor de \texttt{corexdc} intercambiando los parámetros.

\item \texttt{corexdc(xs, ys, k, base, warning)}: devuelve la diferencia entre \texttt{tcd(xs)} y \texttt{ctcdc(xs, ys, k)}.

\item Las funciones auxiliares \texttt{add\_noise}, \texttt{query\_neighbors}, \texttt{count\_neighbors}, \texttt{avgdigamma} y \texttt{build\_tree} se han ido comentando a medida que aparecían.

\item \texttt{shuffle\_test}: es una función a la que pasamos la medida a utilizar (entropía, información mutua, condicionadas, \dots) y los vectores \texttt{x} e \texttt{y}. Repite \texttt{ns} veces la medida barajando el vector \texttt{x}, devuelve la media y el intervalo de confianza de los resultados.
\end{itemize}

Además del archivo con los estimadores, en esta implementación se incluye un segundo archivo \texttt{test.py} en el que se realizan varias pruebas sobre los mismos. Si lo ejecutamos devuelve:

\begin{lstlisting}
For a uniform distribution with width alpha, the differential entropy is log_2 alpha, setting alpha = 2
and using k=1, 2, 3, 4, 5
result: [0.9675938258710816, 1.0283110450873976, 0.9611523946690828, 0.9875186838882535, 1.0917394218821124]

Gaussian random variables

Conditional Mutual Information
covariance matrix
[[4 3 1]
 [3 4 1]
 [1 1 2]]
true CMI(x:y|x) 0.5148736716970265
samples used [10, 25, 50, 100, 200]
estimated CMI [0.2253106080187056, 0.3892879127835147, 0.4398035226533797, 0.49760670969901555, 0.512074966143492]
95% conf int. (a, b) means (mean - a, mean + b)is interval
 [(0.297216360850314, 0.4346078735815755), (0.3998450496742388, 0.41093314959735017), (0.32034107567533604, 0.34254631457985824), (0.23963575516458396, 0.2875658611969877), (0.17492887513865663, 0.18358453574822897)]
Mutual Information
true MI(x:y) 0.5963225389711981
samples used [10, 25, 50, 100, 200]
estimated MI [0.36751110421959715, 0.508806849561461, 0.5987137761055255, 0.6182090681075484, 0.6174309289477204]
95% conf int.
 [(0.39751000110157275, 0.5830073784740326), (0.46139075928905027, 0.43094430206983014), (0.31222200485617396, 0.3922391298958362), (0.250926144320218, 0.35084718632752154), (0.18857541697637004, 0.19108560308455347)]

IF you permute the indices of x, e.g., MI(X:Y) = 0
samples used [10, 25, 50, 100, 200]
estimated MI [-0.005738147775725934, -0.02720997970818474, 0.0010112811193075707, -0.008517516095735011, 0.0015400222685101536]
95% conf int.
 [(0.24707698319910223, 0.43997790512266205), (0.26641749338269727, 0.2214406621687565), (0.1889408437170837, 0.21236801559420707), (0.18470276983064027, 0.19294766099193206), (0.14993655857703755, 0.16121527948941508)]


Test of the discrete entropy estimators

For z = y xor x, w/x, y uniform random binary, we should get H(x)=H(y)=H(z) = 1, H(x:y) etc = 0, H(x:y|z) = 1
H(x), H(y), H(z) 1.0 1.0 1.0
H(x:y), etc 0.0 0.0 0.0
H(x:y|z), etc 1.0 1.0 1.0


Kl divergence estimator (not symmetric, not required to have same num samples in each sample set
should be 0 for same distribution
result: -0.03587664487774093
should be infinite for totally disjoint distributions (but this estimator has an upper bound like log(dist) between disjoint prob. masses)
result: 7.818662829432822

Test discrete.
random: I(X; Y) = 0.0279 ± 0.0048 (maximum possible 2.3194)
deterministic: I(X; Y) = 3.3168 ± 0.0019 (maximum possible 3.3168)
noisy: I(X; Y) = 2.7575 ± 0.0264 (maximum possible 3.3168)
\end{lstlisting}

En primer lugar, se calcula la entropía de variables con una distribución uniforme en un intervalo $[0,\alpha]$. En el ejemplo \ref{ej:uni} calculamos la entropía teórica, $h(x) = \log \alpha$. En este caso se tomó $\alpha = 2$, luego la entropía teórica vale 1. Observamos que para los valores $k$ siempre hay una pequeña diferencia entre el valor obtenido y la entropía teórica. A continuación, calcula la información mutua (condicionada y sin condicionar) de una variable aleatoria con distribución normal, de forma teórica y usando el estimador. Por último, prueba los estimadores para el caso discreto. 


%-------------------------COMPARACIÓN-----------------------------------
\chapter{Metodología para la comparación de estimadores}
\section{Tecnología utilizada}

El lenguaje de programación utilizado para implementar las diferentes pruebas que nos permiten comparar las implementaciones de los estimadores presentadas anteriormente es Python \cite{python}. No solo es el lenguaje en el que están escritas las implementaciones, es un lenguaje con numerosos módulos y paquetes, tanto en su biblioteca estándar como fuera de la misma, que nos ayudarán a realizar la comparativa desde diferentes puntos de vista.\\

Python es un lenguaje interpretado, interactivo y orientado a objetos. Incluye módulos, excepciones, tipado dinámico. Soporta varios paradigmas de programación además de la programación orientada a objetos se permite la programación procedural o funcional. Una de las características principales de este lenguaje es su clara sintaxis.\\

Entre las bibliotecas destacamos el uso de NumPy \cite{numpy} y pandas \cite{pandas} para almacenar información, matplotlib \cite{matplotlib} para la generación de gráficas, random \cite{random} para generar los conjuntos de datos con los que realizar pruebas, cProfile y pstats \cite{profilers} para trabajar con las medidas de los tiempos de ejecución.\\

Para la construcción de los programas se ha utilizado un diseño basado en prototipos. El programa inicial es revisado y mejorado sucesivas veces hasta obtener la versión final. De esta forma, podremos ir viendo cómo se desarrollan las diferentes pruebas para comparar los estimadores si lo están haciendo de forma correcta o no e ir añadiendo funcionalidades o mejoras a medida que nos demos cuenta de que son necesarias. Se evaluó la posibilidad de usar TDD (\textit{Test-Driven Development} o desarrollo guiado por pruebas), pero para este proyecto, esta metodología quedó descartada.\\

Para tener control sobre las diferentes versiones se ha utilizado Git \cite{git}, un sistema de control de versiones libre que nos permitirá añadir las nuevas versiones de forma que podamos observar con facilidad los cambios entre las versiones así como deshacerlos si fuera necesario. El repositorio utilizado en el desarrollo de este trabajo se encuentra en \cite{sofia}.\\

A continuación, se explican con más detalle las herramientas utilizadas para medir el tiempo de ejecución. Además, se describirá cómo funciona el paralelismo en Python, la existencia del cerrojo global del intérprete (GIL) y cómo realizar el cálculo de los vecinos más cercanos utilizando la biblioteca \texttt{scikit-learn}, ya que ambas implementaciones hacen uso de la misma.\\

\subsection{Herramientas para medir el tiempo de ejecución}

Puesto que estamos comparando dos implementaciones diferentes que deben realizar el mismo cálculo, además de la precisión de sus resultados, podemos medir sus tiempos de ejecución para comprobar si hay o no diferencias significativas entre ellos. Para ello, se utilizará la herramienta \texttt{cProfile}, que nos permite hacer \textit{profiling} determinístico de programas escritos en Python, y la clase \texttt{Stats} que nos permite acceder a los medidas realizadas por la herramienta anterior \cite{profilers}.\\

La herramienta \texttt{cProfile} mide el número de veces que se ejecuta cada función (\texttt{ncalls}), el tiempo total empleado en cada función (\texttt{tottime}), el tiempo empleado en cada función cada vez que se llamó a la función (\texttt{percall}), el tiempo acumulado de cada función y todas a las que estas llamen (\texttt{cumtime}) y el tiempo acumulado entre el número de veces que se llame a esta función (\texttt{percall}). Esta herramienta será precisa incluso en el caso de funciones recursivas, al distinguir el número de veces que se llama a la función del número de veces que fue invocada de forma recursiva.\\

El uso de esta herramienta se complementa con la utilización de la clase \texttt{Stats}, que permite trabajar con las medidas realizadas por \texttt{cProfile}. Usando esta clase podremos ordenar y especificar el número o nombre de las funciones cuyos tiempo queremos observar, así como almacenar en archivos la información para consultarla más adelante.\\

Por ejemplo, si quisiéramos obtener información sobre los tiempos de ejecución de la función \texttt{function} y todas a las que esta llama para saber cuáles emplean más tiempo, podríamos ejecutar el siguiente código:\\

\begin{lstlisting}[language=Python]
  # Activamos el profiler
  pr = cProfile.Profile()
  pr.enable()
  
  # Ejecutamos la función
  function(*args)
  
  # Desactivamos el profiler
  pr.disable()

  # Creamos objeto de la clase Stats
  stats = pstats.Stats(pr).strip_dirs()

  # Imprimimos las 10 subfunciones que consuman más tiempo
  stats.sort_stats(pstats.SortKey.TIME).print_stats(10)
\end{lstlisting}

%----------------------------------------------------------------------
% PARALELISMO EN PYTHON
%-----------------------------------------------------------------------
\subsection{Paralelismo en Python}

El uso del paralelismo a nivel de hebra en Python se ve limitado por el cerrojo global del intérprete (GIL). A continuación, veremos qué es GIL y algunas herramientas para desarrollar código en paralelo, a nivel de hebras o de procesos en Python.\\

GIL, abreviatura de \textit{global interpreter lock}, \cite{glossary_GIL}, \cite{GIL}, es el mecanismo usado por el intérprete de Python para asegurar que solo  se ejecuta una hebra en cada momento. Su objetivo es hacer más seguro el acceso concurrente a memoria y esto lo consigue al poner un cerrojo sobre el intérprete al completo. Sin este cerrojo, se podrían dar problemas de sincronización en programas multihebra.\\

Algunos módulos, de la librería estándar o no, liberan este cerrojo para ejecutar código muy costoso computacionalmente. También es habitual liberarlo en operaciones de entrada o salida, que son más lentas, liberando este cerrojo se permite la ejecución de otras hebras  en lo que se completan las operaciones de entrada o salida.\\

El uso de GIL es controvertido, ya que no permite que programas multihebra aprovechen sistemas multiprocesadores. Se ha intentado crear un intérprete que permita la ejecución de múltiples hebras simultáneas sin un cerrojo global, sino con cerrojos locales. Sin embargo, no se ha conseguido, ya que los intentos anteriores bajaban el rendimiento en sistemas monoprocesador. Además, se cree que solucionar este problema haría la implementación mucho más complicada y, por ello, más difícil de mantener.\\

% 
El módulo \texttt{threading} \cite{threading} de la biblioteca estándar de Python proporciona una interfaz de alto nivel para trabajar con hebras en este lenguaje. La clase \texttt{Thread} permite crear y ejecutar hebras diferentes a la principal. Contiene un constructor para crearlas indicando la función a ejecutar, el método \texttt{start()} que permite comenzar su actividad y el método \texttt{join()} para esperar a que la hebra termine, entre otros. Sin embargo, debido al cerrojo global (GIL) solo se puede ejecutar una hebra a la vez. El paralelismo a nivel de hebra podría ser apropiado cuando queramos ejecutar tareas cuyo cuello de botella sea la entrada salida, en cuyo caso se liberaría este cerrojo. \\


Una solución para poder ejecutar código en paralelo es usar múltiples procesos en vez de múltiples hebras. El paquete \texttt{multiprocessing} \cite{multiprocessing} de la biblioteca estándar de Python permite la ejecución concurrente de procesos, evitando el cerrojo global del intérprete al usar subprocesos en vez de hebras. La clase \texttt{Process} es análoga a la clase \texttt{Thread} para el caso de procesos. Como los procesos no comparten memoria es necesario implementar un método de comunicación entre procesos, para esto están la clase \texttt{Queue} y la función \texttt{Pipe()}. Además, se proporcionan varias alternativas para sincronizar los procesos como cerrojos (\texttt{Lock}), semáforos (\texttt{Semaphore}) o barreras (\texttt{Barrier}). Así, como una clase (\texttt{Pool}) para gestionar las tareas que queremos que ejecuten los diferentes procesos.\\

No solo existen estos módulos para tratar el paralelismo, de hecho, en la documentación de Python se referencian algunas bibliotecas para ello \cite{parallelprocessing}. Entre estas comentaremos \texttt{Joblib}, pues es la que utilizan las funciones de \texttt{scikit-learn} usadas en las implementaciones estudiadas.\\

Uno de los objetivos principales de \texttt{Joblib} \cite{joblib} es permitir la ejecución en paralelo de forma sencilla. Esta biblioteca nos permite hacer uso de la concurrencia a nivel de hebra o de proceso según le indiquemos, notamos que a nivel de hebra solo será efectivo si el GIL ha sido liberado. Por defecto, utilizará el módulo \texttt{'loky'} para separar procesos y ejecutar tareas simultáneamente en varios procesadores, pero es posible indicarle que utilice el módulo \texttt{threading} o \texttt{multiprocessing}.\\
%----------------------------------------------------------------------
% Fin de Paralelismo en Python
%-----------------------------------------------------------------------

\subsection{Cálculo de los vecinos más cercanos en \texttt{scikit-learn}}
La biblioteca \texttt{scikit-learn} proporciona funcionalidades para el uso métodos basados en los $k$ vecinos más cercanos \cite{doc_nn}.\\

Para el cálculo de los $k$ vecinos más cercanos de un conjunto de puntos se puede utilizar la clase \texttt{NearestNeighbors} \cite{nearestneighbors} y su método \texttt{kneighbors}, que devuelve los índices y distancias de los vecinos de cada punto. De esta forma se realiza el cálculo de los vecinos más cercanos en la implementación 1 de la entropía. Internamente, la clase \texttt{NearestNegihbors} utiliza las clases \texttt{KDTree} y \texttt{BallTree} para encontrar los vecinos más cercanos. La implementación 2 crea directamente uno de estos árboles (según la dimensión) e invoca directamente los métodos que va necesitando.\\

Si nos encontramos con un conjunto de $n$ puntos, cada uno de dimensión $d$, podremos utilizar uno de los tres algoritmos proporcionados por la biblioteca \texttt{scikit-learn} para el cálculo de los vecinos más cercanos: \texttt{brute force}, \texttt{K-D Tree} y \texttt{Ball Tree}.\\

\texttt{Brute force} es un método de búsqueda por fuerza bruta, realizará todos los cálculos para saber qué puntos están más próximos entre sí. Aunque resulta competitivo para muestras pequeñas, su eficiencia en tiempo es $O(dn^2)$.\\

Con el objetivo de obtener algoritmos más eficientes que el de fuerza bruta, se crean estructuras de datos basadas en árboles para tratar de minimizar el número de cálculos de distancias. Se puede utilizar un árbol $k$ dimensional (\texttt{KDTree}), que es un árbol binario en el que se van repartiendo recursivamente los puntos según su valor respecto de alguno de los ejes. La construcción de este árbol es muy rápida, porque no requiere el cálculo de distancias $d$ dimensionales, es en la búsqueda de un $k$ vecino más cercano cuando hay que calcularlas. El uso de esta estructura será eficiente para dimensiones menores que 20 aproximadamente, con eficiencia en tiempo $O(d\log n)$, sin embargo, a medida que crece la dimensión, se vuelve ineficiente con un coste en tiempo casi de $O(dn)$.\\

Para solucionar la ineficiencia de los árboles $k$ dimensionales para dimensiones altas, se desarrolló la estructura \textit{ball tree}. En ella, en vez de dividir los puntos según sus valores respecto a algún eje, se hace según una serie de hiperesferas. La construcción de este tipo de árboles es más costosa que la de los árboles $k$ dimensionales, pero mejora la eficiencia de estos para realizar búsquedas cuando las dimensiones sean altas, sus tiempos son del orden $O(d\log n)$.\\

Para un número de muestras muy pequeño, $n<30$, los algoritmos de fuerza bruta podrían resultar más eficientes que los basados en árboles. Por ello, las clases basadas en árboles, proporcionan un parámetro \texttt{leaf\_size} que controla el número de muestras para el que utilizar el algoritmo de fuerza bruta. Este parámetro afecta a los tiempos de ejecución de las aproximaciones basadas en árboles, ya que el árbol se desarrollará hasta que el número de muestras  en un nodo hoja alcance el valor \texttt{leaf\_size}, para calcular la distancia entre puntos de un nodo hoja se utilizará el algoritmo de fuerza bruta. Cuanto mayor sea \texttt{leaf\_size}, menor será el tiempo de construcción del árbol, ya que se crean menos nodos. Para buscar un $k$ vecino más cercano, si \texttt{leaf\_size} es muy pequeño, el tiempo de recorrer los nodos puede elevar los tiempos de ejecución, pero si es muy grande, las consultas se realizarán prácticamente por fuerza bruta.\\

\section{\textit{T-Test} para comparar los estimadores}

Uno de los objetivos de este trabajo es conocer el comportamiento de las implementaciones de la entropía y la información mutua. Queremos saber si, en media, el comportamiento de ambos estimadores es similar. Para ello, tomaremos como hipótesis nula, $H_0$, que los estimadores en media tienen el mismo comportamiento y como hipótesis alternativa, $H_1$, que las diferencias en el comportamiento de los estimadores son significativas; y realizaremos un contraste de hipótesis. Para llevar a cabo el contraste de hipótesis nos basaremos en un estadístico al que llamaremos \textit{t}. Calcularemos el $p$-valor, la probabilidad de que bajo la hipótesis nula la variable aleatoria $T$, definida a continuación, pudiera obtener valores más extremos que el valor observado en $t$. Cuando este sea muy bajo, la hipótesis nula no se corresponderá con los datos observados y podremos rechazarla.\\

En este caso, los valores de los estimadores dependen de la muestra, por ello, en vez de comparar la media de los estimadores en diferentes datos de muestra, para cada muestra se realiza la estimación con ambas implementaciones y se calcula la diferencia entre ellas, emparejando de esta forma las observaciones. Estas son observaciones $d_1,\dots,d_N$ de la muestra aleatoria $D_1,\dots, D_N$ de la variable aleatoria ``diferencia entre ambas estimaciones'', se asumirá que sigue una distribución normal, con media $\mu_D$ y varianza $\sigma^2_D$. La hipótesis nula será,\[
H_0: \mu_D = 0,\text{ es decir, la diferencia entre las medias es nula}.
\]
La variable $T$ se define como\[
T = \frac{\olsi{D} - \mu_D}{S_d/\sqrt{N}}.
\]
El estadístico $t$ vendrá dado por\[
t = \frac{\olsi{d}}{s_d/\sqrt{N}},
\]
donde $s^2_d$ es la varianza muestral. El $p$-valor, $p$, a partir de él se calcula como \[
p = 2 P \left [ T > t | \mu_D = 0 \right ]. 
\]\\[-10pt]

Utilizaremos la implementación del \textit{test} encontrada en \cite{ttest}. Dados dos vectores medirá si la diferencia entre ellos es significativa o no, devuelve el valor del estadístico $t$ y el $p$-valor. Lo utilizaremos dos veces, en un caso pasándole como parámetro las estimaciones de $N$ valores de la entropía realizadas con ambos estimadores, en otro, las estimaciones de $N$ valores de la información  mutua realizadas con ambos estimadores.\\

\chapter{Experiencia computacional y discusión de resultados}

\section{Condiciones de experimentación}

Los experimentos mostrados en este capítulo se han realizado en un equipo con las siguientes características:

\begin{itemize}
\item Arquitectura x86\_64.
\item Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz.
\item 1 \textit{socket}, 4 núcleos por \textit{socket}, 2 hilos de procesamiento por núcleo.
\item 16 GB RAM.
\item Sistema operativo Pop!\_OS 20.04 LTS.\\
\end{itemize}

Se ha utilizado la versión de Python 3.8.2, así como las siguientes versiones de los paquetes: NumPy 1.18.4, pandas 1.0.3, scikit-learn 0.23.1, joblib 0.15.1 y matplotlib 3.2.1.\\

Además, para generar las muestras se ha creado la clase \texttt{data.py}, donde se crea un conjunto de datos de tamaño y dimensión dada y que siga una distribución normal, uniforme o que sea aleatoria.\\

\section{Análisis de los estimadores}

Pasamos a implementar el programa \texttt{compare.py} para poder comparar ambas implementaciones y realizar las modificaciones que consideremos.\\

\subsection{Estimadores de la entropía}

Comenzaremos comparando las estimaciones de la entropía. Para ello, creamos la función \texttt{ent} a la que pasamos dos variables aleatorias y calculamos su entropía a partir de los estimadores dados por las dos implementaciones.

\begin{lstlisting}
Entropía de X:
Estimador 1:  29.423303907819346
Estimador 2:  43.499531309829294
Entropía de Y:
Estimador 1:  4.232705400287453
Estimador 2:  6.084047562318489

\end{lstlisting}

Los estimadores obtienen resultados notablemente diferentes. Esto se debe a que uno de ellos nos está dando la entropía en bits (al pasarlo a base 2) y el otro en nats. Modificaremos el primer estimador, dividiendo su resultado entre $\log(2)$, para que los dos estén en las mismas unidades.\\

Pasamos a comparar estas entropías con una que conozcamos teóricamente, la distribución uniforme, vista en el Ejemplo \ref{ej:uni}, cuya entropía teórica es $\log(a)$. El código utilizado se encuentra en la función \texttt{example\_ent\_unif} y los resultados obtenidos son los siguientes:

\begin{lstlisting}
Entropía x: (a = 2)
Teórica:  1.0
Estimador 1:  1.0261633399126422
Estimador 2:  1.0261633406219435

Entropía x: (a = 100)
Teórica:   6.643856189774725
Estimador 1:   6.683574884777654
Estimador 2:   6.68357488477827

\end{lstlisting}

Las estimaciones son similares, coinciden con la entropía teórica en el primer decimal y entre hasta el undécimo decimal. Notamos que en este caso la variable aleatoria $X$ es unidimensional.\\

Para comparar las estimaciones de la entropía en una variable multidimensional, aprovechamos la función \texttt{test\_entropy} de la primera implementación y la adaptamos para calcular también la entropía con la segunda implementación. Además, añadimos el parámetro \texttt{d}, con el que podremos variar la dimensión de la variable considerada, y el parámetro \texttt{k} que marcará el número de vecinos a utilizar. La matriz \texttt{P}, utilizada para generar la matriz de covarianzas, ya no está definida de forma fija, sino que se define de forma aleatoria según la dimensión a utilizar. Comenzamos a estudiar el caso d = 2, las estimaciones son parecidas entre sí (suelen coincidir en varios decimales), si ejecutamos la función en bucle, en general se obtiene una pequeña diferencia con el estimador real, pero no se llega a disparar el \textit{assert}. Este salta algunas veces, cuando la entropía es negativa, si los estimadores dan un resultado menor que la entropía real.\\

Queremos ser realmente conscientes de la magnitud del error que se está cometiendo, para ello, implementamos la función \texttt{err\_entropy}. Esta función no es más que una modificación de \texttt{test\_entropy} en la que se eliminan los \textit{assert} y se calcula la diferencia entre la entropía gaussiana teórica y las estimadas. Notamos que si aumentamos el valor de $d$, ambas estimaciones obtienen valores próximos, difiriendo de la entropía teórica.\\

Creamos la función \texttt{exp\_err\_ent}. En ella, se pretende obtener un idea más precisa del error obtenido para cada dimensión con ambos estimadores. Para ello, se mide el error un número (\texttt{reps}) determinado de veces y se calcula la media del error obtenido para cada dimensión. Finalmente, se dibuja una gráfica del error obtenido. Ejecutando esta función con \texttt{reps = 20}, \texttt{k = 3}\footnote{En \cite{kraskov} proponen utilizar un valor $k$ entre 2 y 4 excepto cuando se está comprobando la independencia.}, \texttt{d = 2, ..., 10}, se obtienen los errores que podemos observar en la Tabla \ref{tab:err_ent}, nótese cómo a medida que aumenta la dimensión, aumenta el error de forma similar para ambos estimadores.\\

\begin{table}[!htb]
%\large
\centering
\caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica.}
\label{tab:err_ent}
\begin{tabular}{lll}
\toprule
$d$ & Error estimador 1 & Error estimador 2\\ \midrule
2 & 0.014212339128172302 & 0.014680402744924015\\
3 & 0.11105323234117655 & 0.1228230352418274\\
4 & 0.16456589959078247 & 0.1634349916264736\\
5 & 0.699756197647123 & 0.7335137192470153\\
6 & 1.11385415187634 & 1.1622332193590927\\
7 & 1.2283705866481953 & 1.2922942070634487\\
8 & 1.5855188720644953 & 1.6860491894433907\\
9 & 2.262297509187 & 2.436619705610153\\
10 & 2.433512217901498 & 2.660198051294997\\
\bottomrule
\end{tabular}
\end{table}

Representando gráficamente los valores anteriores compararemos con más facilidad el error obtenido con ambos estimadores. En la Figura \ref{fig:err_ent} encontramos esta representación. Vemos que los errores obtenidos con los dos estimadores eran prácticamente $0$ para $d=2$ y que estos van aumentando con la dimensión, siendo ligeramente superior el error obtenido con el segundo estimador, 20 repeticiones por iteración.\\

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{err_ent}
    \caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica.}
    \label{fig:err_ent}
\end{figure}

El tiempo de cómputo a medida que aumentamos la dimensión se incrementa considerablemente por lo que para hacernos una idea del comportamiento de los estimadores al aumentar la dimensión, repetimos el experimento con menos repeticiones por dimensión y llegamos hasta $d = 15$. Se obtienen los errores mostrados en la Figura \ref{fig:err_ent15}. Notamos que los errores no superan al anterior en todos los casos, se puede deber a que en las 10 repeticiones se obtengan estimaciones de la entropía que fueran mejores y ayuden a disminuir el error. Además es destacable cómo, a medida que aumenta la dimensión, crece la diferencia entre las dos estimaciones.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{err_ent_2_15_10reps}
    \caption{Errores obtenidos con ambas estimaciones al calcular la entropía de una variable aleatoria con distribución normal y compararla con la entropía teórica, 10 repeticiones por iteración.}
    \label{fig:err_ent15}
\end{figure}

\subsection{Estimadores de la información mutua}

A continuación, realizamos el estudio comparativo para los valores de las estimaciones de la información mutua. Las implementaciones realizan la estimación de la información mutua de forma diferente. La implementación 1 utiliza la fórmula que calcula la información mutua como suma de las entropías marginales de las variables y resta de la entropía conjunta. Los errores encontrados en los cálculos individuales podrían afectar al cálculo total. La implementación 2 utiliza el estimador (\ref{eq:est1}) para realizar su estimación de la entropía, es posible que los errores cometidos al calcular la entropía no se vean arrastrados.\\

Realizamos pruebas comparando los resultados de los estimadores con los de una gaussiana (en la función \texttt{test\_mutual\_information\_mod}) y calculamos el error como la diferencia entre información mutua teórica y estimada, calculamos la media tras repetir el experimento 100 veces, con $d=2$ y $k=3$. Se obtienen los siguientes resultados:\\

\begin{lstlisting}
Error estimador 1:  0.010671581846025138
Error estimador 2:  0.007051668015342052

\end{lstlisting}

El error en el primer estimador es ligeramente superior al error en el segundo.\\

Al pasar a dimensión 3, los estimadores comienzan a diferir notablemente de la entropía teórica. Revisando el código notamos que no se adapta al cambio de dimensión directamente, el cálculo de la entropía teórica de la gaussiana se hace sumando las entropías marginales y restándosela a la conjunta (en este caso como la entropía se calcula correctamente no se arrastran errores), pero la función estaba originalmente desarrollada para el caso $d = 2$. Se modifica utilizando un bucle sobre la dimensión para que, efectivamente, sume todas las entropías marginales. Una vez realizada la modificación correspondiente, realizamos la prueba para $k = 3,\ d = 3$ y 100 repeticiones, con lo que obtenemos:\\

\begin{lstlisting}
Error estimador 1:  1.6417490587527461
Error estimador 2:  1.6426795618610046

\end{lstlisting}

Al igual que en el análisis de la entropía, crearemos una función, \texttt{exp\_err\_im}, en la que repetiremos la prueba para las dimensiones dadas. Repitiendo el experimento para $d=2,...,15$; 25 veces por dimensión (calculando la media del error), con $k=3$ se obtienen los errores mostrados en la Tabla \ref{tab:err_im}. Notamos que los errores van aumentando al ir añadiendo más dimensiones. Visualizamos los errores obtenidos en la Figura \ref{fig:err_im}. Destaca que los errores obtenidos con ambas estimaciones son similares, de hecho, se superponen en la gráfica. El error en la estimación es creciente en la dimensión.\\

\begin{table}[H]
\centering
\caption{Errores obtenidos con ambas estimaciones al calcular la información mutua de variables aleatorias con distribución normal y compararla con la información mutua teórica.}
\label{tab:err_im}
\begin{tabular}{lll}
\toprule
$d$ & Error estimador 1 & Error estimador 2\\ \midrule
2 & 0.0077895466646103586 & 0.005879135481256652\\
3 & 1.852406411791869 & 1.8495568835932743\\
4 & 2.328067362081687 & 2.330375193324987\\
5 & 4.022186430366867 & 4.022166466891652\\
6 & 4.552915119298815 & 4.555859073764772\\
7 & 5.325584640708861 & 5.327399887079806\\
8 & 5.696270621561146 & 5.696928152061884\\
9 & 6.86921976358911 & 6.871685971363305\\
10 & 7.492106258014951 & 7.4935380268611915\\
11 & 8.401680241357042 & 8.400306763900506\\
12 & 9.010038140658828 & 9.011324360858382\\
13 & 9.983816029579954 & 9.986518788178701\\
14 & 10.470383330007241 & 10.468634751932003\\
15 & 11.476246581716687 & 11.47645425484027\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{err_im_25reps}
    \caption{Errores obtenidos con ambas estimaciones al calcular la información mutua de variables aleatorias con distribución normal y compararla con la información mutua teórica.}
    \label{fig:err_im}
\end{figure}

%-----------------------------------------------------------------------
\section{Análisis mediante \textit{T-Test}}

Nos preguntamos si el comportamiento en media de los estimadores de ambas implementaciones es el mismo, por ello, en el archivo \texttt{ttest.py} se escribe el código necesario para llevar a cabo un \textit{T-Test}.\\

Tomaremos muestras de tamaño $N=30$, un número superior incrementaría notablemente los tiempos de ejecución. Realizaremos el \textit{test} para diferentes grupos de muestras, según la dimensión $d$ y el tamaño de muestra $n$ utilizado para realizar la estimación. Consideraremos dimensiones pequeñas, $d=2$, medianas, $d=10$, y grandes, $d=25$; a su vez, tamaños de muestra pequeños, $n=1000$, medianos $n=30000$, y grandes, $n=100000$\footnote{El caso de dimensión grande y tamaño de muestra grande, resulta demasiado costoso computacionalmente por lo que se omite en este análisis}.\\

Realizamos un \textit{test} de normalidad sobre la muestra, de esta forma nos aseguramos de que nos encontramos bajo las condiciones del \textit{T-Test}.\\

Recordamos que el contraste de hipótesis a realizar toma como hipótesis nula que los estimadores son similares y como hipótesis alternativa que los estimadores son significativamente diferentes. Calcularemos el valor del estadístico $t$ y su $p$-valor asociado en los diferentes casos. Si el $p$-valor es menor o igual que $0.05$ supondremos que tenemos evidencia suficiente de que los estimadores son distintos y rechazaremos la hipótesis nula. En caso contrario, no tendremos evidencia suficiente para rechazar la hipótesis nula y concluiremos que los estimadores en media son similares.\\

En la Tabla \ref{tab:ttest-ent} se recogen los valores del estadístico $t$ y el $p$-valor obtenido en los diferentes casos en los que se ha estimado la entropía. Vemos cómo el valor absoluto del estimador $t$ aumenta a medida que aumenta la dimensión. Esto nos dice que la media muestral de las diferencias entre los estimadores crece con la dimensión. El $p$ valor, al contrario, disminuye a medida que aumenta la dimensión. Podemos atender a la Tabla \ref{tab:ttest-ent-reg} para saber en qué casos rechazamos la hipótesis nula y en cuáles no. Para $d=2$ no tenemos evidencia suficiente para rechazar la hipótesis nula, por tanto, concluimos que en media, para dimensiones pequeñas, los estimadores tienen un comportamiento similar. Al aumentar la dimensión, se observa suficiente evidencia de que los estimadores son diferentes, por lo que se rechaza la hipótesis nula y se afirma que su comportamiento es diferente en media. Notamos que estos resultados no varían según la dimensión de la muestra.\\

\begin{table}[!htb]
\centering
\caption{Resultados del \textit{T-test} para comparar la diferencia entre las estimaciones de la entropía para diferentes valores de $d$ y $n$.}
\label{tab:ttest-ent}
\begin{tabular}{r|l|rrr}
  \toprule
\backslashbox{$d$}{$n$} &  & 1000  & 30000 & 100000\\
\midrule
2  & $p$ &  $0.77008$ &  $0.051889$ &  $0.97277$ \\
   & $t$ &  $0.29502$ & $-2.0275$ & $-0.034430$ \\[5pt]
10 & $p$ &  $8.7473\cdot10^{-22}$ &  $1.2869\cdot10^{-22}$ &  $2.1281\cdot10^{-23}$ \\
   & $t$ & $-26.302$ & $-28.169$ & $-30.034$ \\[5pt]
25 & $p$ &  $8.6948\cdot10^{-43}$ &  $2.6519\cdot10^{-61}$ &          \\
   & $t$ & $-142.08$ & $-618.45$ &          \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{Resultado del contraste de hipótesis para comparar la diferencia entre las estimaciones de la entropía para diferentes valores de $d$ y $n$ mediante el \textit{T-Test}. Rechazo significa que rechazamos la hipótesis nula, No rechazo, que no la rechazamos.}
\label{tab:ttest-ent-reg}
\begin{tabular}{r|rrr}
  \toprule
\backslashbox{$d$}{$n$}  & 1000  & 30000 & 100000\\
\midrule
2  &  No rechazo &  No rechazo &  No rechazo \\
10 &     Rechazo &     Rechazo &     Rechazo \\
25 &     Rechazo &     Rechazo &   \\
\bottomrule
\end{tabular}
\end{table}


Veremos ahora qué ocurre en el caso de la estimación de la información mutua. Si ambos estimadores calcularan la información mutua a partir de la entropía podríamos suponer que en los casos en que las diferencias en la estimación de la entropía fueran significativas, también lo serían en el caso de la información mutua; pero como en la implementación 2 la estimación de la información mutua no se hace a partir de la entropía, repetiremos el \textit{test} para el caso de la información mutua para comprobar en qué casos las diferencias son significativas y en qué casos no lo son.\\

Los valores del estadístico $t$ y sus $p$-valores asociados al calcular las diferencias entre las implementaciones de la información mutua se encuentran en la Tabla \ref{tab:ttest-mi}. En este caso, la media de las diferencias entre los estimadores normalizada que nos da el valor $t$ aumenta a medida que crece el tamaño de la muestra y la dimensión. Los $p$-valores en este caso son menores  y disminuyen a medida que aumenta el tamaño de muestra y la dimensión de sus elementos. Los casos en los que se rechaza o no se rechaza la hipótesis nula se encuentran en la Tabla \ref{tab:ttest-mi-reg}. A la vista de los menores $p$ valores no nos extraña que en este caso se rechazará la hipótesis nula en todos los casos. Las diferencias en media entre ambas implementaciones son significativas.\\

\begin{table}[!htb]
\centering
\caption{Resultados del \textit{T-test} para comparar la diferencia entre las estimaciones de la información mutua para diferentes valores de $d$ y $n$.}
\label{tab:ttest-mi}
\begin{tabular}{r|l|rrr}
  \toprule
\backslashbox{$d$}{$n$} &  & 1000  & 30000 & 100000\\
\midrule
2  & $p$ &  $1.9822\cdot10^{-05}$ &  $3.5483\cdot10^{-14}$ &  $4.3197\cdot10^{-16}$ \\
   & $t$ &  $5.0887$ &  $13.676$ &  $16.231$ \\[5pt]
10 & $p$ &  $9.1502\cdot10^{-43}$ &  $3.7462\cdot10^{-57}$ &  $1.5652\cdot10^{-61}$ \\
   & $t$ & $-141.83$ & $-444.82$ & $-629.79$ \\[5pt]
25 & $p$ &  $1.2286\cdot10^{-46}$ &  $6.7789\cdot10^{-73}$ &          \\
   & $t$ & $-192.95$ & $-1552.6$ &          \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{Resultado del contraste de hipótesis para comparar la diferencia entre las estimaciones de la información mutua para diferentes valores de $d$ y $n$ mediante el \textit{T-Test}. Rechazo significa que rechazamos la hipótesis nula, No rechazo, que no la rechazamos.}
\label{tab:ttest-mi-reg}
\begin{tabular}{r|rrr}
  \toprule
\backslashbox{$d$}{$n$}  & 1000  & 30000 & 100000\\
\midrule
2  &  Rechazo &  Rechazo &     Rechazo \\
10 &  Rechazo &  Rechazo &     Rechazo \\
25 &  Rechazo &  Rechazo &   \\
\bottomrule
\end{tabular}
\end{table}


%-----------------------------------------------------------------------
\section{Análisis de rendimiento}

Para realizar la comparación de los tiempos de ejecución de las funciones se ha utilizado el archivo \texttt{profiling.py}.\\

Se ha realizado un análisis de los tiempos de ejecución de las dos implementaciones de la entropía y de las dos implementaciones de la información mutua estudiadas. Para ello, se han realizado 5 ejecuciones y nos hemos quedado con los tiempos medios de ejecución para diferentes valores del número de muestras y dimensión de las mismas. Remarcamos que el tiempo de ejecución considerado es el tiempo acumulado por la función a estudiar y todas las funciones a las que esta invoque.\\

Se han considerado tres rangos para el tamaño de las muestras: pequeñas (1000 muestras), medianas (30000 muestras) y grandes (100000). De forma similar, en función a la dimensión, se realizaron estimaciones con tres tipos de dimensiones: pequeñas (dimensión 2), medianas (dimensión 10) y grandes (dimensión 25).\\

Comenzaremos analizando los tiempos obtenidos al calcular la entropía. En la Tabla \ref{tab:tiempos_ent_1} podemos ver los tiempos de de ejecución, en segundos, de la función \texttt{entropy} de la primera implementación para los valores de $n$ y $d$ estudiados. Como es de esperar, a medida que aumenta el número de muestras y la dimensión, se incrementan los tiempos de ejecución en ambos casos. Observamos también la desviación típica de las medidas realizadas, para constatar que el número de medidas tomadas es suficiente. Consideramos que las desviaciones típicas obtenidas son lo suficientemente pequeñas en relación a los tiempos medios de ejecución.\\

\begin{table}[!htb]
%  \begin{adjustwidth}{-.45in}{-.45in}  
    \caption{Tiempos de ejecución de la función \texttt{entropy} de la implementación 1 para diferentes valores de $n$ y $d$.}
    \label{tab:tiempos_ent_1}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Media de los tiempos obtenidos en las 5 ejecuciones.}
        \begin{tabular}{l|rrr}
\toprule
\backslashbox{$d$}{$n$} & 1000   &  30000  &   100000 \\
\midrule
2 & 0.0020\,s &  0.0774\,s &   0.4067\,s \\
10  & 0.0149\,s &  2.5013\,s &  16.0703\,s \\
25 & 0.1158\,s & 34.2677\,s & 654.9519\,s \\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Desviaciones típicas de los tiempos obtenidos.}
        \begin{tabular}{l|rrr}
\toprule
\backslashbox{$d$}{$n$} & 1000   & 30000  &  100000 \\
\midrule
2 & 0.0003\,s & 0.0064\,s &  0.0119\,s \\
10  & 0.0005\,s & 0.2573\,s &  0.6337\,s \\
25 & 0.0019\,s & 2.2058\,s & 10.1255\,s \\
\bottomrule
\end{tabular}

    \end{subtable}
 %   \end{adjustwidth}
\end{table}

La Tabla \ref{tab:tiempos_ent_2} contiene la misma información, esta vez sobre la función \texttt{entropy} de la implementación 2. En esta implementación de la entropía, los tiempos también aumentan al incrementar el número de muestras y la dimensión. Las desviaciones típicas vuelven a ser no significativas de acuerdo a los datos medios.\\

\begin{table}[!htb]
%  \begin{adjustwidth}{-.45in}{-.45in}  
    \caption{Tiempos de ejecución de la función \texttt{entropy} de la implementación 1 para diferentes valores de $n$ y $d$.}
    \label{tab:tiempos_ent_2}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Media de los tiempos obtenidos en las 5 ejecuciones.}
        \begin{tabular}{l|rrr}
\toprule
\backslashbox{$d$}{$n$} & 1000   &  30000  &   100000 \\
\midrule
2 & 0.0017\,s &  0.0648\,s &   0.2515\,s \\
10  & 0.0071\,s &  0.8797\,s &   5.6956\,s \\
25 & 0.0475\,s & 30.6916\,s & 876.7703\,s \\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Desviaciones típicas de los tiempos obtenidos.}
        \begin{tabular}{l|rrr}
\toprule
\backslashbox{$d$}{$n$} & 1000   & 30000  &  100000 \\
\midrule
2 & 0.0001\,s & 0.0054\,s &  0.0137\,s \\
10  & 0.0002\,s & 0.0823\,s &  0.2450\,s \\
25 & 0.0001\,s & 0.1998\,s & 12.1683\,s \\
\bottomrule
\end{tabular}

    \end{subtable}
 %   \end{adjustwidth}
\end{table}


Para realizar la comparación atenderemos a la Tabla \ref{tab:tiempos_ent}, que contiene la información relativa a los tiempos medios de ejecución estructurada de forma que resulte más fácil comparar. Cuando la dimensión es 2, los tiempos son próximos para muestras pequeñas y medianas, diferenciándose más para muestras grandes. Sin embargo, en casi todos los casos la implementación 1 es más lenta que la 2. Al aumentar la dimensión, se incrementan los tiempos de ejecución de ambas implementaciones, pero también las diferencias entre ellas. Para una muestra pequeña la implementación 1 invierte aproximadamente el doble de tiempo que la 2. Con muestras mayores la diferencia de tiempo se incrementa, para un tamaño de muestra grande, la implementación 1 tarda casi el triple de tiempo que la 2. Para una dimensión grande, los tiempos de ejecución son considerablemente mayores en ambas implementaciones. Destaca que para una dimensión grande y un tamaño de muestra grande se invierte la tendencia observada en el resto de combinaciones de tamaño de muestra y dimensión, en este caso la implementación 1 tarda menos que la 2.\\

\begin{table}[H]
\centering
\caption{Tiempos de ejecución medios de la función \texttt{entropy} en ambas implementaciones para diferentes valores del parámetro $d$ y $n$.}
\label{tab:tiempos_ent}
\begin{tabular}{r|l|rrr}
  \toprule
Implementación & \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
 \midrule
1 & 2 & 0.0020\,s &  0.0774\,s &   0.4067\,s \\
2 & 2 & 0.0017\,s &  0.0648\,s &   0.2515\,s \\[5pt]
1 & 10  & 0.0149\,s &  2.5013\,s &  16.0703\,s \\
2 & 10  & 0.0071\,s &  0.8797\,s &   5.6956\,s \\[5pt]
1 & 25 & 0.1158\,s & 34.2677\,s & 654.9519\,s \\
2 & 25 & 0.0475\,s & 30.6916\,s & 876.7703\,s \\
\bottomrule
\end{tabular}
\end{table}

Podemos concluir, por tanto, que la segunda implementación es más rápida que la primera para calcular la entropía, salvo para dimensión y tamaño de muestra grande. El cálculo que realizan ambas estimaciones es el mismo, utilizan el mismo estimador, varía únicamente la forma de implementarlo, en cuanto a tiempo de cómputo será preferible la implementación 2.\\

Si analizamos qué pasos emplean más tiempo en las implementaciones de la entropía nos sorprende descubrir que es en la misma función, el método \texttt{query} de \texttt{sklearn.neighbors.\_kd\_tree.BinaryTree}. Sin embargo, aunque ambas implementaciones hacen uso de este método, la primera invierte más tiempo en ella que la segunda. Por ejemplo, en una ejecución de la implementación 1 para tamaños de muestra y dimensiones medianas que requirió $2.2\,$s, esta función utilizó $2.125\,$s., mientras que la implementación 2 invirtió $0.739\,$s en esta función de una ejecución que en total consumió $0.776\,$s, véase Tabla \ref{tab:profile-ent-22}. Destaca que esta función no es solo la que más tiempo consume sino que la mayor parte del tiempo del cálculo de la entropía se emplea en realizar este cálculo, siendo los tiempos de las siguientes funciones mucho menores.\\

\begin{table}[!htb]
 %\begin{adjustwidth}{-.3in}{-.3in}  
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la entropía para ambas implementaciones, caso $d = 10$, $n = 30000$, tiempos en segundos.}
    \label{tab:profile-ent-22}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución $2.202$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
1 &   2.125 &   2.125 &  \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
1 &   0.072 &   0.072 &   \texttt{\_base.py:349(\_fit)}\\
10 &   0.002 &   0.000 &  \texttt{built-in method numpy.array}\\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $0.776$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
1  &  0.739 & 0.739 & \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
1 & 0.034 & 0.034 & \texttt{entropy\_estimators.py:280(build\_tree)}\\
1 & 0.002 & 0.002 & \texttt{method 'random\_sample' of 'numpy.random.mtrand.RandomState' objects}\\
\bottomrule
\end{tabular}

    \end{subtable}
 %  \end{adjustwidth}
\end{table}

En la implementación 1 se obtienen los vecinos más cercanos utilizando la clase \texttt{sklearn.neighbors.NearestNeighbors}, invocando al método \texttt{kneighbors} \cite{imp_kneighbors}. Este método realiza una serie de comprobaciones antes de invocar al método \texttt{query} para obtener realmente los vecinos más cercanos y sus distancias. Además, la llamada a \texttt{query} se realiza usando métodos que proporcionan paralelismo de la biblioteca \texttt{joblib}, indicando que se utilizará paralelismo a nivel de hebra. En este caso, al crear el árbol con las opciones por defecto, se utiliza la métrica de Minkoswki con $p=2$, es decir, la distancia euclídea. Además, el parámetro \texttt{leaf\_size} se inicializa a 30.\\

En la implementación 2, en primer lugar, se crea un árbol, que será de tipo \texttt{sklearn.neighbors.BallTree} cuando la dimensión sea mayor que 20 y de tipo \texttt{sklearn.neighbors.KDTree} cuando la dimensión sea menor que 20; y a continuación, se calculan los vecinos más cercanos llamando directamente al método \texttt{query} de la clase correspondiente. La métrica utilizada es la de Chebyshev y \texttt{leaf\_size = 40}.\\

El incremento de tiempo en las llamadas a \texttt{query} en la primera implementación podría provenir del coste de realizar las llamadas en paralelo y no aprovecharlo realmente, sabemos que el paralelismo a nivel de hebra en Python depende de si el cerrojo global del intérprete (GIL) está activado o desactivado. En la segunda implementación,  se invoca al método directamente y no existe este coste.\\

La diferencia de tiempos también podría venir causada por el valor de \texttt{leaf\_size}. La diferencia entre los valores de este parámetro en ambas implementaciones es 10, comparado con el número de muestras puede parecer pequeño, pero al influir en el cálculo de los vecinos más cercanos de todos los puntos podría afectar notablemente.\\

La métrica utilizada podría generar diferencias en los tiempos de ejecución. La métrica de Minkowski requiere calcular la raíz cuadrada de las diferencias al cuadrado, mientras que la de Chebyshev el máximo de las diferencias.\\ 

El tipo de árbol utilizado para buscar los vecinos más cercanos puede afectar a los tiempos de ejecución, de hecho, aunque la documentación recomiende usar \texttt{BallTree} para dimensiones mayores que 20, este parámetro no es exacto y es posible que en este caso llegue a resultar más costoso utilizarlo frente a \texttt{KDTree}.\\

Analizamos específicamente el caso en el que tamaño de muestra y dimensión son grandes, con el fin de averiguar qué ocurre exactamente para que la implementación 1 sea más rápida que la 2 solo en este caso. En la Tabla \ref{tab:profile-ent-33} observamos que en las dos implementaciones es el método \texttt{query} el que más tiempo emplea. Con la diferencia de que en la implementación 1 es el método \texttt{query} de \texttt{KDTree} y en la 2 el de \texttt{BallTree}. Sin embargo, sabemos que la implementación 2 utiliza el método \texttt{query} de la clase \texttt{BallTree} en todas las ejecuciones en las que $d= 25$ y en las demás sigue siendo más rápida que la implementación 1. Sospechamos que el incremento en el número de muestras provoca que esta implementación sea más lenta únicamente en este caso.\\

\begin{table}[!htb]
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la entropía para ambas implementaciones, caso $d = 25$, $n = 100000$, tiempos en segundos.}
    \label{tab:profile-ent-33}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución 643.479\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
1 & 643.026 & 643.026 & \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
1  &  0.440  &  0.440  & \texttt{\_base.py:349(\_fit)}\\
10 &   0.007 &   0.001 & \texttt{built-in method numpy.array}\\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $867.12$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
 1 & 866.847 & 866.847 & \texttt{method 'query' of 'sklearn.neighbors.\_ball\_tree.BinaryTree' objects}\\
1 &   0.249 &   0.249 & \texttt{entropy\_estimators.py:280(build\_tree)}\\1 & 0.016 &   0.016 & \texttt{method 'random\_sample' of 'numpy.random.mtrand.RandomState' objects}\\
\bottomrule
\end{tabular}
    \end{subtable}
    \end{table}
    
%-----------------------------------------------------------------------
Pasamos a estudiar qué implementación es más rápida en el cálculo de la información mutua. Recordamos que en este caso las implementaciones no realizan el mismo cálculo. La primera implementación halla la información mutua a través de las entropías de las variables marginales, mientras que la segunda implementación lo hace a través de $I^{(2)}$.\\

En la Tabla \ref{tab:tiempos_im_1} se recogen los tiempos de ejecución de la implementación 1 para los valores de $n$ y $d$ considerados.
Al igual que para la entropía, los tiempos en todos los casos aumentan al considerar muestras más grandes y de mayor dimensión. Además, notamos que los tiempos de ejecución son superiores a los necesitados para calcular la entropía, esto no es de extrañar, en uno de los casos se calcula la entropía de varias variables y luego se opera con ellas para obtener la información mutua. Las desviaciones típicas son pequeñas en relación a los valores medios.\\

\begin{table}[!htb]
%  \begin{adjustwidth}{-.5in}{-.5in}  
    \caption{Tiempos de ejecución del cálculo de la información mutua para diferentes valores de $n$ y $d$, implementación 1.}
    \label{tab:tiempos_im_1}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Tiempos medios de ejecución.}
        \begin{tabular}{l|rrr}
          \toprule
          \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
          \midrule
2  & 0.0067\,s &   0.3754\,s &    1.3876\,s \\
10  & 0.0681\,s &  24.1955\,s &  344.3863\,s \\
25 & 0.2173\,s & 173.4488\,s & 2811.0427\,s \\
\bottomrule
\end{tabular}
\end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Desviaciones típicas de los tiempos de ejecución.}
        \begin{tabular}{l|rrr}
          \toprule
          \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
          \midrule
2 & 0.0008\,s & 0.0089\,s &  0.0833\,s \\
10  & 0.0013\,s & 1.5809\,s & 10.2065\,s \\
25 & 0.0013\,s & 4.2758\,s & 87.2810\,s \\
\bottomrule
        \end{tabular}
    \end{subtable}
%    \end{adjustwidth}
\end{table}

De la misma forma, en la Tabla \ref{tab:tiempos_im_2} encontramos los tiempos de ejecución de la información mutua en el caso de la implementación 2. Atendiendo a los tiempos medios de ejecución, vemos que crecen a medida que aumenta el tamaño de muestra y dimensión, siendo más altos que los tiempos de cálculo de la entropía. Las desviaciones típicas son también, en este caso, pequeñas en comparación a los tiempos que comparan, por tanto, concluimos que 5 mediciones han sido suficientes para conocer los tiempos de ejecución de las diferentes funciones con precisión.\\

\begin{table}[!htb]
%  \begin{adjustwidth}{-.5in}{-.5in}  
    \caption{Tiempos de ejecución del cálculo de la información mutua para diferentes valores de $n$ y $d$, implementación 2.}
    \label{tab:tiempos_im_2}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Tiempos medios de ejecución.}
        \begin{tabular}{l|rrr}
          \toprule
          \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
          \midrule
2 & 0.0066\,s &   0.6363\,s &    2.7960\,s \\
10  & 0.0485\,s &  33.6843\,s &  798.8034\,s \\
25 & 0.1271\,s & 154.2582\,s & 3036.5627\,s \\
\bottomrule
\end{tabular}
\end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Desviaciones típicas de los tiempos de ejecución.}
        \begin{tabular}{l|rrr}
          \toprule
          \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
          \midrule
2 & 0.0012\,s & 0.0256\,s &  0.1403\,s \\
10  & 0.0005\,s & 0.6881\,s & 15.5813\,s \\
25 & 0.0010\,s & 0.4396\,s & 22.6054\,s \\
\bottomrule
        \end{tabular}
    \end{subtable}
%    \end{adjustwidth}
\end{table}

Compararemos los tiempos de ejecución de ambas implementaciones utilizando la Tabla \ref{tab:tiempos_mi}. Para un tamaño de muestra pequeña y dimensión pequeña, los tiempos son próximos, pero a medida que van aumentando, tanto dimensión como tamaño de muestra, aumenta también la diferencia entre los tiempos de ejecución de las dos implementaciones. Si nos fijamos en el tamaño de muestra pequeño, la implementación 1 resulta más lenta que la 2 para todas las dimensiones consideradas, ocurriendo lo contrario cuando nos fijamos en un tamaño de muestra grande, en los que la implementación 1 resulta más rápida que la 2 para todas las dimensiones. Para un tamaño de muestra mediano, el comportamiento varía según la dimensión, para $d=2, d= 10$ la implementación 1 resulta más rápida que la 2, mientras que para $d=25$ ocurre lo contrario.\\

\begin{table}[!htb]
\centering
\caption{Tiempos de ejecución del cálculo de la información mutua en ambas implementaciones para diferentes valores del parámetro $d$ y $n$.}
\label{tab:tiempos_mi}
\begin{tabular}{r|l|rrr}
  \toprule
Implementación & \backslashbox{$d$}{$n$} & 1000  & 30000 & 100000\\
 \midrule
1 & 2  & 0.0067\,s &   0.3754\,s &    1.3876\,s \\
2 & 2 & 0.0066\,s &   0.6363\,s &    2.7960\,s \\[5pt]
1 & 10  & 0.0681\,s &  24.1955\,s &  344.3863\,s \\
2 & 10  & 0.0485\,s &  33.6843\,s &  798.8034\,s \\[5pt]
1 & 25 & 0.2173\,s & 173.4488\,s & 2811.0427\,s \\
2 & 25 & 0.1271\,s & 154.2582\,s & 3036.5627\,s \\
\bottomrule
\end{tabular}
\end{table}

En la primera implementación, vuelve a ser el método \texttt{query} de \texttt{KDTree} el que requiere más tiempo de ejecución, aunque destacamos que esta vez el número de veces que se llama a esta función es 3, ya que se calculan tres entropías. Dos de estas llamadas serán sobre puntos de dimensión $d$, cuando calculemos $h(X)$ y $h(Y)$, mientras que la restante es sobre puntos de dimensión $2d$, en el cálculo de $h(X, Y)$.\\

En la segunda implementación la función que más tiempo consume es también el método \texttt{query}, de la clase \texttt{KDTree} o \texttt{BallTree} según la dimensión. Notamos que en esta implementación, antes de llamar al método \texttt{query}, se unen los puntos de las variables para las que calculamos la información mutua, por tanto, para $d=10$ se consideraría un árbol con puntos de dimensión $20$. Además, la segunda función que más tiempo consume se aproxima, en tiempo total, a \texttt{query}; es la función  \texttt{query\_radius}, utilizada para calcular el valor de la media de $\psi(n)$. Esta función se llama dos veces, una para contar los puntos de la primera variable que están a menor distancia que el $k$-ésimo vecino y otra para realizar el mismo cálculo respecto de los puntos de la segunda variable. Cada una de estas llamadas individuales a \texttt{query\_radius}, donde consideramos puntos de dimensión $d$, será más rápida que la llamada a \texttt{query}, donde se consideran puntos de dimensión $2d$.\\

En resumen, las funciones que más tiempo consumen son: en la implementación 1 las 2 llamadas a \texttt{query} para puntos de dimensión $d$ y la llamada a \texttt{query} para puntos de dimensión $2d$, en la implementación 2 la llamada a \texttt{query} sobre puntos de dimensión $2d$ y las 2 llamadas a \texttt{query\_radius} para puntos de dimensión $d$.\\

Comenzamos analizando qué ocurre para un número de muestras pequeño, $n=1000$, en la Tabla \ref{tab:profile-im-21} los resultados del \textit{profiling} para muestras de dimensión 10. Por un lado, al estudiar el caso de la entropía vimos que los tiempos del cálculo de \texttt{query} en la implementación 1 eran más lentos que en la 2. Por otro lado, es posible que para muestras pequeñas el cálculo de \texttt{query\_radius} sea más rápido que el de \texttt{query}. Estos motivos nos permiten justificar que la implementación 2 haya resultado más rápida para este tamaño de muestra.\\

\begin{table}[!htb]
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la información mutua para ambas implementaciones, caso $d = 10$, $n = 1000$, tiempos en segundos.}
    \label{tab:profile-im-21}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución $0.069$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
3 &   0.064 &   0.021 &   \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
3 &   0.003 &   0.001 &   \texttt{\_base.py:349(\_fit)}\\
12 &   0.000 &   0.000 &   \texttt{method 'reduce' of 'numpy.ufunc' objects}\\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $0.049$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
1 &   0.025 &   0.025 & \texttt{method 'query' of 'sklearn.neighbors.\_ball\_tree.BinaryTree' objects}\\
2 &   0.022 &   0.011 &  \texttt{method 'query\_radius' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
3 &   0.002 &   0.001 &  \texttt{entropy\_estimators.py:280(build\_tree)}\\
\bottomrule
\end{tabular}
    \end{subtable}
    \end{table}

Para tamaños de muestras grandes, $n = 100000$, se observa el comportamiento contrario, la implementación 1 es más rápida que la 2. Si observamos en la Tabla \ref{tab:profile-im-31} los tiempos de las funciones que emplearon más tiempo, vemos claramente el motivo. El tiempo de ejecución de \texttt{query\_radius} en la implementación 2 supera a los tiempos de ejecución de \texttt{query} en la implementación 1. De hecho, el cálculo de 2 \texttt{query\_radius} supera en tiempo de ejecución al cálculo de 3 \texttt{query}.\\

\begin{table}[!htb]
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la información mutua para ambas implementaciones, caso $d = 2$, $n = 100000$, tiempos en segundos.}
    \label{tab:profile-im-31}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución $1.348$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
        3 &   1.125 &   0.375 &  \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
        3 &   0.205 &   0.068 &   \texttt{\_base.py:349(\_fit)}\\
36/22 &   0.006  &  0.000  &  \texttt{built-in method numpy.core.\_multiarray\_umath.implement\_array\_function}\\
\bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $02.650$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
2 &   2.010 &   1.005 &  \texttt{method 'query\_radius' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
1 &   0.424 &   0.424 &   \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
3 &   0.204 &   0.068 &  \texttt{entropy\_estimators.py:280(build\_tree)}\\
\bottomrule
\end{tabular}
    \end{subtable}
\end{table}

Para tamaños de muestras medianos, $n = 30000$, tendremos que distinguir según la dimensión. Para dimensiones pequeñas y medianas la implementación 1 resulta más rápida debido a que los tiempos de cómputo de \texttt{query\_radius} superan a los de \texttt{query}, véase Tabla \ref{tab:profile-im-21}. Sin embargo, para dimensiones grandes, la implementación 1 resulta más lenta, esto se debe a que al cambiar el tipo de árbol con el que se realiza \texttt{query\_radius} de \texttt{KDTree} a \texttt{BallTree} disminuye su tiempo de cálculo, siendo inferior que los \texttt{query} que realiza la implementación 1, como vemos en la Tabla \ref{tab:profile-im-23}.\\

\begin{table}[!htb]
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la información mutua para ambas implementaciones, caso $d = 2$, $n = 30000$, tiempos en segundos.}
    \label{tab:profile-im-21}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución $0.37$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
         3 &   0.325 &   0.108 & \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
        3 &   0.039 &   0.013 &  \texttt{\_base.py:349(\_fit)}\\
        36/22  &  0.002 &   0.000  & \texttt{built-in method numpy.core.\_multiarray\_umath.implement\_array\_function}\\
        \bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $0.635$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
        2 &   0.455 &   0.228 & \texttt{method 'query\_radius' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
        1 &   0.134 &   0.134 &   \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
        3 &   0.043 &   0.014 &  \texttt{entropy\_estimators.py:280(build\_tree)}\\
\bottomrule
\end{tabular}
    \end{subtable}
\end{table}

\begin{table}[!htb]
    \caption{Resultados \texttt{cProfiler}. Funciones que consumen más tiempo en el cálculo de la información mutua para ambas implementaciones, caso $d = 25$, $n = 30000$, tiempos en segundos.}
    \label{tab:profile-im-23}
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 1, tiempo total de ejecución $172.812$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
         3 &   0.325 &   0.108 & \texttt{method 'query' of 'sklearn.neighbors.\_kd\_tree.BinaryTree' objects}\\
        3 &   0.039 &   0.013 & \texttt{\_base.py:349(\_fit)}\\
        36/22  &  0.002 &   0.000  & \texttt{built-in method numpy.core.\_multiarray\_umath.implement\_array\_function}\\
        \bottomrule
\end{tabular}
    \end{subtable}\\[10pt]
    \begin{subtable}{\linewidth}
      \centering
        \caption{Implementación 2, tiempo total de ejecución $154.347$\,s.}
        \begin{tabular}{lllp{0.55\textwidth}}
\toprule
\texttt{ncalls} &  \texttt{tottime} & \texttt{percall} & \texttt{filename:lineno(function)} \\
\midrule
1 &  95.822 &  95.822 &  \texttt{method 'query' of 'sklearn.neighbors.\_ball\_tree.BinaryTree' objects}\\
        2 &  58.309 &  29.154 & \texttt{method 'query\_radius' of 'sklearn.neighbors.\_ball\_tree.BinaryTree' objects}\\
        3 &   0.197 &   0.066 &   \texttt{entropy\_estimators.py:280(build\_tree)}\\
\bottomrule
\end{tabular}
    \end{subtable}
\end{table}

\chapter{Conclusiones}

Este trabajo comienza con una revisión de los conceptos necesarios para abordar sus objetivos, se estudian incialmente los conceptos de entropía e información mutua, tanto en el caso discreto como en el caso continuo. Posteriormente, se han estudiado sus propiedades, conceptos relacionados y relaciones entre ellas. A continuación, se analiza teóricamente algunas propuestas de estimadores de la entropía, basadas en los $k$ vecinos y también del tipo de Kozachenko - Leonenko, y de la información mutua, basadas en particiones y en el estimador de la entropía del tipo de Kozachenko - Leonenko.\\

El análisis teórico y experimental de los estimadores nos permite obtener algunas conclusiones. El análisis de su comportamiento en el caso de la distribución normal permite afirmar que los errores en ambos estimadores aumentan a medida que crece la dimensión de los datos. En el caso de la entropía es ligeramente inferior el error obtenido por la implementación 1 \cite{estimating}, pero en el caso de la información mutua estos errores resultan similares en ambas implementaciones.\\

La realización del \textit{T-Test} sobre las implementaciones de la entropía permite concluir que el comportamiento de los estimadores es similar para dimensiones pequeñas, pero a medida que la dimensión crece, las diferencias en media entre los estimadores son significativas. En el caso de la entropía, las pruebas realizadas comparando con variables aleatorias generadas con una distribución normal, permitirían concluir que el estimador 1 \cite{estimating} presenta errores menores y, por tanto, resultaría preferible al estimador 2\cite{npeet}. En el caso de la información mutua, sin embargo, no concluimos nada en este sentido, ya que los errores obtenidos por ambos estimadores fueron similares.\\

Atendiendo al rendimiento, la implementación más rápida para realizar el cálculo de la entropía es la dada en la implementación 2 \cite{npeet}, excepto cuando el número de observaciones y la dimensión sean grandes, en cuyo caso resulta más rápida la implementación 1 \cite{estimating}. En el estudio de la información mutua, si tenemos que elegir la estimación más rápida, dependerá del tamaño de la muestra y de la dimensión. Para muestras de tamaño pequeño resulta más veloz la implementación 2 \cite{npeet}, al igual que para muestras de tamaño medio y dimensión grande. En el resto de casos, muestras de tamaño grande y muestras de tamaño mediano con dimensiones pequeñas y medianas es más rápida la implementación 1 \cite{estimating}. Notamos que para dimensiones grandes y tamaños de muestra grande los tiempos de ejecución se incrementan notablemente, tiempos cercanos a unos pocos segundos de ejecución cuando la dimensión es pequeña se transforman en tiempos entre 45 minutos y una hora para dimensiones grandes y tamaños de muestra grandes. Se hace necesario, por tanto, el uso de paralelismo a nivel de ejecución si se desea obtener resultados en tiempo aceptables.\\

\chapter{Estimación del coste y planificación}

\section{Estimación del coste}

Se realiza un presupuesto del coste del trabajo, en el que se incluyen las horas dedicadas a cada concepto y se realiza una estimación a precio 35€/hora, véase Tabla \ref{tab:coste}.\\

Para la realización del presupuesto se ha dividido el trabajo en tres módulos: módulo teórico, módulo práctico y módulo general.\\

En el módulo teórico se recogen las tareas asociadas al estudio y análisis de los contenidos de carácter teórico. Notamos que aquí se incluye el estudio de las diferentes nociones a partir de varias fuentes para su posterior síntesis.\\

En el módulo experimental se recogen las diferentes tareas asociadas al análisis de las implementaciones. No sólo se realiza un análisis de las mismas sino que es necesario la implementación y validación de código para llevar a cabo el análisis desde diferentes perspectivas. Además, se tiene en cuenta el equipo utilizado y el tiempo dedicado a la instalación del sistema operativo y las bibliotecas necesarias para llevar a cabo el análisis.\\

En el módulo general se encuentran las tareas relativas a la redacción de este documento, además de las correcciones que ha requerido, así como las reuniones con los tutores.\\

\begin{table}[hpt]
  
\begin{adjustwidth}{-.4in}{-.4in} 
\centering
\caption{Estimación del coste del trabajo.}
\label{tab:coste}
\begin{tabular}{p{7.3cm}R{2cm}R{2cm}R{2cm}}
  \toprule
Concepto & Cantidad (horas) & Coste unidad (€/h) & Coste total (€)\\
 \midrule
Módulo teoría & & &\\
 \midrule
 Análisis/estudio conceptos previos & 15 & 35 & 525\\[3pt]
 Análisis/estudio caso discreto & 25 & 35 & 875\\[3pt]
 Análisis/estudio caso continuo & 25 & 35 & 875\\[3pt]
 Análisis/estudio estimadores & 60 & 35 & 2100\\[5pt]
 \midrule
 Módulo experimental & & &\\
 \midrule
 Equipo de trabajo: Dell portátil Vostro 5590 & & 1085 & 1085\\[3pt]
 Instalación sistema operativo, bibliotecas, entorno de desarrollo & 10 & 35 & 350\\[3pt]
 Análisis implementaciones estimadores & 20 & 35 & 700\\[3pt]
 Análisis/desarrollo código análisis del error & 30 & 35 & 1050\\[3pt]
 Análisis/desarrollo/validación clase para generación de muestras & 5 & 35 & 175\\[3pt]
 Análisis del \textit{T-Test}, desarrollo del \textit{T-Test}, validación del \textit{T-Test} & 25 & 35 & 875\\[3pt]
 Análisis \textit{profilers}, implementación/integración/validación código para \textit{profiling}, ejecución & 32 & 35 & 1120\\[3pt]
 Análisis de resultados obtenidos & 20 & 35 & 700\\[5pt]
 \midrule
 Módulo general& & &\\
 \midrule
 Redacción memoria & 50 & 35 & 1750\\[3pt]
 Corrección memoria & 15 & 35 & 525\\[3pt]
 Reuniones con los tutores & 30 & 35 & 1050\\[3pt]
 \midrule
 Total & 362 & &13755\\
 \bottomrule\\
 IVA no incluido.&&&\\
\end{tabular}
\end{adjustwidth}
\end{table}

\section{Planificación}

Se incluye en la Figura \ref{fig:plan} la planificación optimista del desarrollo del trabajo. En ella se han dividido las tareas también en módulo general (de color rosa), módulo teórico (de color azul) y módulo práctico (de color verde). Notamos que las tareas del módulo general en realidad se encontrarán presentes a lo largo de todo el desarrollo del proyecto, ya que la redacción de la memoria, corrección de la misma y reuniones con los tutores para comentar estas correcciones y/o posibles dudas se realizan a medida que se avanza en la realización de las tareas específicas. En la planificación se ha tenido en cuenta que el primer cuatrimestre habría más carga docente  que el segundo cuatrimestre (y con ello menos tiempo para la realización de este trabajo), así como el periodo de exámenes en el que prácticamente no se avanzaría. \\

\begin{figure}[!htb]
    \centering
  \centerline{  \includegraphics[width=1.3\textwidth]{planificacion-optimista}}
    \caption{Planificación optimista del desarrollo del trabajo.}
    \label{fig:plan}
\end{figure}

Sin embargo, no todo ha salido según lo previsto, por ejemplo, el tiempo en estudiar y comprender los estimadores fue superior al estimado y requirió la consulta de bibliografía adicional. Además, el estudio de los estimadores fue intercalado con algunas tareas del módulo práctico, como vemos en la Figura \ref{fig:plan2}, donde encontramos una planificación más realista del trabajo.\\

\begin{figure}[!htb]
    \centering
  \centerline{  \includegraphics[width=1.3\textwidth]{planificacion}}
    \caption{Planificación realista del desarrollo del trabajo.}
    \label{fig:plan2}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%       REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography

\end{document}
